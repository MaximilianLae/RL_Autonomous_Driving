{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "\n",
    "    # Parametrization bellow cannot be changed\n",
    "    \"lanes_count\" : 10, # The environment must always have 10 lanes\n",
    "    \"vehicles_count\": 50, # The environment must always have 50 other vehicles\n",
    "    \"duration\": 120,  # [s] The environment must terminate never before 120 seconds\n",
    "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.IDMVehicle\", # This is the policy of the other vehicles\n",
    "    \"initial_spacing\": 2, # Initial spacing between vehicles needs to be at most 2\n",
    "\n",
    "    # Refer to refer to https://highway-env.farama.org/observations/ to change observation space type\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\"\n",
    "    },\n",
    "\n",
    "    # Refer to refer to https://highway-env.farama.org/actions/ to change action space type\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteMetaAction\",\n",
    "    },\n",
    "\n",
    "    # Parameterization bellow can be changed (as it refers mostly to the reward system)\n",
    "    # \"collision_reward\": -10,  # The reward received when colliding with a vehicle. (Can be changed)\n",
    "    # \"reward_speed_range\": [20, 30],  # [m/s] The reward for high speed is mapped linearly from this range to [0, HighwayEnv.HIGH_SPEED_REWARD]. (Can be changed)\n",
    "    \"simulation_frequency\": 15, #15,  # [Hz] (Can be changed)\n",
    "    \"policy_frequency\": 5, #5,  # [Hz] (Can be changed)\n",
    "\n",
    "    \"collision_reward\": -100,  # The reward received when colliding with a vehicle.\n",
    "    \"right_lane_reward\": 0.1,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
    "    # zero for other lanes.\n",
    "    \"high_speed_reward\": 100,  # The reward received when driving at full speed, linearly mapped to zero for\n",
    "    # lower speeds according to config[\"reward_speed_range\"].\n",
    "    \"lane_change_reward\": 0,  # The reward received at each lane change action.\n",
    "    \"reward_speed_range\": [20, 30],\n",
    "    \n",
    "    # Parameters defined bellow are purely for visualiztion purposes! You can alter them as you please\n",
    "    \"screen_width\": 800,  # [px]\n",
    "    \"screen_height\": 600,  # [px]\n",
    "    \"centering_position\": [0.5, 0.5],\n",
    "    \"scaling\": 5,\n",
    "    \"show_trajectories\": True,\n",
    "    \"render_agent\": True,\n",
    "    \"offscreen_rendering\": False\n",
    "}\n",
    "\n",
    "default_config = configuration.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 91.46669937980411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occupancyGrid = configuration.copy()\n",
    "occupancyGrid[\"observation\"] =  {\n",
    "    \"type\": \"OccupancyGrid\",\n",
    "    # \"vehicles_count\": 50,\n",
    "    \"features\": [\n",
    "                \"presence\",\n",
    "                #\"x\", \"y\", \n",
    "                #\"vx\", \"vy\"\n",
    "                ],\n",
    "    # \"features_range\": {\n",
    "    #      \"x\": [-500, 500],\n",
    "    #      \"y\": [-500, 500],\n",
    "    #     \"vx\": [-20, 20],\n",
    "    #     \"vy\": [-20, 20]\n",
    "    # },\n",
    "    \"grid_size\": [[-100, 100], [-100, 100]],    # X controls how many lanes, Y controls how far ahead\n",
    "    \"grid_step\": [1, 1],\n",
    "    #\"absolute\": False,                     # Not implemented in the library\n",
    "    #\"as_image\": True,\n",
    "    # \"align_to_vehicle_axes\" : True\n",
    "}\n",
    "\n",
    "# The higher the number, the more frequent the policy and the simulation frequencies, the slower the simulation\n",
    "occupancyGrid[\"simulation_frequency\"] = 15\n",
    "occupancyGrid[\"policy_frequency\"] = 1\n",
    "occupancyGrid[\"normalize_reward\"] = False\n",
    "\n",
    "env = gym.make('highway-v0', render_mode='human', config=occupancyGrid)\n",
    "\n",
    "obs, info = env.reset(seed = 30)\n",
    "# Do one step\n",
    "action = 3\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "print(\"Reward:\", reward)\n",
    "\n",
    "env.close()\n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.06666666666667 50.06666666666667\n",
      "91.52225493535967 141.58892160202635\n",
      "98.60652722998472 240.19544883201107\n",
      "99.81714601602411 340.0125948480352\n",
      "-99.93333333333334 240.07926151470184\n"
     ]
    }
   ],
   "source": [
    "# Generate an episode of the environment and show the rewards and cumulative rewards\n",
    "cum_reward = 0\n",
    "with gym.make(\"highway-v0\", config=occupancyGrid, render_mode='human') as env:\n",
    "    obs = env.reset()\n",
    "    for _ in range(1000):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "        print(reward, cum_reward)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccupancyGrid():\n",
    "    def __init__(\n",
    "            self,\n",
    "            grid_size=[[-50, 50], [-50, 50]],  # X controls the lane-width, Y controls how far ahead\n",
    "            grid_step=[1, 1],\n",
    "            colision_reward=-100,\n",
    "            skew_speed=1,\n",
    "            policy=None,\n",
    "            sim_frequency=10,\n",
    "            policy_frequency=1,\n",
    "            render_mode = 'human',\n",
    "            seed = 50,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Occupancy view class constructor\n",
    "        Arguments:\n",
    "            grid_size: list of lists, the size of the grid in the x and y direction, where x controls the lane-width and y controls how far ahead. Lanes are 5m wide, and the car position is (0,0)\n",
    "            grid_step: list, the step size of the grid in the x and y direction, in meters\n",
    "            colision_reward: float, the reward to give when a colision occurs\n",
    "            skew_speed: float, the skew speed to apply to the reward\n",
    "            policy: function, the policy to use in the simulation\n",
    "            sim_frequency: int, the frequency of the simulation\n",
    "            policy_frequency: int, the frequency of the policy\n",
    "            render_mode: str, the mode to render the simulation\n",
    "            seed: int, the seed to use in the simulation\n",
    "        \"\"\"\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.grid_step = grid_step\n",
    "        self.config = default_config.copy()\n",
    "        self.config[\"observation\"] =  {\n",
    "            \"type\": \"OccupancyGrid\",\n",
    "            \"features\": [\"presence\"],\n",
    "            \"grid_size\": grid_size,    # X controls how many lanes, Y controls how far ahead\n",
    "            \"grid_step\": grid_step,\n",
    "        }\n",
    "        self.config[\"simulation_frequency\"] = sim_frequency\n",
    "        self.config[\"policy_frequency\"] = policy_frequency\n",
    "        self.render_mode = render_mode\n",
    "        self.seed = seed\n",
    "        self.policy = policy\n",
    "        self.colision_reward = colision_reward\n",
    "        self.skew_speed = skew_speed\n",
    "        self.initialize_states()\n",
    "\n",
    "\n",
    "    def initialize_states(self):\n",
    "        \"\"\"\n",
    "        Initialize the states of the occupancy grid\n",
    "        \"\"\"\n",
    "        # Start the environment\n",
    "        with gym.make('highway-v0', render_mode=self.render_mode, config=self.config) as env:\n",
    "            obs, info = env.reset(seed = self.seed)\n",
    "            self.current_obs = obs\n",
    "            self.env = env\n",
    "        \n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Get the state of the environment\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def test_env(self):\n",
    "        \"\"\"\n",
    "        Function to test the environment with a random policy, or with a policy\n",
    "        \"\"\"\n",
    "        obs, info = self.env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        while not done:\n",
    "            # start = time.time()\n",
    "            if self.policy is None:\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                action = self.policy()\n",
    "            obs, reward, done, truncate, info = self.env.step(action)\n",
    "            self.current_obs = obs\n",
    "            print(self.get_state(type='lane-wise', decode=True), decode_meta_action(action), reward)\n",
    "            time.sleep(1)\n",
    "            # end = time.time()\n",
    "            # print(f\"Time taken: {end-start}\")\n",
    "        self.env.close()\n",
    "        return info[\"score\"]\n",
    "\n",
    "# ------------------- SARSA -------------------    \n",
    "class Sarsa(OccupancyGrid):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha=0.75,\n",
    "        gamma=0.95,\n",
    "        epsilon=0.6,\n",
    "        print_stats=False,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        SARSA class constructor\n",
    "        Arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            m: int, the number of episodes to train the agent for\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.initialize_Q(print_stats)\n",
    "        self.alpha, self.gamma, self.epsilon = alpha, gamma, epsilon\n",
    "\n",
    "    def policy_Q(self, state):\n",
    "        values = [self.Q[(state, action)] for action in range(5)]\n",
    "        return np.argmax(values)   \n",
    "\n",
    "    def initialize_Q(self, print_stats = False):\n",
    "        # Combine the possible states with the possible actions\n",
    "        keys = list(itertools.product(self.states, range(5)))       # 5 possible actions, 0-4: left, idle, right, accelerate, decelerate\n",
    "        if len(keys) > 150000:\n",
    "            print(\"Warning: The number of states is too large, consider reducing the number of states\")\n",
    "        if print_stats:\n",
    "            print(f\"Number of states: {len(keys)}\")   \n",
    "        self.Q = {key: 0 for key in keys}\n",
    "\n",
    "    def epsilon_greedy(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(5)\n",
    "        else:\n",
    "            values = [self.Q[(state, action)] for action in range(5)]\n",
    "            return np.argmax(values)\n",
    "        \n",
    "    def train(self, m=100, verbose=0): \n",
    "        env = gym.make('highway-v0', render_mode=None, config=self.config)\n",
    "        obs, info = env.reset(seed = np.random.randint(1000))\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        state = self.get_state()\n",
    "        action = self.epsilon_greedy(state)\n",
    "        q_explored = np.count_nonzero(list(self.Q.values()))\n",
    "        for i in tqdm(range(m)):\n",
    "            env.reset()\n",
    "            cum_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                next_state = self.get_state()\n",
    "                next_action = self.epsilon_greedy(next_state)\n",
    "\n",
    "                print(next_state, decode_meta_action(next_action), reward) if verbose > 2 else None\n",
    "                if self.Q[(state, action)] == 0:\n",
    "                    q_explored += 1\n",
    "                cum_reward += reward\n",
    "\n",
    "                self.Q[(state, action)] += self.alpha*(reward + self.gamma*self.Q[(next_state, next_action)] - self.Q[(state, action)])\n",
    "                state, action = next_state, next_action\n",
    "                self.current_obs = next_obs\n",
    "            print(f\"Episode {i+1} completed, cumulative reward: {cum_reward}\") if verbose > 0 else None\n",
    "            print(f\"Q explored: {100*q_explored/len(self.Q)}\") if verbose > 1 else None\n",
    "        env.close()\n",
    "\n",
    "    def test(self):\n",
    "        env = gym.make('highway-v0', render_mode=self.render_mode, config=self.config)\n",
    "        obs, info = env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        state = self.get_state(self.state_type)\n",
    "        action = self.policy_Q(state)\n",
    "        while not done:\n",
    "            next_obs, reward, done, truncate, info = env.step(action)\n",
    "            next_state = self.get_state(self.state_type)\n",
    "            next_action = self.policy_Q(next_state)\n",
    "            state, action = next_state, next_action\n",
    "            self.current_obs = next_obs\n",
    "            print(next_state)\n",
    "        env.close()\n",
    "        return info[\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- grid_size=[[-50, 50], [-50, 50]],  # X controls the lane-width, Y controls how far ahead\n",
    "- grid_step=[1, 1],\n",
    "- n_closest=3,\n",
    "- ss_bins=[5,6],\n",
    "- crop_dist=[[-10,10], [-10,25]],\n",
    "- policy=None,\n",
    "- sim_frequency=15,\n",
    "- policy_frequency=5,\n",
    "- render_mode = 'human',\n",
    "- seed = 50,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "copyQ = a.Q.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 540\n"
     ]
    }
   ],
   "source": [
    "a = Sarsa(print_stats=True, epsilon=0.6, alpha=0.3, gamma=0.8)\n",
    "a.Q = copyQ.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! TODO !!!\n",
    "\n",
    "- Use kinematics to substitute the current occupancy grid class\n",
    "\n",
    "- He cannot know when he cant turn left or right because the lane is the final one\n",
    "\n",
    "- Change the occupancy class to use occupancy grid with 5m grid size, 3 lanes and -30, 30 ahead\n",
    "\n",
    "- Make a plot history \n",
    "\n",
    "- Check how many times each state is visited\n",
    "\n",
    "- Check the action distribution, so as to see if slowing down is the most chosen action and the one with the best Q value\n",
    "\n",
    "- Change the reward for colision, so that the agent goes faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! IDEA WITH KINEMATICS !!!\n",
    "- State space in this maner: (danger ahead, danger left, danger right, danger behind, lane position, (maybe) speed)\n",
    "- Lane position can be 0 if in the middle, 1 if in the right, -1 if in the left\n",
    "\n",
    "- We need speed, because if the speed is too fast, it might not be able to turn in time\n",
    "- Actually, we could just increase the safety distance, and then we wouldn't need speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c280dc5269724502bed494f225b52c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[106], line 307\u001b[0m, in \u001b[0;36mSarsa.train\u001b[1;34m(self, m, verbose)\u001b[0m\n\u001b[0;32m    305\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m--> 307\u001b[0m     next_obs, reward, done, truncate, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     reward \u001b[38;5;241m=\u001b[39m fix_reward(reward, done\u001b[38;5;241m=\u001b[39mdone, colision_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolision_reward, skew_speed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskew_speed)\n\u001b[0;32m    309\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:257\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_control\u001b[39m\u001b[38;5;124m\"\u001b[39m] \\\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:324\u001b[0m, in \u001b[0;36mRoad.act\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decide the actions of each entity on the road.\"\"\"\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[1;32m--> 324\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:95\u001b[0m, in \u001b[0;36mIDMVehicle.act\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_road()\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_lane_change:\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchange_lane_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteering_control(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_lane_index)\n\u001b[0;32m     97\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:214\u001b[0m, in \u001b[0;36mIDMVehicle.change_lane_policy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# Does the MOBIL model recommend a lane change?\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmobil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlane_index\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_lane_index \u001b[38;5;241m=\u001b[39m lane_index\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:236\u001b[0m, in \u001b[0;36mIDMVehicle.mobil\u001b[1;34m(self, lane_index)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Do I have a planned route for a specific lane which is safe for me to access?\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m old_preceding, old_following \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneighbour_vehicles\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m self_pred_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macceleration(ego_vehicle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, front_vehicle\u001b[38;5;241m=\u001b[39mnew_preceding)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroute \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroute[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# Wrong direction\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:361\u001b[0m, in \u001b[0;36mRoad.neighbour_vehicles\u001b[1;34m(self, vehicle, lane_index)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vehicle \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, Landmark):  \u001b[38;5;66;03m# self.network.is_connected_road(v.lane_index,\u001b[39;00m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;66;03m# lane_index, same_lane=True):\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m         s_v, lat_v \u001b[38;5;241m=\u001b[39m \u001b[43mlane\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lane\u001b[38;5;241m.\u001b[39mon_lane(v\u001b[38;5;241m.\u001b[39mposition, s_v, lat_v, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    363\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:190\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    188\u001b[0m delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m    189\u001b[0m longitudinal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection)\n\u001b[1;32m--> 190\u001b[0m lateral \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a.train(m=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.37037037037037\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for keys, values in a.Q.items():\n",
    "    if values != 0:\n",
    "        count += 1\n",
    "\n",
    "print(100*count/len(a.Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: LANE_LEFT, Q-value: -46.81659543807035\n",
      "Action: IDLE, Q-value: -13.458657418241788\n",
      "Action: LANE_RIGHT, Q-value: -12.65590344059557\n",
      "Action: FASTER, Q-value: -7.4392318919133285\n",
      "Action: SLOWER, Q-value: -8.453300283131032\n"
     ]
    }
   ],
   "source": [
    "# Check the values for Q for this state (15, 30, 18, 18)\n",
    "state = (30, 30, 18, 18)\n",
    "for action in range(5):\n",
    "    print(f\"Action: {decode_meta_action(action)}, Q-value: {a.Q[(state, action)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 30, 18, 18)\n",
      "(30, 30, 18, 6)\n",
      "(30, 30, 18, 18)\n",
      "(15, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(30, 30, 18, 18)\n",
      "(30, 30, 18, 6)\n",
      "(30, 30, 18, 12)\n",
      "(15, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 12)\n",
      "(5, 30, 18, 12)\n",
      "(5, 30, 18, 12)\n",
      "(10, 30, 18, 12)\n",
      "(10, 30, 18, 12)\n",
      "(30, 30, 12, 6)\n",
      "(15, 30, 18, 12)\n",
      "(10, 30, 18, 12)\n",
      "(10, 30, 18, 6)\n",
      "(30, 10, 6, 6)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[106], line 339\u001b[0m, in \u001b[0;36mSarsa.test\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mprint\u001b[39m(next_state)\n\u001b[0;32m    338\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'score'"
     ]
    }
   ],
   "source": [
    "a.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_occupancy = OcupancyGrid(render_mode=None)\n",
    "# print(new_occupancy.x_bins, new_occupancy.y_bins, len(new_occupancy.states))\n",
    "# new_occupancy.test_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________\n",
    "### Using kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 83.56591  ,  28.       ,  25.       ,   0.       ],\n",
       "       [  9.4535885,   8.       ,  -1.5146654,   0.       ],\n",
       "       [ 20.187185 , -24.       ,  -1.9328905,   0.       ],\n",
       "       [ 31.133034 , -24.       ,  -1.129222 ,   0.       ],\n",
       "       [ 41.896053 ,   0.       ,  -1.7400944,   0.       ],\n",
       "       [ 51.945835 ,  -8.       ,  -3.565016 ,   0.       ],\n",
       "       [ 62.276524 , -16.       ,  -3.3216567,   0.       ],\n",
       "       [ 72.60146  ,   8.       ,  -1.0905089,   0.       ],\n",
       "       [ 82.24522  ,  -8.       ,  -2.182405 ,   0.       ],\n",
       "       [ 91.352806 , -20.       ,  -3.5367248,   0.       ],\n",
       "       [100.35392  , -16.       ,  -3.2140508,   0.       ],\n",
       "       [109.795944 ,  -4.       ,  -3.2532372,   0.       ],\n",
       "       [119.59274  ,   0.       ,  -1.6719042,   0.       ],\n",
       "       [129.31784  ,   4.       ,  -2.6289418,   0.       ],\n",
       "       [138.54662  ,  -4.       ,  -2.0985248,   0.       ],\n",
       "       [149.64609  , -16.       ,  -1.698277 ,   0.       ],\n",
       "       [159.69713  ,   0.       ,  -1.612952 ,   0.       ],\n",
       "       [169.55093  ,   4.       ,  -2.264635 ,   0.       ],\n",
       "       [179.66725  ,   0.       ,  -3.794033 ,   0.       ],\n",
       "       [190.1403   ,   4.       ,  -2.7294507,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kinematics = configuration.copy()\n",
    "kinematics[\"observation\"] =  {\n",
    "    \"type\": \"Kinematics\",\n",
    "    \"vehicles_count\": 50,\n",
    "    \"features\": [\"x\", \"y\", \"vx\", \"vy\"],\n",
    "    # \"features_range\": {\n",
    "    #     \"x\": [-40, 40],\n",
    "    #     \"y\": [-40, 40],\n",
    "    #     \"vx\": [-200, 200],\n",
    "    #     \"vy\": [-200, 200]\n",
    "    # }, \n",
    "    \"absolute\": False,\n",
    "    \"normalize\": False,\n",
    "}\n",
    "\n",
    "# The higher the number, the more frequent the policy and the simulation frequencies, the slower the simulation\n",
    "kinematics[\"simulation_frequency\"] = 10\n",
    "kinematics[\"policy_frequency\"] = 2\n",
    "\n",
    "env = gym.make('highway-v0', render_mode='human', config=kinematics)\n",
    "\n",
    "obs, info = env.reset(seed = 10)\n",
    "\n",
    "env.close()\n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_reward(reward, position, to_right_reward=5, to_right_skewness=2, change_lane_reward=-0.5, past_position=None):\n",
    "    \"\"\"\n",
    "    This function is used to correct the reward function, which is not correctly outputted by the environment\n",
    "    Params: \n",
    "        reward: float, the reward to fix    \n",
    "        position: tuple, the position of the car\n",
    "        to_right_reward: float, the reward to give to the driver \n",
    "        to_right_skewness: float, the skewness to apply to the to_right_reward\n",
    "        change_lane_reward: float, the reward to give when changing lanes\n",
    "    \"\"\"\n",
    "    \n",
    "    lane = position[1]\n",
    "    lane_value = to_right_reward * ((lane/36)**to_right_skewness)\n",
    "\n",
    "    if past_position is not None:\n",
    "        past_lane = past_position[1]\n",
    "        if np.abs(past_lane - lane) > 1.5: \n",
    "            reward += change_lane_reward\n",
    "    \n",
    "    return reward + lane_value\n",
    "    \n",
    "\n",
    "def decode_meta_action(action):\n",
    "    \"\"\"\n",
    "    Function to output the corresponding action in text-form\n",
    "    \"\"\"\n",
    "    assert action in range(5), \"The action must be between 0 and 4\"\n",
    "    if action == 0:\n",
    "        return \"LANE_LEFT\"\n",
    "    elif action == 1:\n",
    "        return \"IDLE\"\n",
    "    elif action == 2:\n",
    "        return \"LANE_RIGHT\"\n",
    "    elif action == 3:\n",
    "        return \"FASTER\"\n",
    "    elif action == 4:\n",
    "        return \"SLOWER\"\n",
    "    \n",
    "def decode_danger(state):\n",
    "    \"\"\"\n",
    "    Function to decode the danger state\n",
    "    \"\"\"\n",
    "    state_meaning = ['front', 'back', 'left', 'right']\n",
    "    to_return = ''\n",
    "    for i in range(3): \n",
    "        if state[i] == 1:\n",
    "            if to_return == '':\n",
    "                to_return = 'Danger in '\n",
    "            to_return += state_meaning[i] + ', '\n",
    "    if to_return == '': \n",
    "        to_return = 'No danger'\n",
    "    if state[4] == -1:\n",
    "        to_return += '. Cant turn left'\n",
    "    elif state[4] == 1:\n",
    "        to_return += '. Cant turn right'\n",
    "    return to_return\n",
    "    \n",
    "\n",
    "def decode_Q(Q): \n",
    "    \"\"\"\n",
    "    Function to decode the Q-values\n",
    "    \"\"\"\n",
    "    return {(decode_danger(key[0]), decode_meta_action(key[1])) : value for key, value in Q.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "class ObservationType: \n",
    "    def __init__(self, \n",
    "                sim_frequency=10,\n",
    "                policy_frequency=2,\n",
    "                render_mode='human',\n",
    "                seed=None,\n",
    "                colision_reward=-20, high_speed_reward=5, reward_speed_range=[20, 30], to_right_reward=5, to_right_skewness=2, change_lane_reward=-0.5):\n",
    "\n",
    "        \"\"\"\n",
    "        Constructor for the ObservationType class\n",
    "        Arguments:\n",
    "            sim_frequency: int, the frequency of the simulation\n",
    "            policy_frequency: int, the frequency of the policy\n",
    "            render_mode: str, the mode to render the simulation\n",
    "            seed: int, the seed to use in the simulation\n",
    "            colision_reward: float, the reward to give when a colision occurs\n",
    "            high_speed_reward: float, the reward to give when driving at high speed\n",
    "            reward_speed_range: list, the range of speeds to give the high speed reward\n",
    "            to_right_reward: float, the reward to give when driving to the right, mapped polynomially with to_right_skewness\n",
    "            to_right_skewness: float, the skewness to apply to the to_right_reward\n",
    "            change_lane_reward: float, the reward to give when changing lanes\n",
    "        \"\"\"\n",
    "        self.config = default_config.copy()\n",
    "\n",
    "        self.config.update({\n",
    "            \"simulation_frequency\": sim_frequency,\n",
    "            \"policy_frequency\": policy_frequency,\n",
    "            \"collision_reward\": colision_reward,\n",
    "            \"high_speed_reward\": high_speed_reward,\n",
    "            \"reward_speed_range\": reward_speed_range\n",
    "        })\n",
    "        self.config['normalize_reward'] = False\n",
    "        self.seed = seed\n",
    "        self.render_mode = render_mode\n",
    "        self.to_right_reward, self.to_right_skewness, self.change_lane_reward = to_right_reward, to_right_skewness, change_lane_reward\n",
    "\n",
    "\n",
    "class Kinematics(ObservationType):\n",
    "    def __init__(self, \n",
    "                 seed=None, \n",
    "                 state_type='danger',\n",
    "                 policy=None,\n",
    "                 crop=100, lane_tolerance=2, danger_threshold_x=10, danger_threshold_y=15, x_speed_coef=1, y_speed_coef=1,\n",
    "                 **kwargs):\n",
    "\n",
    "            \"\"\"\n",
    "            Kinematics class constructor\n",
    "            Arguments:\n",
    "                seed: int, the seed to use in the test environment\n",
    "                state_type: str, the type of state to use. Options are 'n_neighbours' or 'danger'\n",
    "                policy: function, the policy to use in the simulation\n",
    "                crop: int, the crop distance to use in the state\n",
    "                lane_tolerance: int, the tolerance to use in the lane\n",
    "                danger_threshold_x: float, the threshold to use in the x direction for the danger state\n",
    "                danger_threshold_y: float, the threshold to use in the y direction for the danger state\n",
    "                x_speed_coef: float, the coefficient to use in the x direction for the danger state\n",
    "                y_speed_coef: float, the coefficient to use in the y direction for the danger state\n",
    "            \n",
    "            Other Arguments (for Observation Type):\n",
    "                sim_frequency: int, the frequency of the simulation\n",
    "                policy_frequency: int, the frequency of the policy\n",
    "                render_mode: str, the mode to render the simulation\n",
    "                seed: int, the seed to use in the simulation\n",
    "                colision_reward: float, the reward to give when a colision occurs\n",
    "                high_speed_reward: float, the reward to give when driving at high speed\n",
    "                reward_speed_range: list, the range of speeds to give the high speed reward\n",
    "                to_right_reward: float, the reward to give when driving to the right, mapped polynomially with to_right_skewness\n",
    "                to_right_skewness: float, the skewness to apply to the to_right_reward\n",
    "                change_lane_reward: float, the reward to give when changing lanes\n",
    "            \"\"\"\n",
    "            super().__init__(**kwargs)\n",
    "\n",
    "            self.config[\"observation\"] =  {\n",
    "                \"type\": \"Kinematics\",\n",
    "                \"vehicles_count\": 50,\n",
    "                \"features\": [\"x\", \"y\", \"vx\", \"vy\"],\n",
    "                \"absolute\": False,\n",
    "                \"normalize\": False,\n",
    "            }\n",
    "\n",
    "            self.policy = policy\n",
    "\n",
    "            with gym.make('highway-v0', render_mode=self.render_mode, config=self.config) as env:\n",
    "                self.env = env\n",
    "                obs, info = env.reset(seed = self.seed)\n",
    "                self.current_obs = obs\n",
    "\n",
    "            self.state_type = state_type\n",
    "            self.crop, self.lane_tolerance, self.danger_threshold_x, self.danger_threshold_y, self.x_speed_coef, self.y_speed_coef = crop, lane_tolerance, danger_threshold_x, danger_threshold_y, x_speed_coef, y_speed_coef\n",
    "            self.initialize_states()\n",
    "\n",
    "    def initialize_states(self):\n",
    "        \"\"\"\n",
    "        Initialize the states of the occupancy grid\n",
    "        \"\"\"\n",
    "        # If the state type is danger, we need to initialize the states\n",
    "        if self.state_type == 'danger':\n",
    "            # The states will be the possible combinations of 0s and 1s for the 4 features + {-1,0,1} for the lane \n",
    "            a = list(itertools.product([0, 1], repeat=4))\n",
    "            # Now we need to add the product of {-1,0,1}\n",
    "            a = list(itertools.product(a, [-1, 0, 1]))\n",
    "            flattened = [(*x, y) for x, y in a]\n",
    "            self.states = flattened\n",
    "\n",
    "        elif self.state_type == 'binned': \n",
    "            pass\n",
    "\n",
    "    def get_state(self, decode=False):\n",
    "        \"\"\"\n",
    "        Get the state of the environment\n",
    "        Arguments:\n",
    "            type: str, the type of state to get. Options are 'n_neighbours' or 'danger' :\n",
    "                n_neighbours: the state is the n closest neighbours in the form (x1,x2,...,xn), (y1,y2,...,yn)\n",
    "                danger: the state is an array with 4 binary variables representing whether there is danger ahead, behind, on the left or right lanes\n",
    "            decode: bool, whether to decode the state\n",
    "        \"\"\"\n",
    "        assert self.state_type in ['n_neighbours', 'danger'], \"The type of state must be either 'n_neighbours' or 'lane-wise'\"\n",
    "        if self.state_type == 'n_neighbours':\n",
    "            state = self.state_n_neighbours()\n",
    "        elif self.state_type == 'danger':\n",
    "            state = self.state_danger()\n",
    "        return state\n",
    "\n",
    "    def state_danger(self):\n",
    "\n",
    "        global values_x\n",
    "        global values_y\n",
    "        global turn_possibility\n",
    "        \"\"\"\n",
    "        Get the state of the environment\n",
    "        \"\"\"\n",
    "        def get_sign(num): \n",
    "            sign = num/np.abs(num)\n",
    "            return sign\n",
    "\n",
    "        lane = self.current_obs[0,1]\n",
    "        observation = self.current_obs[1:][:,0:4]\n",
    "        observation = observation[~np.all(observation == 0, axis=1)]\n",
    "\n",
    "        # Lane observations\n",
    "        same_lane = observation[np.abs(observation[:,1]) <= self.lane_tolerance]\n",
    "        lane_front = same_lane[(same_lane[:,0] > 0)]\n",
    "        lane_back = same_lane[(same_lane[:,0] < 0)]\n",
    "\n",
    "        # For the left and right lanes we consider 2 lanes, instead of just one \n",
    "        left_lanes = observation[(observation[:,1] >= -8 - self.lane_tolerance) & (observation[:,1] <= -4 + self.lane_tolerance)]\n",
    "        right_lanes = observation[(observation[:,1] <= 8 + self.lane_tolerance) & (observation[:,1] >= 4 - self.lane_tolerance)]\n",
    "\n",
    "        # Calculating the adjusted distances\n",
    "        front_dist = lane_front[0,0] if len(lane_front) > 0 else self.crop \n",
    "        front_speed_diff = lane_front[0,2] if len(lane_front) > 0 else 0\n",
    "        front_adj_dist = front_dist + self.x_speed_coef*front_speed_diff   # The more the front speed diff, the harder it is to get to the car in the front, so adjusted distance is higher\n",
    "\n",
    "        back_dist = -lane_back[0,0] if len(lane_back) > 0 else self.crop\n",
    "        back_speed_diff = lane_back[0,2] - 5 if len(lane_back) > 0 else 0\n",
    "        back_adj_dist = back_dist - self.x_speed_coef*back_speed_diff    # The faster the car in the back is driving, the more dangerous it is, so the adjusted distance is lower\n",
    "\n",
    "        left_signs = [get_sign(left_lanes[i,0]) for i in range(len(left_lanes))]\n",
    "        left_adj_dists = (np.abs(left_lanes[:,1])-4)*1.5 + left_lanes[:,0] + self.x_speed_coef*left_lanes[:,2]*left_signs - self.y_speed_coef*left_lanes[:,3]\n",
    "        left_adj_dist = np.min(left_adj_dists) if len(left_adj_dists) > 0 else self.crop\n",
    "\n",
    "        right_signs = [get_sign(right_lanes[i,0]) for i in range(len(right_lanes))]\n",
    "        right_adj_dists = (np.abs(right_lanes[:,1])-4)*1.5 + right_lanes[:,0] + self.x_speed_coef*right_lanes[:,2]*right_signs + self.y_speed_coef*right_lanes[:,3]\n",
    "        right_adj_dist = np.min(right_adj_dists) if len(right_adj_dists) > 0 else self.crop\n",
    "\n",
    "        turn_possibility = -1 if lane < 2 else 1 if lane > 34 else 0\n",
    "        values = np.array([front_adj_dist, back_adj_dist, left_adj_dist, right_adj_dist])\n",
    "\n",
    "        # Use the danger threshold to make 0 or 1 \n",
    "        values_x = np.where(values[:2] < self.danger_threshold_x, 1, 0)\n",
    "        values_y = np.where(values[2:] < self.danger_threshold_y, 1, 0)\n",
    "\n",
    "        values = np.append(values_x, values_y)\n",
    "        values = np.append(values, turn_possibility)\n",
    "        return tuple(values)\n",
    "\n",
    "    def get_n_closest(self):\n",
    "        \"\"\"\n",
    "        Get the n closest cars to the agent\n",
    "        Returns:\n",
    "            closest_car_positions: np.array, the positions of the n closest cars to the agent. If there are less than n_closest cars, the array is padded with the crop_dist values\n",
    "        \"\"\"\n",
    "\n",
    "        car_positions = self.get_car_positions()\n",
    "        distances = np.linalg.norm(car_positions, axis=1)\n",
    "\n",
    "        # Remove the agent position\n",
    "        closest = np.argsort(distances)[1:self.n_closest+1]\n",
    "        closest_car_positions = car_positions[closest]\n",
    "\n",
    "        # If there are less than n_closest cars, pad the array with the crop_dist values\n",
    "        if len(closest_car_positions) < self.n_closest:\n",
    "            n_missing = self.n_closest - len(closest_car_positions)\n",
    "            closest_car_positions = np.pad(closest_car_positions, ((0, n_missing), (0,0)), 'constant', constant_values=(self.crop_dist[0][0], self.crop_dist[1][0]))\n",
    "\n",
    "        # Values that are \n",
    "        return closest_car_positions\n",
    "    \n",
    "\n",
    "    def state_n_neighbours(self):\n",
    "        \"\"\"\n",
    "        Get the state of the environment in a neighbour-wise manner\n",
    "        Returns:\n",
    "            state: tuple, the state of the environment in the form (x1,x2,...,xn), (y1,y2,...,yn)\n",
    "        \"\"\"\n",
    "        n_closest = self.get_n_closest()\n",
    "        # For the closest cars, get the state\n",
    "        state_x, state_y = [], []\n",
    "        # Get the bin values for each of the x,y positions, and return a tuple with the values\n",
    "        for car in n_closest:\n",
    "            x = np.digitize(car[0], self.x_bins) - 1\n",
    "            y = np.digitize(car[1], self.y_bins) - 1\n",
    "            x_val, y_val = self.x_bins[x], self.y_bins[y]\n",
    "            state_x.append(x_val)\n",
    "            state_y.append(y_val)\n",
    "        state = (tuple(state_x), tuple(state_y))\n",
    "        return tuple(state)\n",
    "    \n",
    "    @staticmethod\n",
    "    def bin_values(values, bins, digitize=False):\n",
    "        \"\"\"\n",
    "        Function to bin the values\n",
    "        Arguments:\n",
    "            values: np.array, the values to bin\n",
    "            bins: list, the bins to use\n",
    "                Example: bins = [[5,10,15,30], [5,10,30], [8,14,20], [8,14,20]], for x,y,vx,vy\n",
    "            digitize: bool, whether to digitize the values. If set to False, the values will be returned as the bin index\n",
    "        Returns:\n",
    "            binned_values: np.array, the binned values\n",
    "        \"\"\"\n",
    "        # Check if there are as much bins as values\n",
    "        if len(values.shape) == 1:\n",
    "            assert len(bins) == len(values), \"The number of bins must be equal to the number of values\"\n",
    "            binned_values = []\n",
    "            if digitize:\n",
    "                binned_values = [np.digitize(values[i], bins[i]) for i in range(len(bins))]\n",
    "                return np.array(binned_values)\n",
    "\n",
    "            binned_values = [bins[i][np.digitize(values[i], bins[i])-1] for i in range(len(bins))]\n",
    "            return np.array(binned_values)\n",
    "\n",
    "        # For a NxM matrix, with N>1\n",
    "        assert len(bins) == values.shape[1], \"The number of bins must be equal to the number of values\"\n",
    "        binned_values = []\n",
    "        for i in range(len(values)):\n",
    "            if digitize:\n",
    "                binned_values.append([np.digitize(values[i,j], bins[j]) for j in range(len(bins))])\n",
    "            else:\n",
    "                binned_values.append([bins[j][np.digitize(values[i,j], bins[j])-1] for j in range(len(bins))])\n",
    "        return np.array(binned_values)\n",
    "\n",
    "    \n",
    "    def test_env(self, sleep_time=0.2):\n",
    "        \"\"\"\n",
    "        Function to test the environment with a random policy, or with a policy\n",
    "        \"\"\" \n",
    "        obs, info = self.env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        while not done:\n",
    "            # start = time.time()\n",
    "            if self.policy is None:\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                action = self.policy()\n",
    "            obs, reward, done, truncate, info = self.env.step(action)\n",
    "            speed = obs[0,2]\n",
    "            reward = fix_reward(reward, obs[0], self.to_right_reward, self.to_right_skewness, self.change_lane_reward, self.current_obs[0])\n",
    "            self.current_obs = obs\n",
    "            print(self.get_state(), speed, decode_meta_action(action), reward)\n",
    "            time.sleep(sleep_time)\n",
    "            # end = time.time()\n",
    "            # print(f\"Time taken: {end-start}\")\n",
    "        self.env.close()\n",
    "\n",
    "        \n",
    "class Algorithm(Kinematics):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha=0.75,\n",
    "        gamma=0.95,\n",
    "        epsilon=0.6,\n",
    "        epsilon_decay=1, min_epsilon=0.05,\n",
    "        print_stats=False,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Algorithm class constructor\n",
    "        Arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            epsilon_decay: float, the decay value for epsilon. If set to 1, the epsilon will not decay\n",
    "            min_epsilon: float, the minimum value for epsilon. If the epsilon is lower than this value, it will not decay\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \n",
    "        Other Arguments (for the Kinematics observation):\n",
    "            seed: int, the seed to use in the test environment\n",
    "            state_type: str, the type of state to use. Options are 'n_neighbours' or 'danger'\n",
    "            policy: function, the policy to use in the simulation\n",
    "            crop: int, the crop distance to use in the state\n",
    "            lane_tolerance: int, the tolerance to use in the lane\n",
    "            danger_threshold_x: float, the threshold to use in the x direction for the danger state\n",
    "            danger_threshold_y: float, the threshold to use in the y direction for the danger state\n",
    "            x_speed_coef: float, the coefficient to use in the x direction for the danger state\n",
    "            y_speed_coef: float, the coefficient to use in the y direction for the danger state\n",
    "\n",
    "        Other Arguments (for Observation Type):\n",
    "            sim_frequency: int, the frequency of the simulation\n",
    "            policy_frequency: int, the frequency of the policy\n",
    "            render_mode: str, the mode to render the simulation\n",
    "            seed: int, the seed to use in the simulation\n",
    "            colision_reward: float, the reward to give when a colision occurs\n",
    "            high_speed_reward: float, the reward to give when driving at high speed\n",
    "            reward_speed_range: list, the range of speeds to give the high speed reward\n",
    "            to_right_reward: float, the reward to give when driving to the right, mapped polynomially with to_right_skewness\n",
    "            to_right_skewness: float, the skewness to apply to the to_right_reward\n",
    "            change_lane_reward: float, the reward to give when changing lanes\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.initialize_Q(print_stats)\n",
    "        self.alpha, self.gamma, self.epsilon = alpha, gamma, epsilon\n",
    "        self.epsilon_decay, self.min_epsilon = epsilon_decay, min_epsilon\n",
    "        self.Q_stats = self.Q.copy()\n",
    "        self.rewards_hist = []\n",
    "        self.hit_random = []\n",
    "\n",
    "    def policy_Q(self, state):\n",
    "        values = [self.Q[(state, action)] for action in range(5)]\n",
    "        return np.argmax(values)   \n",
    "    \n",
    "    def initialize_Q(self, print_stats = False):\n",
    "        # Combine the possible states with the possible actions\n",
    "        keys = list(itertools.product(self.states, range(5)))       # 5 possible actions, 0-4: left, idle, right, accelerate, decelerate\n",
    "        if len(keys) > 150000:\n",
    "            print(\"Warning: The number of states is too large, consider reducing the number of states\")\n",
    "        if print_stats:\n",
    "            print(f\"Number of states: {len(keys)}\")   \n",
    "        self.Q = {key: 0 for key in keys}\n",
    "\n",
    "    def epsilon_greedy(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(5), 1\n",
    "        else:\n",
    "            values = [self.Q[(state, action)] for action in range(5)]\n",
    "            return np.argmax(values), 0\n",
    "\n",
    "    def decay_epsilon(self, episode):\n",
    "        if self.epsilon > self.min_epsilon:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def test(self, sleep_time=1, time_after_crash=10):\n",
    "        with gym.make('highway-v0', render_mode='human', config=self.config) as env:\n",
    "            obs, info = env.reset(seed = self.seed)\n",
    "            self.current_obs = obs\n",
    "            done = False\n",
    "            state = self.get_state()\n",
    "            action = self.policy_Q(state)\n",
    "            while not done:\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                next_state = self.get_state()\n",
    "                next_action = self.policy_Q(next_state)\n",
    "                state, action = next_state, next_action\n",
    "                reward = fix_reward(reward, next_obs[0], self.to_right_reward, self.to_right_skewness, self.change_lane_reward, self.current_obs[0])\n",
    "                self.current_obs = next_obs\n",
    "                print(state, decode_meta_action(action), reward, next_state)\n",
    "                past_positon = obs[0]\n",
    "                time.sleep(sleep_time)\n",
    "            time.sleep(time_after_crash)\n",
    "\n",
    "    def get_state_visits(self, state=None):\n",
    "        state_visits = {state: np.sum([self.Q_stats[(state, action)] for action in range(5)]) for state in self.states}\n",
    "        state_visits = {k:100*v/np.sum(list(state_visits.values())) for k, v in sorted(state_visits.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return state_visits if state is None else state_visits[state]\n",
    "\n",
    "    def plot_rewards_history(self, smoothing=10):\n",
    "        plt.plot(self.rewards_hist, label='Original')\n",
    "        smoothing_window = np.ones(smoothing) / smoothing\n",
    "        padded_rewards = np.concatenate((np.zeros(smoothing - 1), self.rewards_hist))\n",
    "        smoothed_rewards = np.convolve(padded_rewards, smoothing_window, mode='valid')\n",
    "        plt.plot(smoothed_rewards, label='Smoothed')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Cumulative reward')\n",
    "        plt.title('Cumulative reward per episode')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_hit_random(self, smoothing=10):\n",
    "        smoothing_window = np.ones(smoothing) / smoothing\n",
    "        padded_random_hits = np.concatenate((np.zeros(smoothing - 1), self.rewards_hist))\n",
    "        smoothed_random_hits = np.convolve(padded_random_hits, smoothing_window, mode='valid')\n",
    "        plt.plot(smoothed_random_hits)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Hits')\n",
    "        plt.title('Smoothed random hits')\n",
    "        plt.show()\n",
    "\n",
    "    def search_Q(self, state):\n",
    "        assert len(state) == 5, \"The state must be a tuple of 5 values\"\n",
    "        Q_vals = {decode_meta_action(a) : self.Q[state, a] for a in range(5)}\n",
    "        # Order a \n",
    "        Q_vals = {k: v for k, v in sorted(Q_vals.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return Q_vals\n",
    "\n",
    "    def save(self, directory=\"saved_models\", prefix=\"model\", name=None):\n",
    "        \"\"\"Saves the entire model object to a pickle file.\"\"\"\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M\")\n",
    "        if name is None:\n",
    "            filename = f\"{prefix}_{self.algorithm_type}_{timestamp}.pkl\"\n",
    "        else: \n",
    "            filename = f\"{prefix}_{self.algorithm_type}_{name}.pkl\"\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(self, f) \n",
    "\n",
    "    @staticmethod\n",
    "    def load(filepath):\n",
    "        \"\"\"Loads the entire model object from a pickle file.\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            loaded_model = pickle.load(f)\n",
    "        return loaded_model \n",
    "\n",
    "\n",
    "class Sarsa(Algorithm):\n",
    "    def __init__(\n",
    "        self,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        SARSA class constructor\n",
    "        Algorithm arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            m: int, the number of episodes to train the agent for\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.algorithm_type = 'Sarsa'\n",
    "        \n",
    "    def train(self, m=100, verbose=0): \n",
    "        env = gym.make('highway-v0', render_mode=None, config=self.config)\n",
    "        obs, info = env.reset(seed = np.random.randint(1000))\n",
    "        self.current_obs = obs\n",
    "        done, state, action = False, self.get_state(), self.epsilon_greedy(state)\n",
    "        q_explored = np.count_nonzero(list(self.Q.values()))\n",
    "        for i in tqdm(range(m)):\n",
    "            last_state = None\n",
    "            env.reset(seed = np.random.randint(1000))\n",
    "            cum_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                reward = fix_reward(reward, next_obs[0], self.to_right_reward, self.to_right_skewness, self.change_lane_reward, self.current_obs[0])\n",
    "                next_state = self.get_state()\n",
    "                if done:\n",
    "                    last_state = state\n",
    "                    self.hit_random.append(random)\n",
    "\n",
    "                next_action, random = self.epsilon_greedy(next_state)\n",
    "                print(next_state, decode_meta_action(next_action), reward) if verbose > 2 else None\n",
    "                if self.Q[(state, action)] == 0:\n",
    "                    q_explored += 1\n",
    "                cum_reward += reward\n",
    "\n",
    "                self.Q[(state, action)] += self.alpha*(reward + self.gamma*self.Q[(next_state, next_action)] - self.Q[(state, action)])\n",
    "                self.Q_stats[(state, action)] += 1\n",
    "                state, action = next_state, next_action\n",
    "                self.current_obs = next_obs\n",
    "            self.rewards_hist.append(cum_reward)\n",
    "            print(f\"Episode {i+1} completed on state {last_state} with cumulative reward: {cum_reward}\") if verbose > 0 else None\n",
    "            print(f\"Q explored: {100*q_explored/len(self.Q)}. Epsilon: {self.epsilon}\") if verbose > 1 else None\n",
    "            self.decay_epsilon(i)\n",
    "        env.close()\n",
    "    \n",
    "\n",
    "class Q_learning(Algorithm):\n",
    "    def __init__(\n",
    "        self,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Q-learning class constructor\n",
    "        Arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            m: int, the number of episodes to train the agent for\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.algorithm_type = 'Q_learning'\n",
    "\n",
    "    def train(self, m=100, verbose=0):\n",
    "        env = gym.make('highway-v0', render_mode=None, config=self.config)\n",
    "        obs, info = env.reset(seed = np.random.randint(1000))\n",
    "        self.current_obs = obs\n",
    "        state, done = self.get_state(), False\n",
    "        q_explored = np.count_nonzero(list(self.Q.values()))\n",
    "        for i in tqdm(range(m)):\n",
    "            last_state = None\n",
    "            env.reset(seed = np.random.randint(1000))\n",
    "            cum_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, random = self.epsilon_greedy(state)\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                reward = fix_reward(reward, next_obs[0], self.to_right_reward, self.to_right_skewness, self.change_lane_reward, self.current_obs[0])\n",
    "                next_state = self.get_state()\n",
    "                if done:\n",
    "                    last_state = state\n",
    "                    self.hit_random.append(random)\n",
    "\n",
    "                print(next_state, decode_meta_action(action), reward) if verbose > 2 else None\n",
    "                if self.Q[(state, action)] == 0:\n",
    "                    q_explored += 1\n",
    "                cum_reward += reward\n",
    "\n",
    "                self.Q[(state, action)] += self.alpha*(reward + self.gamma*np.max([self.Q[(next_state, a)] for a in range(5)]) - self.Q[(state, action)])\n",
    "                self.Q_stats[(state, action)] += 1\n",
    "                state = next_state\n",
    "                self.current_obs = next_obs\n",
    "            self.rewards_hist.append(cum_reward)\n",
    "            print(f\"Episode {i+1} completed on state {last_state} with cumulative reward: {cum_reward}\") if verbose > 0 else None\n",
    "            print(f\"Q explored: {100*q_explored/len(self.Q)}. Epsilon: {self.epsilon}\") if verbose > 1 else None\n",
    "            self.decay_epsilon(i)\n",
    "        env.close()\n",
    "\n",
    "\n",
    "class MonteCarlo(Algorithm):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"Constructor for the MonteCarlo algorithm class.\"\"\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.algorithm_type = 'Q_learning'\n",
    "        self.returns = {}  # Store returns for each (state, action) pair\n",
    "        self.visits = {}   # Store visit counts for each (state, action) pair\n",
    "\n",
    "    def train(self, m=100, verbose=0):\n",
    "        \"\"\"Train the MonteCarlo agent.\"\"\"\n",
    "        env = gym.make('highway-v0', render_mode=None, config=self.config)\n",
    "\n",
    "        for i in tqdm(range(m)):\n",
    "            episode_history = []\n",
    "            cum_reward = 0\n",
    "\n",
    "            obs, info = env.reset(seed=np.random.randint(1000))\n",
    "            self.current_obs = obs\n",
    "            done = False\n",
    "            state = self.get_state()\n",
    "\n",
    "            while not done:\n",
    "                action, random = self.epsilon_greedy(state) \n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                if done:\n",
    "                    self.hit_random.append(random)\n",
    "\n",
    "                reward = fix_reward(reward, next_obs[0], self.to_right_reward, self.to_right_skewness, self.change_lane_reward, self.current_obs[0])\n",
    "                next_state = self.get_state()\n",
    "\n",
    "                episode_history.append((state, action, reward))\n",
    "\n",
    "                cum_reward += reward\n",
    "                state = next_state\n",
    "                self.current_obs = next_obs\n",
    "\n",
    "            G = 0\n",
    "            for state, action, reward in reversed(episode_history):\n",
    "                G = self.gamma * G + reward \n",
    "                if (state, action) not in [(s, a) for s, a, _ in episode_history[:-1]]: # First visit\n",
    "                    self.returns[(state, action)] = self.returns.get((state, action), 0) + G\n",
    "                    self.visits[(state, action)] = self.visits.get((state, action), 0) + 1\n",
    "                    self.Q[(state, action)] = self.returns[(state, action)] / self.visits[(state, action)]\n",
    "\n",
    "            self.rewards_hist.append(cum_reward)\n",
    "            print(f\"Episode {i+1} completed with cumulative reward: {cum_reward}. Epsilon: {self.epsilon}\") if verbose > 0 else None\n",
    "\n",
    "            self.decay_epsilon(i)\n",
    "            \n",
    "        env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kin = Kinematics(seed=10, \n",
    "                state_type='danger', \n",
    "                render_mode='human', \n",
    "                sim_frequency=10, \n",
    "                policy_frequency=2, \n",
    "                colision_reward=-50,\n",
    "                high_speed_reward=0,\n",
    "                reward_speed_range=[20, 30],\n",
    "                to_right_reward=5, to_right_skewness=2, change_lane_reward=-1)\n",
    "\n",
    "kin.test_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 240\n"
     ]
    }
   ],
   "source": [
    "# TODO learn the best danger thresholds !!\n",
    "\n",
    "Q = Q_learning(print_stats=True, \n",
    "        epsilon=0.9, \n",
    "        epsilon_decay=0.98,\n",
    "        min_epsilon=0.05,\n",
    "        alpha=0.1, \n",
    "        gamma=0.98, \n",
    "        state_type='danger', \n",
    "        policy_frequency=2, \n",
    "        sim_frequency=10, \n",
    "        danger_threshold_x=15,\n",
    "        danger_threshold_y=12.5,   # Changed to 12.5 from 10, bc the driver takes time to notice other cars on the right \n",
    "        x_speed_coef=1.2,\n",
    "        colision_reward=-75,\n",
    "        high_speed_reward=5,\n",
    "        reward_speed_range=[20, 30],\n",
    "        to_right_reward=2.5, \n",
    "        to_right_skewness=2,         # Skew more ? Increase to_right_reward and skew? \n",
    "        change_lane_reward=-0.25)    # Changing lanes too often (0,0,0,0,1) best action is lane left. Maybe skew more ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0f329da2a040968b240c35542f8a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 completed on state (0, 0, 1, 0, 0) with cumulative reward: 7.83718783753973\n",
      "Q explored: 4.583333333333333. Epsilon: 0.9\n",
      "Episode 2 completed on state (1, 0, 0, 1, -1) with cumulative reward: 58.08784736530964\n",
      "Q explored: 9.583333333333334. Epsilon: 0.882\n",
      "Episode 3 completed on state (1, 0, 1, 0, 0) with cumulative reward: -41.7621295495596\n",
      "Q explored: 11.666666666666666. Epsilon: 0.86436\n",
      "Episode 4 completed on state (0, 0, 1, 1, 0) with cumulative reward: 66.13875663201287\n",
      "Q explored: 16.25. Epsilon: 0.8470728\n",
      "Episode 5 completed on state (0, 0, 1, 1, 0) with cumulative reward: -28.079258889892955\n",
      "Q explored: 17.083333333333332. Epsilon: 0.830131344\n",
      "Episode 6 completed on state (0, 0, 1, 1, 0) with cumulative reward: 52.43635824896404\n",
      "Q explored: 17.083333333333332. Epsilon: 0.81352871712\n",
      "Episode 7 completed on state (0, 0, 0, 1, 0) with cumulative reward: -25.07162425190708\n",
      "Q explored: 17.916666666666668. Epsilon: 0.7972581427776\n",
      "Episode 8 completed on state (1, 0, 0, 1, 0) with cumulative reward: -20.33443889972397\n",
      "Q explored: 18.75. Epsilon: 0.781312979922048\n",
      "Episode 9 completed on state (0, 0, 0, 1, -1) with cumulative reward: 2.883020916574779\n",
      "Q explored: 18.75. Epsilon: 0.765686720323607\n",
      "Episode 10 completed on state (0, 0, 1, 1, 0) with cumulative reward: -15.448493088143444\n",
      "Q explored: 19.166666666666668. Epsilon: 0.7503729859171349\n",
      "Episode 11 completed on state (1, 0, 0, 0, 1) with cumulative reward: -44.590485808848015\n",
      "Q explored: 19.583333333333332. Epsilon: 0.7353655261987921\n",
      "Episode 12 completed on state (0, 0, 0, 1, 0) with cumulative reward: -61.01850748295179\n",
      "Q explored: 20.416666666666668. Epsilon: 0.7206582156748162\n",
      "Episode 13 completed on state (0, 0, 1, 1, 0) with cumulative reward: -59.12645086267318\n",
      "Q explored: 20.833333333333332. Epsilon: 0.7062450513613199\n",
      "Episode 14 completed on state (1, 0, 0, 1, 0) with cumulative reward: -15.494408065418057\n",
      "Q explored: 22.083333333333332. Epsilon: 0.6921201503340935\n",
      "Episode 15 completed on state (1, 0, 1, 1, 0) with cumulative reward: -51.24085330249504\n",
      "Q explored: 22.083333333333332. Epsilon: 0.6782777473274115\n",
      "Episode 16 completed on state (1, 0, 0, 1, 0) with cumulative reward: 2.665418673234285\n",
      "Q explored: 22.083333333333332. Epsilon: 0.6647121923808633\n",
      "Episode 17 completed on state (1, 0, 0, 0, 1) with cumulative reward: -35.482297986401015\n",
      "Q explored: 22.916666666666668. Epsilon: 0.6514179485332461\n",
      "Episode 18 completed on state (1, 0, 1, 0, 0) with cumulative reward: -0.1817158793121223\n",
      "Q explored: 24.166666666666668. Epsilon: 0.6383895895625812\n",
      "Episode 19 completed on state (0, 0, 1, 0, 0) with cumulative reward: -33.78283219065803\n",
      "Q explored: 24.166666666666668. Epsilon: 0.6256217977713295\n",
      "Episode 20 completed on state (1, 0, 1, 0, 0) with cumulative reward: -15.052090996250044\n",
      "Q explored: 24.166666666666668. Epsilon: 0.6131093618159029\n",
      "Episode 21 completed on state (1, 0, 0, 0, 0) with cumulative reward: -60.674822794633414\n",
      "Q explored: 24.583333333333332. Epsilon: 0.6008471745795848\n",
      "Episode 22 completed on state (0, 0, 0, 1, 0) with cumulative reward: -64.40296720614063\n",
      "Q explored: 24.583333333333332. Epsilon: 0.5888302310879932\n",
      "Episode 23 completed on state (0, 0, 1, 0, 0) with cumulative reward: 156.5153940802951\n",
      "Q explored: 25.416666666666668. Epsilon: 0.5770536264662333\n",
      "Episode 24 completed on state (0, 0, 0, 1, 0) with cumulative reward: 22.84833403710779\n",
      "Q explored: 25.833333333333332. Epsilon: 0.5655125539369086\n",
      "Episode 25 completed on state (0, 0, 1, 1, 0) with cumulative reward: 9.94235019583948\n",
      "Q explored: 26.25. Epsilon: 0.5542023028581704\n",
      "Episode 26 completed on state (0, 0, 1, 1, 0) with cumulative reward: 0.783446955350172\n",
      "Q explored: 26.25. Epsilon: 0.543118256801007\n",
      "Episode 27 completed on state (0, 0, 1, 0, 0) with cumulative reward: -25.66954552643751\n",
      "Q explored: 27.083333333333332. Epsilon: 0.5322558916649869\n",
      "Episode 28 completed on state (1, 0, 1, 1, 0) with cumulative reward: -34.23820447131676\n",
      "Q explored: 27.5. Epsilon: 0.5216107738316871\n",
      "Episode 29 completed on state (1, 0, 0, 1, 0) with cumulative reward: 16.325011515403915\n",
      "Q explored: 27.916666666666668. Epsilon: 0.5111785583550533\n",
      "Episode 30 completed on state (1, 0, 0, 0, 0) with cumulative reward: 98.89943638825483\n",
      "Q explored: 27.916666666666668. Epsilon: 0.5009549871879523\n",
      "Episode 31 completed on state (1, 0, 0, 0, 1) with cumulative reward: 15.813252555167182\n",
      "Q explored: 28.75. Epsilon: 0.49093588744419325\n",
      "Episode 32 completed on state (0, 0, 1, 0, 0) with cumulative reward: 92.08627498743354\n",
      "Q explored: 28.75. Epsilon: 0.4811171696953094\n",
      "Episode 33 completed on state (1, 0, 0, 0, 0) with cumulative reward: 12.328168797822272\n",
      "Q explored: 28.75. Epsilon: 0.47149482630140316\n",
      "Episode 34 completed on state (0, 0, 1, 0, 1) with cumulative reward: -68.60882378915169\n",
      "Q explored: 28.75. Epsilon: 0.4620649297753751\n",
      "Episode 35 completed on state (0, 0, 0, 1, 0) with cumulative reward: -58.94165665715159\n",
      "Q explored: 28.75. Epsilon: 0.4528236311798676\n",
      "Episode 36 completed on state (0, 0, 0, 1, 0) with cumulative reward: -11.869394068799863\n",
      "Q explored: 28.75. Epsilon: 0.44376715855627025\n",
      "Episode 37 completed on state (1, 0, 0, 1, 0) with cumulative reward: 60.7374000915958\n",
      "Q explored: 29.166666666666668. Epsilon: 0.4348918153851448\n",
      "Episode 38 completed on state (1, 0, 0, 0, 0) with cumulative reward: 46.43598234417355\n",
      "Q explored: 29.166666666666668. Epsilon: 0.42619397907744194\n",
      "Episode 39 completed on state (1, 0, 0, 0, 1) with cumulative reward: 71.07233708275058\n",
      "Q explored: 29.583333333333332. Epsilon: 0.4176700994958931\n",
      "Episode 40 completed on state (1, 0, 1, 1, 0) with cumulative reward: 4.459013406322711\n",
      "Q explored: 30.0. Epsilon: 0.40931669750597527\n",
      "Episode 41 completed on state (0, 0, 1, 1, 0) with cumulative reward: 24.240118902592513\n",
      "Q explored: 30.0. Epsilon: 0.40113036355585574\n",
      "Episode 42 completed on state (1, 0, 0, 0, 0) with cumulative reward: 69.93741198438995\n",
      "Q explored: 30.0. Epsilon: 0.3931077562847386\n",
      "Episode 43 completed on state (1, 0, 0, 1, 0) with cumulative reward: 85.06502865841628\n",
      "Q explored: 30.0. Epsilon: 0.3852456011590438\n",
      "Episode 44 completed on state (0, 0, 0, 1, 0) with cumulative reward: -41.46537602621354\n",
      "Q explored: 30.0. Epsilon: 0.3775406891358629\n",
      "Episode 45 completed on state (1, 0, 1, 0, 0) with cumulative reward: 100.55849259214328\n",
      "Q explored: 30.0. Epsilon: 0.36998987535314565\n",
      "Episode 46 completed on state (1, 0, 0, 1, 0) with cumulative reward: 192.50515643709684\n",
      "Q explored: 30.0. Epsilon: 0.3625900778460827\n",
      "Episode 47 completed on state (0, 0, 1, 1, 0) with cumulative reward: 3.4178924685588186\n",
      "Q explored: 30.0. Epsilon: 0.35533827628916104\n",
      "Episode 48 completed on state (1, 0, 1, 1, 0) with cumulative reward: 110.48314517128206\n",
      "Q explored: 30.833333333333332. Epsilon: 0.3482315107633778\n",
      "Episode 49 completed on state (0, 0, 0, 1, 0) with cumulative reward: -58.214157992686864\n",
      "Q explored: 31.25. Epsilon: 0.3412668805481103\n",
      "Episode 50 completed on state (0, 0, 1, 1, 0) with cumulative reward: -8.711568807540331\n",
      "Q explored: 31.25. Epsilon: 0.33444154293714806\n",
      "Episode 51 completed on state (1, 0, 0, 0, 0) with cumulative reward: -52.333775908263895\n",
      "Q explored: 31.666666666666668. Epsilon: 0.3277527120784051\n",
      "Episode 52 completed on state (1, 0, 1, 0, 1) with cumulative reward: 2.3865870902108526\n",
      "Q explored: 31.666666666666668. Epsilon: 0.321197657836837\n",
      "Episode 53 completed on state (0, 0, 1, 1, 0) with cumulative reward: 24.47518374385136\n",
      "Q explored: 31.666666666666668. Epsilon: 0.31477370468010024\n",
      "Episode 54 completed on state (1, 0, 1, 1, 0) with cumulative reward: 129.63154863450666\n",
      "Q explored: 31.666666666666668. Epsilon: 0.3084782305864982\n",
      "Episode 55 completed on state (1, 0, 1, 0, 0) with cumulative reward: -43.45295399367852\n",
      "Q explored: 31.666666666666668. Epsilon: 0.30230866597476824\n",
      "Episode 56 completed on state (0, 0, 1, 1, 0) with cumulative reward: 87.52476655356281\n",
      "Q explored: 31.666666666666668. Epsilon: 0.2962624926552729\n",
      "Episode 57 completed on state (1, 0, 1, 0, 0) with cumulative reward: -45.45224276880635\n",
      "Q explored: 31.666666666666668. Epsilon: 0.29033724280216744\n",
      "Episode 58 completed on state (1, 0, 0, 0, 0) with cumulative reward: 23.714137565774962\n",
      "Q explored: 31.666666666666668. Epsilon: 0.28453049794612406\n",
      "Episode 59 completed on state (1, 0, 0, 1, 0) with cumulative reward: -6.809188004689432\n",
      "Q explored: 31.666666666666668. Epsilon: 0.27883988798720155\n",
      "Episode 60 completed on state (1, 0, 0, 0, 0) with cumulative reward: -31.292107978398676\n",
      "Q explored: 31.666666666666668. Epsilon: 0.27326309022745754\n",
      "Episode 61 completed on state (1, 0, 0, 1, 0) with cumulative reward: -10.962985934325872\n",
      "Q explored: 31.666666666666668. Epsilon: 0.2677978284229084\n",
      "Episode 62 completed on state (1, 0, 0, 0, 0) with cumulative reward: 21.35640458331767\n",
      "Q explored: 31.666666666666668. Epsilon: 0.2624418718544502\n",
      "Episode 63 completed on state (1, 0, 1, 0, 0) with cumulative reward: -10.945523860314225\n",
      "Q explored: 31.666666666666668. Epsilon: 0.25719303441736124\n",
      "Episode 64 completed on state (1, 0, 1, 1, 0) with cumulative reward: 82.25397222170912\n",
      "Q explored: 31.666666666666668. Epsilon: 0.25204917372901403\n",
      "Episode 65 completed on state (1, 0, 1, 0, 0) with cumulative reward: 231.53652269559265\n",
      "Q explored: 31.666666666666668. Epsilon: 0.24700819025443374\n",
      "Episode 66 completed on state (1, 0, 1, 0, 0) with cumulative reward: 18.657925397292942\n",
      "Q explored: 31.666666666666668. Epsilon: 0.24206802644934505\n",
      "Episode 67 completed on state (1, 0, 1, 1, 0) with cumulative reward: 175.6083760620902\n",
      "Q explored: 31.666666666666668. Epsilon: 0.23722666592035815\n",
      "Episode 68 completed on state (0, 0, 1, 0, 0) with cumulative reward: 138.44629489084213\n",
      "Q explored: 31.666666666666668. Epsilon: 0.232482132601951\n",
      "Episode 69 completed on state (1, 0, 0, 1, 0) with cumulative reward: -12.189628696982858\n",
      "Q explored: 31.666666666666668. Epsilon: 0.22783248994991198\n",
      "Episode 70 completed on state (0, 0, 1, 1, 0) with cumulative reward: 51.11869573229782\n",
      "Q explored: 31.666666666666668. Epsilon: 0.22327584015091373\n",
      "Episode 71 completed on state (0, 0, 1, 1, 0) with cumulative reward: 449.18899950095874\n",
      "Q explored: 31.666666666666668. Epsilon: 0.21881032334789546\n",
      "Episode 72 completed on state (0, 0, 0, 1, 0) with cumulative reward: 51.338802802577774\n",
      "Q explored: 31.666666666666668. Epsilon: 0.21443411688093755\n",
      "Episode 73 completed on state (0, 0, 0, 1, -1) with cumulative reward: -34.3004475572002\n",
      "Q explored: 31.666666666666668. Epsilon: 0.2101454345433188\n",
      "Episode 74 completed on state (1, 0, 1, 0, 0) with cumulative reward: 30.239732622614014\n",
      "Q explored: 31.666666666666668. Epsilon: 0.20594252585245243\n",
      "Episode 75 completed on state (1, 0, 0, 0, 0) with cumulative reward: 73.27073115467573\n",
      "Q explored: 31.666666666666668. Epsilon: 0.20182367533540338\n",
      "Episode 76 completed on state (0, 0, 0, 1, 0) with cumulative reward: 16.15865405725053\n",
      "Q explored: 31.666666666666668. Epsilon: 0.1977872018286953\n",
      "Episode 77 completed on state (1, 0, 0, 1, -1) with cumulative reward: -43.469935070756165\n",
      "Q explored: 31.666666666666668. Epsilon: 0.19383145779212138\n",
      "Episode 78 completed on state (1, 0, 0, 0, 0) with cumulative reward: 50.450768796920116\n",
      "Q explored: 32.083333333333336. Epsilon: 0.18995482863627894\n",
      "Episode 79 completed on state (1, 0, 1, 0, 0) with cumulative reward: 152.36629598024282\n",
      "Q explored: 32.083333333333336. Epsilon: 0.18615573206355335\n",
      "Episode 80 completed on state (0, 0, 1, 1, 0) with cumulative reward: 231.44444306781293\n",
      "Q explored: 32.083333333333336. Epsilon: 0.18243261742228228\n",
      "Episode 81 completed on state (0, 0, 1, 1, 0) with cumulative reward: -19.121643142093923\n",
      "Q explored: 32.083333333333336. Epsilon: 0.17878396507383665\n",
      "Episode 82 completed on state (1, 0, 1, 0, 0) with cumulative reward: 300.718930448901\n",
      "Q explored: 32.083333333333336. Epsilon: 0.17520828577235992\n",
      "Episode 83 completed on state (0, 0, 1, 1, 0) with cumulative reward: 133.85673120468317\n",
      "Q explored: 32.5. Epsilon: 0.1717041200569127\n",
      "Episode 84 completed on state (1, 0, 1, 0, 1) with cumulative reward: 185.1343902745474\n",
      "Q explored: 32.5. Epsilon: 0.16827003765577445\n",
      "Episode 85 completed on state (0, 0, 1, 1, 0) with cumulative reward: 73.8380665582934\n",
      "Q explored: 32.5. Epsilon: 0.16490463690265894\n",
      "Episode 86 completed on state (1, 0, 1, 0, 0) with cumulative reward: -22.99023242138042\n",
      "Q explored: 32.5. Epsilon: 0.16160654416460576\n",
      "Episode 87 completed on state (1, 0, 0, 0, 0) with cumulative reward: -18.23384335053062\n",
      "Q explored: 32.5. Epsilon: 0.15837441328131366\n",
      "Episode 88 completed on state (0, 0, 0, 1, 0) with cumulative reward: -63.42048432288458\n",
      "Q explored: 32.5. Epsilon: 0.1552069250156874\n",
      "Episode 89 completed on state (1, 0, 1, 0, 0) with cumulative reward: -52.63763173093716\n",
      "Q explored: 32.5. Epsilon: 0.15210278651537365\n",
      "Episode 90 completed on state (1, 0, 0, 1, 0) with cumulative reward: 86.2944252527072\n",
      "Q explored: 32.5. Epsilon: 0.14906073078506618\n",
      "Episode 91 completed on state (1, 0, 1, 1, 0) with cumulative reward: 220.21181528200574\n",
      "Q explored: 32.5. Epsilon: 0.14607951616936485\n",
      "Episode 92 completed on state (0, 0, 1, 1, 0) with cumulative reward: 50.155183184497545\n",
      "Q explored: 32.5. Epsilon: 0.14315792584597756\n",
      "Episode 93 completed on state (0, 0, 0, 1, -1) with cumulative reward: 3.393849620392132\n",
      "Q explored: 32.5. Epsilon: 0.140294767329058\n",
      "Episode 94 completed on state (1, 0, 0, 0, 0) with cumulative reward: 249.66246432268883\n",
      "Q explored: 32.916666666666664. Epsilon: 0.13748887198247683\n",
      "Episode 95 completed on state (1, 0, 0, 0, 0) with cumulative reward: 46.130243970740025\n",
      "Q explored: 32.916666666666664. Epsilon: 0.1347390945428273\n",
      "Episode 96 completed on state (1, 0, 0, 1, 0) with cumulative reward: 6.187016885986949\n",
      "Q explored: 32.916666666666664. Epsilon: 0.13204431265197075\n",
      "Episode 97 completed on state (0, 0, 1, 1, 0) with cumulative reward: -4.863582427802584\n",
      "Q explored: 32.916666666666664. Epsilon: 0.12940342639893135\n",
      "Episode 98 completed on state (1, 0, 0, 0, 1) with cumulative reward: 344.94168692323507\n",
      "Q explored: 32.916666666666664. Epsilon: 0.12681535787095272\n",
      "Episode 99 completed on state (1, 0, 0, 0, 0) with cumulative reward: -65.71847700210134\n",
      "Q explored: 32.916666666666664. Epsilon: 0.12427905071353366\n",
      "Episode 100 completed on state (1, 0, 1, 0, 0) with cumulative reward: 32.61458365227334\n",
      "Q explored: 32.916666666666664. Epsilon: 0.121793469699263\n",
      "Episode 101 completed on state (1, 0, 1, 1, 0) with cumulative reward: 203.12468572250575\n",
      "Q explored: 32.916666666666664. Epsilon: 0.11935760030527773\n",
      "Episode 102 completed on state (0, 0, 0, 1, 0) with cumulative reward: -14.329350014194418\n",
      "Q explored: 32.916666666666664. Epsilon: 0.11697044829917218\n",
      "Episode 103 completed on state (0, 0, 1, 1, 0) with cumulative reward: 6.72218036279159\n",
      "Q explored: 32.916666666666664. Epsilon: 0.11463103933318873\n",
      "Episode 104 completed on state (1, 0, 0, 1, 0) with cumulative reward: 342.93628991447656\n",
      "Q explored: 32.916666666666664. Epsilon: 0.11233841854652496\n",
      "Episode 105 completed on state (1, 0, 0, 0, 0) with cumulative reward: 55.235340140571665\n",
      "Q explored: 32.916666666666664. Epsilon: 0.11009165017559445\n",
      "Episode 106 completed on state (1, 0, 0, 0, 0) with cumulative reward: -25.435707322782513\n",
      "Q explored: 32.916666666666664. Epsilon: 0.10788981717208257\n",
      "Episode 107 completed on state (0, 0, 0, 1, 0) with cumulative reward: -66.07833466535654\n",
      "Q explored: 32.916666666666664. Epsilon: 0.10573202082864092\n",
      "Episode 108 completed on state (0, 0, 1, 0, 0) with cumulative reward: -66.75918906616943\n",
      "Q explored: 32.916666666666664. Epsilon: 0.1036173804120681\n",
      "Episode 109 completed on state (0, 0, 1, 1, 0) with cumulative reward: -42.67105087691762\n",
      "Q explored: 32.916666666666664. Epsilon: 0.10154503280382673\n",
      "Episode 110 completed on state (1, 0, 0, 0, -1) with cumulative reward: 113.28779210003029\n",
      "Q explored: 33.333333333333336. Epsilon: 0.09951413214775019\n",
      "Episode 111 completed on state (1, 0, 0, 0, -1) with cumulative reward: -64.44772224221481\n",
      "Q explored: 33.75. Epsilon: 0.09752384950479519\n",
      "Episode 112 completed on state (0, 0, 1, 0, 0) with cumulative reward: -55.11480978740728\n",
      "Q explored: 33.75. Epsilon: 0.09557337251469929\n",
      "Episode 113 completed on state (1, 0, 0, 0, 0) with cumulative reward: -67.5960142477681\n",
      "Q explored: 33.75. Epsilon: 0.0936619050644053\n",
      "Episode 114 completed on state (1, 0, 0, 0, 0) with cumulative reward: 362.28691604968776\n",
      "Q explored: 33.75. Epsilon: 0.0917886669631172\n",
      "Episode 115 completed on state (1, 0, 0, 0, 1) with cumulative reward: 163.1221789515563\n",
      "Q explored: 34.166666666666664. Epsilon: 0.08995289362385486\n",
      "Episode 116 completed on state (1, 0, 0, 0, 0) with cumulative reward: -54.687786554276755\n",
      "Q explored: 34.166666666666664. Epsilon: 0.08815383575137777\n",
      "Episode 117 completed on state (1, 0, 1, 0, 0) with cumulative reward: 20.413846971438034\n",
      "Q explored: 34.166666666666664. Epsilon: 0.08639075903635021\n",
      "Episode 118 completed on state (1, 0, 0, 0, 0) with cumulative reward: -18.50665521970528\n",
      "Q explored: 34.166666666666664. Epsilon: 0.0846629438556232\n",
      "Episode 119 completed on state (0, 0, 0, 1, 0) with cumulative reward: -67.57997472447015\n",
      "Q explored: 34.166666666666664. Epsilon: 0.08296968497851073\n",
      "Episode 120 completed on state (1, 0, 1, 0, 0) with cumulative reward: 440.89658703570814\n",
      "Q explored: 34.166666666666664. Epsilon: 0.08131029127894052\n",
      "Episode 121 completed on state (1, 0, 0, 0, 0) with cumulative reward: 272.0670700309644\n",
      "Q explored: 34.166666666666664. Epsilon: 0.07968408545336171\n",
      "Episode 122 completed on state (1, 0, 1, 0, 0) with cumulative reward: -10.856362970390641\n",
      "Q explored: 34.166666666666664. Epsilon: 0.07809040374429448\n",
      "Episode 123 completed on state (1, 0, 1, 0, 1) with cumulative reward: 188.38041560307468\n",
      "Q explored: 34.166666666666664. Epsilon: 0.07652859566940859\n",
      "Episode 124 completed on state (1, 0, 1, 0, 0) with cumulative reward: 67.99363138924373\n",
      "Q explored: 34.166666666666664. Epsilon: 0.07499802375602041\n",
      "Episode 125 completed on state (0, 0, 1, 1, 0) with cumulative reward: 86.45513594183902\n",
      "Q explored: 34.583333333333336. Epsilon: 0.0734980632809\n",
      "Episode 126 completed on state (1, 0, 0, 0, 0) with cumulative reward: 79.38316093803431\n",
      "Q explored: 34.583333333333336. Epsilon: 0.072028102015282\n",
      "Episode 127 completed on state (0, 0, 1, 1, 0) with cumulative reward: -48.95284359668263\n",
      "Q explored: 34.583333333333336. Epsilon: 0.07058753997497635\n",
      "Episode 128 completed on state (1, 0, 0, 0, 0) with cumulative reward: -68.8172940831815\n",
      "Q explored: 34.583333333333336. Epsilon: 0.06917578917547683\n",
      "Episode 129 completed on state (1, 0, 0, 1, -1) with cumulative reward: 35.621486736268054\n",
      "Q explored: 34.583333333333336. Epsilon: 0.06779227339196729\n",
      "Episode 130 completed on state (1, 0, 0, 1, 0) with cumulative reward: 448.10258155050826\n",
      "Q explored: 34.583333333333336. Epsilon: 0.06643642792412795\n",
      "Episode 131 completed on state (1, 0, 1, 1, 0) with cumulative reward: 69.46797115231682\n",
      "Q explored: 34.583333333333336. Epsilon: 0.06510769936564538\n",
      "Episode 132 completed on state (1, 0, 1, 1, 0) with cumulative reward: 359.98152542809476\n",
      "Q explored: 34.583333333333336. Epsilon: 0.06380554537833247\n",
      "Episode 133 completed on state (1, 0, 1, 0, 0) with cumulative reward: -3.942184970360529\n",
      "Q explored: 34.583333333333336. Epsilon: 0.06252943447076582\n",
      "Episode 134 completed on state (0, 0, 1, 0, 0) with cumulative reward: 553.3913357216569\n",
      "Q explored: 35.0. Epsilon: 0.061278845781350504\n",
      "Episode 135 completed on state (1, 0, 0, 0, 0) with cumulative reward: 64.44511389805116\n",
      "Q explored: 35.0. Epsilon: 0.06005326886572349\n",
      "Episode 136 completed on state (0, 0, 1, 0, 0) with cumulative reward: 161.13735290149498\n",
      "Q explored: 35.0. Epsilon: 0.05885220348840902\n",
      "Episode 137 completed on state (1, 0, 0, 1, 0) with cumulative reward: 194.92695604827423\n",
      "Q explored: 35.0. Epsilon: 0.05767515941864084\n",
      "Episode 138 completed on state (0, 0, 1, 1, 0) with cumulative reward: -53.88327424824348\n",
      "Q explored: 35.0. Epsilon: 0.05652165623026802\n",
      "Episode 139 completed on state (1, 0, 1, 0, 1) with cumulative reward: 15.490108617059889\n",
      "Q explored: 35.0. Epsilon: 0.05539122310566266\n",
      "Episode 140 completed on state (0, 0, 0, 1, 0) with cumulative reward: 399.9562505629733\n",
      "Q explored: 35.0. Epsilon: 0.054283398643549405\n",
      "Episode 141 completed on state (0, 0, 0, 1, 0) with cumulative reward: -52.86932697554444\n",
      "Q explored: 35.0. Epsilon: 0.053197730670678414\n",
      "Episode 142 completed on state (0, 0, 0, 1, 0) with cumulative reward: 226.26061089185998\n",
      "Q explored: 35.0. Epsilon: 0.052133776057264845\n",
      "Episode 143 completed on state (0, 0, 1, 1, 0) with cumulative reward: 80.30292276716091\n",
      "Q explored: 35.0. Epsilon: 0.051091100536119545\n",
      "Episode 144 completed on state (1, 0, 0, 1, 0) with cumulative reward: 54.64514338092221\n",
      "Q explored: 35.0. Epsilon: 0.05006927852539715\n",
      "Episode 145 completed on state (1, 0, 1, 1, 0) with cumulative reward: -21.054975032157742\n",
      "Q explored: 35.0. Epsilon: 0.049067892954889204\n",
      "Episode 146 completed on state (1, 0, 0, 0, 0) with cumulative reward: 143.12616571502906\n",
      "Q explored: 35.0. Epsilon: 0.049067892954889204\n",
      "Episode 147 completed on state (0, 0, 0, 1, 0) with cumulative reward: -47.76137369561032\n",
      "Q explored: 35.0. Epsilon: 0.049067892954889204\n",
      "Episode 148 completed on state (1, 0, 0, 0, 0) with cumulative reward: -33.09311765075491\n",
      "Q explored: 35.0. Epsilon: 0.049067892954889204\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[274], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[272], line 515\u001b[0m, in \u001b[0;36mQ_learning.train\u001b[1;34m(self, m, verbose)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m    514\u001b[0m     action, random \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_greedy(state)\n\u001b[1;32m--> 515\u001b[0m     next_obs, reward, done, truncate, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     reward \u001b[38;5;241m=\u001b[39m fix_reward(reward, next_obs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_right_reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_right_skewness, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchange_lane_reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_obs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    517\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:258\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mact()\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimulation_frequency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:333\u001b[0m, in \u001b[0;36mRoad.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03mStep the dynamics of each entity on the road.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03m:param dt: timestep [s]\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[1;32m--> 333\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:124\u001b[0m, in \u001b[0;36mIDMVehicle.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03mStep the simulation.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m:param dt: timestep\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dt\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:133\u001b[0m, in \u001b[0;36mVehicle.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheading \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msin(beta) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLENGTH \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m dt\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m dt\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:151\u001b[0m, in \u001b[0;36mVehicle.on_state_update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mget_lane(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mrecord_history:\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mappendleft(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_from\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:75\u001b[0m, in \u001b[0;36mIDMVehicle.create_from\u001b[1;34m(cls, vehicle)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_from\u001b[39m(\u001b[38;5;28mcls\u001b[39m, vehicle: ControlledVehicle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIDMVehicle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    Create a new vehicle from an existing one.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    :return: a new vehicle at the same dynamical state\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheading\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_lane_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_lane_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_speed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_speed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43mroute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvehicle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m v\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:58\u001b[0m, in \u001b[0;36mIDMVehicle.__init__\u001b[1;34m(self, road, position, heading, speed, target_lane_index, target_speed, route, enable_lane_change, timer)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     49\u001b[0m              road: Road,\n\u001b[0;32m     50\u001b[0m              position: Vector,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m              enable_lane_change: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     57\u001b[0m              timer: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lane_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_speed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_lane_change \u001b[38;5;241m=\u001b[39m enable_lane_change\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer \u001b[38;5;241m=\u001b[39m timer \u001b[38;5;129;01mor\u001b[39;00m (np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLANE_CHANGE_DELAY\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\controller.py:42\u001b[0m, in \u001b[0;36mControlledVehicle.__init__\u001b[1;34m(self, road, position, heading, speed, target_lane_index, target_speed, route)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     35\u001b[0m              road: Road,\n\u001b[0;32m     36\u001b[0m              position: Vector,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m              target_speed: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m              route: Route \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_lane_index \u001b[38;5;241m=\u001b[39m target_lane_index \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_speed \u001b[38;5;241m=\u001b[39m target_speed \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:40\u001b[0m, in \u001b[0;36mVehicle.__init__\u001b[1;34m(self, road, position, heading, speed, predition_type)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     35\u001b[0m              road: Road,\n\u001b[0;32m     36\u001b[0m              position: Vector,\n\u001b[0;32m     37\u001b[0m              heading: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     38\u001b[0m              speed: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     39\u001b[0m              predition_type: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant_steering\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_type \u001b[38;5;241m=\u001b[39m predition_type\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\objects.py:36\u001b[0m, in \u001b[0;36mRoadObject.__init__\u001b[1;34m(self, road, position, heading, speed)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheading \u001b[38;5;241m=\u001b[39m heading\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m=\u001b[39m speed\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_closest_lane_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheading\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mget_lane(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Enable collision with other collidables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:61\u001b[0m, in \u001b[0;36mRoadNetwork.get_closest_lane_index\u001b[1;34m(self, position, heading)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _to, lanes \u001b[38;5;129;01min\u001b[39;00m to_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _id, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lanes):\n\u001b[1;32m---> 61\u001b[0m             distances\u001b[38;5;241m.\u001b[39mappend(\u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance_with_heading\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     62\u001b[0m             indexes\u001b[38;5;241m.\u001b[39mappend((_from, _to, _id))\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexes[\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmin(distances))]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:125\u001b[0m, in \u001b[0;36mAbstractLane.distance_with_heading\u001b[1;34m(self, position, heading, heading_weight)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m heading \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance(position)\n\u001b[1;32m--> 125\u001b[0m s, r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m angle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_angle(heading, s))\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(r) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(s \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m \u001b[38;5;241m-\u001b[39m s, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m heading_weight\u001b[38;5;241m*\u001b[39mangle\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:189\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlocal_coordinates\u001b[39m(\u001b[38;5;28mself\u001b[39m, position: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    188\u001b[0m     delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[1;32m--> 189\u001b[0m     longitudinal \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m     lateral \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection_lateral)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Q.train(m=250, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Q.plot_rewards_history(smoothing=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1605090542131524"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.get_state_visits((0, 0, 0, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LANE_LEFT': 72.14371059447907,\n",
       " 'FASTER': 38.0264560854596,\n",
       " 'SLOWER': 21.499707190440567,\n",
       " 'LANE_RIGHT': 18.346160847401716,\n",
       " 'IDLE': 13.341732155037946}"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.search_Q((0, 0, 0, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0, 0) LANE_RIGHT 2.7660428890572644 (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 2.8850761928973574 (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 3.192990859430143 (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 3.6102256069467646 (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 4.102682578290887 (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 4.70521064025497 (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 1) LANE_LEFT 5.070644783654647 (0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1) LANE_LEFT 4.368484879214952 (0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 3.766243535382765 (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 4.3777330111106885 (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 4.284442650530519 (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 4.728702416950956 (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 1) LANE_LEFT 5.0743390857245485 (0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1) LANE_LEFT 4.36775467957 (0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 3.7659095236243982 (0, 0, 0, 0, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT 4.377808419700306 (0, 0, 1, 0, 0)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 4.2844751019583 (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 4.728706404095389 (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 1) LANE_LEFT 5.074339736976807 (0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1) LANE_LEFT 4.367754447974999 (0, 0, 0, 0, 1)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT 3.765909445453359 (0, 0, 1, 0, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT 4.377808426638364 (0, 0, 1, 0, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT 4.2844751057801425 (0, 0, 1, 0, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT 4.728706404404692 (0, 0, 1, 0, 0)\n",
      "(0, 0, 1, 0, 1) FASTER 5.074339736996892 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) FASTER 6.589889506179194 (0, 0, 1, 0, 1)\n",
      "(0, 0, 0, 0, 1) LANE_LEFT 7.195240224310003 (0, 0, 0, 0, 1)\n",
      "(0, 0, 1, 0, 1) FASTER 6.724451958366231 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT 7.042099556553186 (0, 0, 1, 0, 0)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 6.96226287564997 (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 1) LANE_LEFT 7.504570712828129 (0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1) LANE_LEFT 6.897517827057531 (0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 6.317639245355094 (0, 0, 0, 0, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT 6.8687031176039595 (0, 0, 1, 0, 0)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 6.8096513153986304 (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 7.2391291284058354 (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 1) LANE_LEFT 7.579358325074708 (0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1) LANE_LEFT 6.893725378688804 (0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 6.315102713341804 (0, 0, 0, 0, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT 6.870306428007947 (0, 0, 1, 0, 0)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 6.810208765938598 (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) LANE_RIGHT 7.239258000897239 (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 1) LANE_LEFT 7.579402773539458 (0, 0, 0, 0, 1)\n",
      "(0, 0, 1, 0, 1) FASTER 6.893743375485271 (0, 0, 1, 0, 1)\n",
      "(1, 0, 1, 0, 0) FASTER 7.1077689739744345 (1, 0, 1, 0, 0)\n",
      "(1, 0, 1, 0, 0) FASTER -67.92213193442002 (1, 0, 1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "Q.test(sleep_time=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar.train(m=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FASTER': 5.133641775043947,\n",
       " 'LANE_RIGHT': 1.8789156890532015,\n",
       " 'SLOWER': 1.7437783264494309,\n",
       " 'IDLE': 0.5959434836555192,\n",
       " 'LANE_LEFT': -1.1294429580928238}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sar.search_Q((1, 0, 0, 1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 1, 0, 0)\n",
      "(0, 0, 1, 0, 0)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(1, 0, 1, 0, 1)\n",
      "(1, 0, 1, 0, 1)\n"
     ]
    }
   ],
   "source": [
    "sar.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation testing kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.03333333333333\n",
      "20.127211934156396\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[157], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m      6\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m----> 7\u001b[0m     obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     cum_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Print non-zero obs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:258\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mact()\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimulation_frequency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:333\u001b[0m, in \u001b[0;36mRoad.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03mStep the dynamics of each entity on the road.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03m:param dt: timestep [s]\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[1;32m--> 333\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:124\u001b[0m, in \u001b[0;36mIDMVehicle.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03mStep the simulation.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m:param dt: timestep\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dt\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:133\u001b[0m, in \u001b[0;36mVehicle.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheading \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msin(beta) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLENGTH \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m dt\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m dt\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:148\u001b[0m, in \u001b[0;36mVehicle.on_state_update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_state_update\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad:\n\u001b[1;32m--> 148\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_closest_lane_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheading\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mget_lane(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index)\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mrecord_history:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:61\u001b[0m, in \u001b[0;36mRoadNetwork.get_closest_lane_index\u001b[1;34m(self, position, heading)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _to, lanes \u001b[38;5;129;01min\u001b[39;00m to_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _id, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lanes):\n\u001b[1;32m---> 61\u001b[0m             distances\u001b[38;5;241m.\u001b[39mappend(\u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance_with_heading\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     62\u001b[0m             indexes\u001b[38;5;241m.\u001b[39mappend((_from, _to, _id))\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexes[\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmin(distances))]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:125\u001b[0m, in \u001b[0;36mAbstractLane.distance_with_heading\u001b[1;34m(self, position, heading, heading_weight)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m heading \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance(position)\n\u001b[1;32m--> 125\u001b[0m s, r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m angle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_angle(heading, s))\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(r) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(s \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m \u001b[38;5;241m-\u001b[39m s, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m heading_weight\u001b[38;5;241m*\u001b[39mangle\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:190\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    188\u001b[0m delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m    189\u001b[0m longitudinal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection)\n\u001b[1;32m--> 190\u001b[0m lateral \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate an episode of the environment and show the rewards and cumulative rewards\n",
    "cum_reward = 0\n",
    "with gym.make(\"highway-v0\", config=kinematics, render_mode='human') as env:\n",
    "    obs = env.reset()\n",
    "    for _ in range(100):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "        # Print non-zero obs\n",
    "        obs_non = obs[~np.all(obs == 0, axis=1)]\n",
    "        print(reward)\n",
    "        if done:\n",
    "            break\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time to collision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.  0.  0.5 1. ]\n",
      " [0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.5 1.  1. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "[[0.5 1.  1.  0.5 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "[[0.  1.  1.  0.5 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 1.  1.  0.  0. ]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [1.  1.  0.5 0.  0. ]]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[0.  0.  0.  0.  0.5]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.5 1.  1.  0. ]]\n",
      "[[0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 1.  1.  0.  0. ]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [1.  1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n"
     ]
    }
   ],
   "source": [
    "class TimeToCollision:\n",
    "    def __init__(horizon=5,\n",
    "                policy_frequency=1,\n",
    "                simulation_frequency=10):\n",
    "\n",
    "        self.config = default_config.copy()\n",
    "        self.config[\"observation\"] =  {\n",
    "        \"type\": \"TimeToCollision\",\n",
    "        \"horizon\": horizon}\n",
    "        self.config['policy_frequency'] = policy_frequency\n",
    "        self.config['simulation_frequency'] = simulation_frequency\n",
    "\n",
    "    def get_state(self, env): \n",
    "        grid = env.vehicle.speed_index\n",
    "        return self.current_obs[grid]\n",
    "\n",
    "    def test_env(self):\n",
    "        with gym.make(\"highway-v0\", config=self.config, render_mode='human') as env:\n",
    "            obs = env.reset()\n",
    "            for _ in range(1000):\n",
    "                action = env.action_space.sample()\n",
    "                obs, reward, done, truncated, info = env.step(action)\n",
    "                print(self.get_state, reward)\n",
    "                if done:\n",
    "                    break\n",
    "                time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
