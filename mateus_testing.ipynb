{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "\n",
    "    # Parametrization bellow cannot be changed\n",
    "    \"lanes_count\" : 10, # The environment must always have 10 lanes\n",
    "    \"vehicles_count\": 50, # The environment must always have 50 other vehicles\n",
    "    \"duration\": 120,  # [s] The environment must terminate never before 120 seconds\n",
    "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.IDMVehicle\", # This is the policy of the other vehicles\n",
    "    \"initial_spacing\": 2, # Initial spacing between vehicles needs to be at most 2\n",
    "\n",
    "    # Refer to refer to https://highway-env.farama.org/observations/ to change observation space type\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\"\n",
    "    },\n",
    "\n",
    "    # Refer to refer to https://highway-env.farama.org/actions/ to change action space type\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteMetaAction\",\n",
    "    },\n",
    "\n",
    "    # Parameterization bellow can be changed (as it refers mostly to the reward system)\n",
    "    # \"collision_reward\": -10,  # The reward received when colliding with a vehicle. (Can be changed)\n",
    "    # \"reward_speed_range\": [20, 30],  # [m/s] The reward for high speed is mapped linearly from this range to [0, HighwayEnv.HIGH_SPEED_REWARD]. (Can be changed)\n",
    "    \"simulation_frequency\": 15, #15,  # [Hz] (Can be changed)\n",
    "    \"policy_frequency\": 5, #5,  # [Hz] (Can be changed)\n",
    "\n",
    "    \"collision_reward\": -100,  # The reward received when colliding with a vehicle.\n",
    "    \"right_lane_reward\": 0.1,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
    "    # zero for other lanes.\n",
    "    \"high_speed_reward\": 100,  # The reward received when driving at full speed, linearly mapped to zero for\n",
    "    # lower speeds according to config[\"reward_speed_range\"].\n",
    "    \"lane_change_reward\": 0,  # The reward received at each lane change action.\n",
    "    \"reward_speed_range\": [20, 30],\n",
    "    \n",
    "    # Parameters defined bellow are purely for visualiztion purposes! You can alter them as you please\n",
    "    \"screen_width\": 800,  # [px]\n",
    "    \"screen_height\": 600,  # [px]\n",
    "    \"centering_position\": [0.5, 0.5],\n",
    "    \"scaling\": 5,\n",
    "    \"show_trajectories\": True,\n",
    "    \"render_agent\": True,\n",
    "    \"offscreen_rendering\": False\n",
    "}\n",
    "\n",
    "default_config = configuration.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occupancyGrid = configuration.copy()\n",
    "occupancyGrid[\"observation\"] =  {\n",
    "    \"type\": \"OccupancyGrid\",\n",
    "    # \"vehicles_count\": 50,\n",
    "    \"features\": [\n",
    "                \"presence\",\n",
    "                #\"x\", \"y\", \n",
    "                #\"vx\", \"vy\"\n",
    "                ],\n",
    "    # \"features_range\": {\n",
    "    #      \"x\": [-500, 500],\n",
    "    #      \"y\": [-500, 500],\n",
    "    #     \"vx\": [-20, 20],\n",
    "    #     \"vy\": [-20, 20]\n",
    "    # },\n",
    "    \"grid_size\": [[-100, 100], [-100, 100]],    # X controls how many lanes, Y controls how far ahead\n",
    "    \"grid_step\": [1, 1],\n",
    "    #\"absolute\": False,                     # Not implemented in the library\n",
    "    #\"as_image\": True,\n",
    "    # \"align_to_vehicle_axes\" : True\n",
    "}\n",
    "\n",
    "# The higher the number, the more frequent the policy and the simulation frequencies, the slower the simulation\n",
    "occupancyGrid[\"simulation_frequency\"] = 15\n",
    "occupancyGrid[\"policy_frequency\"] = 1\n",
    "\n",
    "env = gym.make('highway-v0', render_mode='human', config=occupancyGrid)\n",
    "\n",
    "obs, info = env.reset(seed = 30)\n",
    "\n",
    "env.close()\n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7470041889935329 0.7470041889935329\n",
      "0.7497703427235373 1.4967745317170702\n",
      "0.7497916630997844 2.2465661948168547\n",
      "0.747062037436638 2.9936282322534926\n",
      "0.7498258836702131 3.7434541159237056\n",
      "0.9570215828047481 4.700475698728454\n",
      "0.992425312301707 5.692901011030161\n",
      "0.9984753812783485 6.691376392308509\n",
      "0.9971407464067447 7.6885171387152536\n",
      "0.9996186311465904 8.688135769861844\n",
      "0.9996605856977958 9.687796355559641\n",
      "0.7924914156140906 10.480287771173732\n",
      "0.7542418714829565 11.234529642656689\n",
      "0.747622191656039 11.982151834312727\n",
      "0.7498691284710792 12.732020962783807\n",
      "0.7497170063753728 13.48173796915918\n",
      "0.7468413438148681 14.228579312974048\n",
      "0.7496048082651992 14.978184121239247\n",
      "0.7469279147275342 15.72511203596678\n",
      "5.552779165969887e-05 15.72516756375844\n"
     ]
    }
   ],
   "source": [
    "# Generate an episode of the environment and show the rewards and cumulative rewards\n",
    "cum_reward = 0\n",
    "with gym.make(\"highway-v0\", config=occupancyGrid, render_mode='human') as env:\n",
    "    obs = env.reset()\n",
    "    for _ in range(1000):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "        print(reward, cum_reward)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_reward(reward, colision_reward=-1000, skew_speed=1, done=True): \n",
    "    \"\"\"\n",
    "    This function is used to correct the reward function, which is not correctly outputted by the environment\n",
    "    Params: \n",
    "        reward: float, the reward to fix\n",
    "        colision_reward: float, the colision reward to fix\n",
    "        skew_speed: float, the skew speed to fix. It is an exponent applied to the reward\n",
    "        done: bool, double check if the reward was indeed a colision\n",
    "    \"\"\"\n",
    "    if reward < 0.5 and done:\n",
    "        return colision_reward\n",
    "    else:\n",
    "        return reward**skew_speed\n",
    "\n",
    "def decode_meta_action(action):\n",
    "    \"\"\"\n",
    "    Function to output the corresponding action in text-form\n",
    "    \"\"\"\n",
    "    assert action in range(5), \"The action must be between 0 and 4\"\n",
    "    if action == 0:\n",
    "        return \"LANE_LEFT\"\n",
    "    elif action == 1:\n",
    "        return \"IDLE\"\n",
    "    elif action == 2:\n",
    "        return \"LANE_RIGHT\"\n",
    "    elif action == 3:\n",
    "        return \"FASTER\"\n",
    "    elif action == 4:\n",
    "        return \"SLOWER\"\n",
    "\n",
    "\n",
    "class OccupancyGrid():\n",
    "    def __init__(\n",
    "            self,\n",
    "            grid_size=[[-50, 50], [-50, 50]],  # X controls the lane-width, Y controls how far ahead\n",
    "            grid_step=[1, 1],\n",
    "            state_type='lane-wise',\n",
    "            n_closest=3,\n",
    "            ss_bins=[5,6],\n",
    "            crop_dist=[[-10,10], [-10,25]],\n",
    "            policy=None,\n",
    "            sim_frequency=20,\n",
    "            policy_frequency=2,\n",
    "            render_mode = 'human',\n",
    "            seed = 50,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Occupancy view class constructor\n",
    "        Arguments:\n",
    "            grid_size: list of lists, the size of the grid in the x and y direction, where x controls the lane-width and y controls how far ahead. Lanes are 5m wide, and the car position is (0,0)\n",
    "            grid_step: list, the step size of the grid in the x and y direction, in meters\n",
    "            state_type: str, the type of state to use. Options are 'lane-wise' or 'n_neighbours'\n",
    "            n_closest: int, the number of closest cars to consider in the state space\n",
    "            ss_bins: list, the number of bins to divide the x and y directions\n",
    "            crop_dist: list of lists, the distance to crop the x and y directions, above which the values will be clipped\n",
    "            policy: function, the policy to use in the simulation\n",
    "            sim_frequency: int, the frequency of the simulation\n",
    "            policy_frequency: int, the frequency of the policy\n",
    "            render_mode: str, the mode to render the simulation\n",
    "            seed: int, the seed to use in the simulation\n",
    "        \"\"\"\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.grid_step = grid_step\n",
    "        self.config = default_config.copy()\n",
    "        self.config[\"observation\"] =  {\n",
    "            \"type\": \"OccupancyGrid\",\n",
    "            \"features\": [\"presence\"],\n",
    "            \"grid_size\": grid_size,    # X controls how many lanes, Y controls how far ahead\n",
    "            \"grid_step\": grid_step,\n",
    "        }\n",
    "        self.config[\"simulation_frequency\"] = sim_frequency\n",
    "        self.config[\"policy_frequency\"] = policy_frequency\n",
    "        self.render_mode = render_mode\n",
    "        self.seed = seed\n",
    "        self.state_type = state_type\n",
    "        self.policy = policy\n",
    "        self.n_closest, self.ss_bins, self.crop_dist = n_closest, ss_bins, crop_dist\n",
    "        self.initialize_states()\n",
    "\n",
    "\n",
    "    def initialize_states(self):\n",
    "        \"\"\"\n",
    "        Initialize the states of the occupancy grid\n",
    "        \"\"\"\n",
    "        # Start the environment\n",
    "        with gym.make('highway-v0', render_mode=self.render_mode, config=self.config) as env:\n",
    "            obs, info = env.reset(seed = self.seed)\n",
    "            self.current_obs = obs\n",
    "            self.env = env\n",
    "        \n",
    "        if self.state_type == 'lane-wise':\n",
    "            self.initialize_lane_wise()\n",
    "\n",
    "        elif self.state_type == 'n_neighbours':\n",
    "            self.initialize_n_neighbours()\n",
    "        \n",
    "    def initialize_n_neighbours(self):\n",
    "        # States will be stored in a dictionary, with the key being ((x1,x2,...,xn), (y1,y2,...,yn)), and n is the number of neighbors\n",
    "        # Make ss_bins[0] from the crop_dist[0] and ss_bins[1] from crop_dist[1]\n",
    "        self.bins_left_right = np.linspace(self.crop_dist[0][0], self.crop_dist[0][1], self.ss_bins[0])\n",
    "        self.bins_front = np.linspace(self.crop_dist[1][0], self.crop_dist[1][1], self.ss_bins[1])\n",
    "        self.bins_back = self.bins_front\n",
    "\n",
    "        # Each of the nearest neighbors will have a state of the form (x,y). Create the first key of the dictionary in the form (x1,x2,...xn)\n",
    "        x_keys = list(itertools.product(self.bins_left_right, repeat=self.n_closest))\n",
    "        y_keys = list(itertools.product(self.bins_front, repeat=self.n_closest))\n",
    "        self.states = list(itertools.product(x_keys, y_keys))\n",
    "\n",
    "    def initialize_lane_wise(self):\n",
    "        self.bins_front = [5,10,30]   #  [5,10,15,30]\n",
    "        self.bins_back = [5,10,30]\n",
    "        self.bins_left_right = [-12,-5,5,12]   # [-20,-10,-5,5,10,20]\n",
    "        self.states = list(itertools.product(self.bins_front, self.bins_back, self.bins_left_right, self.bins_left_right))\n",
    "\n",
    "    def get_car_positions(self):\n",
    "        \"\"\"\n",
    "        Get the car positions in the occupancy grid\n",
    "        Returns:\n",
    "            car_positions: np.array, the car positions in the occupancy grid, in the form ([x1,y1], [x2,y2], ...)\n",
    "        \"\"\" \n",
    "        positions = np.nonzero(self.current_obs[0])\n",
    "        car_positions = np.array([positions[0]*self.grid_step[0] + self.grid_size[0][0], positions[1]*self.grid_step[1] + self.grid_size[1][0]]).T\n",
    "        return car_positions\n",
    "\n",
    "    def get_n_closest(self):\n",
    "        \"\"\"\n",
    "        Get the n closest cars to the agent\n",
    "        Returns:\n",
    "            closest_car_positions: np.array, the positions of the n closest cars to the agent. If there are less than n_closest cars, the array is padded with the crop_dist values\n",
    "        \"\"\"\n",
    "\n",
    "        car_positions = self.get_car_positions()\n",
    "        distances = np.linalg.norm(car_positions, axis=1)\n",
    "\n",
    "        # Remove the agent position\n",
    "        closest = np.argsort(distances)[1:self.n_closest+1]\n",
    "        closest_car_positions = car_positions[closest]\n",
    "\n",
    "        # If there are less than n_closest cars, pad the array with the crop_dist values\n",
    "        if len(closest_car_positions) < self.n_closest:\n",
    "            n_missing = self.n_closest - len(closest_car_positions)\n",
    "            closest_car_positions = np.pad(closest_car_positions, ((0, n_missing), (0,0)), 'constant', constant_values=(self.crop_dist[0][0], self.crop_dist[1][0]))\n",
    "\n",
    "        # Values that are \n",
    "        return closest_car_positions\n",
    "    \n",
    "    def get_state(self, type='lane-wise', decode=False):\n",
    "        \"\"\"\n",
    "        Get the state of the environment\n",
    "        Arguments:\n",
    "            type: str, the type of state to get. Options are 'n_neighbours' or 'lane-wise' :\n",
    "                n_neighbours: the state is the n closest neighbours in the form (x1,x2,...,xn), (y1,y2,...,yn)\n",
    "                lane-wise: the state is a matrix with the binned distances of the agent to the car in front, back, left-lane and right-lane\n",
    "            decode: bool, whether to decode the state\n",
    "        \"\"\"\n",
    "        assert type in ['n_neighbours', 'lane-wise'], \"The type of state must be either 'n_neighbours' or 'lane-wise'\"\n",
    "        if type == 'n_neighbours':\n",
    "            state = self.state_n_neighbours()\n",
    "        elif type == 'lane-wise':\n",
    "            state = self.state_lane_wise(decode=decode)\n",
    "        return state\n",
    "\n",
    "    def state_n_neighbours(self):\n",
    "        \"\"\"\n",
    "        Get the state of the environment in a neighbour-wise manner\n",
    "        Returns:\n",
    "            state: tuple, the state of the environment in the form (x1,x2,...,xn), (y1,y2,...,yn)\n",
    "        \"\"\"\n",
    "        n_closest = self.get_n_closest()\n",
    "        # For the closest cars, get the state\n",
    "        state_x, state_y = [], []\n",
    "        # Get the bin values for each of the x,y positions, and return a tuple with the values\n",
    "        for car in n_closest:\n",
    "            x = np.digitize(car[0], self.x_bins) - 1\n",
    "            y = np.digitize(car[1], self.y_bins) - 1\n",
    "            x_val, y_val = self.x_bins[x], self.y_bins[y]\n",
    "            state_x.append(x_val)\n",
    "            state_y.append(y_val)\n",
    "        state = (tuple(state_x), tuple(state_y))\n",
    "        return tuple(state)\n",
    "    \n",
    "    def state_lane_wise(self, decode=False):\n",
    "        \"\"\"\n",
    "        Get the state of the environment in a lane-wise manner\n",
    "        Arguments:\n",
    "            state: np.array, the state of the environment with the binned distances of the agent to the car in front, back, left-lane and right-lane \n",
    "            decode: bool, whether to return the state in a decoded manner\n",
    "        \"\"\"\n",
    "        # Get the car positions in relation to the agent. The agent is at position (0,0)\n",
    "        car_positions = self.get_car_positions()\n",
    "\n",
    "        # The same lane cars are the ones with the same x+-2, while front and back are the ones with y > 0 and y < 0, respectively\n",
    "        same_lane = car_positions[np.abs(car_positions[:,0]) <= 2]\n",
    "        front = same_lane[(same_lane[:,1] > 0) & (same_lane[:,1] < 30)]\n",
    "        back = same_lane[(same_lane[:,1] < 0) & (same_lane[:,1] > -30)]\n",
    "\n",
    "        # The other lane cars have to be in the range of (2,7] and [-2,-7), for the left and right lanes, respectively\n",
    "        left_lane = car_positions[(car_positions[:,0] < -2) & (car_positions[:,0] >= -7) & (np.abs(car_positions[:,1]) < 20)]\n",
    "        right_lane = car_positions[(car_positions[:,0] > 2) & (car_positions[:,0] <= 7) & (np.abs(car_positions[:,1]) < 20)]\n",
    "        \n",
    "        # Now we need to get the cars that are the closest from the arrays above\n",
    "        front_dist = np.min(front[:,1]) if len(front) > 0 else 30            # The one with the smallest y value, e.g.: (0, 4) is closer than (0,12)\n",
    "        back_dist = -np.max(back[:,1]) if len(back) > 0 else 30               # The one with the largest y value, e.g.: (0, -4) is closer than (0,-12)     \n",
    "        \n",
    "        # For the left and right lanes, we need to get the closest euclidean distance\n",
    "        left_closest = left_lane[np.argmin(np.linalg.norm(left_lane, axis=1))] if len(left_lane) > 0 else np.array([30,30])\n",
    "        right_closest = right_lane[np.argmin(np.linalg.norm(right_lane, axis=1))] if len(right_lane) > 0 else np.array([30,30])\n",
    "        \n",
    "        # If the left or right cars are in front of the agent, then the euclidean distance will be positive, otherwise negative\n",
    "        left_dist = np.linalg.norm(left_closest) if left_closest[1] > 0 else -np.linalg.norm(left_closest)\n",
    "        right_dist = np.linalg.norm(right_closest) if right_closest[1] > 0 else -np.linalg.norm(right_closest)\n",
    "\n",
    "        front_dist = self.bins_front[np.digitize(front_dist, self.bins_front)-1]\n",
    "        back_dist = self.bins_back[np.digitize(back_dist, self.bins_back)-1]\n",
    "        left_dist = self.bins_left_right[np.digitize(left_dist, self.bins_left_right)-1]\n",
    "        right_dist = self.bins_left_right[np.digitize(right_dist, self.bins_left_right)-1]\n",
    "\n",
    "        state = tuple((front_dist, back_dist, left_dist, right_dist)) if not decode else \"Front: {}, Back: {}, Left: {}, Right: {}\".format(front_dist, back_dist, left_dist, right_dist)\n",
    "        return state\n",
    "\n",
    "    def test_env(self):\n",
    "        \"\"\"\n",
    "        Function to test the environment with a random policy, or with a policy\n",
    "        \"\"\"\n",
    "        obs, info = self.env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        while not done:\n",
    "            # start = time.time()\n",
    "            if self.policy is None:\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                action = self.policy()\n",
    "            obs, reward, done, truncate, info = self.env.step(action)\n",
    "            self.current_obs = obs\n",
    "            print(self.get_state(type='lane-wise', decode=True), decode_meta_action(action), fix_reward(reward, skew_speed=2))\n",
    "            time.sleep(1)\n",
    "            # end = time.time()\n",
    "            # print(f\"Time taken: {end-start}\")\n",
    "        self.env.close()\n",
    "        return info[\"score\"]\n",
    "\n",
    "# ------------------- SARSA -------------------    \n",
    "class Sarsa(OccupancyGrid):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha=0.75,\n",
    "        gamma=0.95,\n",
    "        epsilon=0.6,\n",
    "        print_stats=False,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        SARSA class constructor\n",
    "        Arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            m: int, the number of episodes to train the agent for\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.initialize_Q(print_stats)\n",
    "        self.alpha, self.gamma, self.epsilon = alpha, gamma, epsilon\n",
    "\n",
    "    def policy_Q(self, state):\n",
    "        values = [self.Q[(state, action)] for action in range(5)]\n",
    "        return np.argmax(values)   \n",
    "\n",
    "    def initialize_Q(self, print_stats = False):\n",
    "        # Combine the possible states with the possible actions\n",
    "        keys = list(itertools.product(self.states, range(5)))       # 5 possible actions, 0-4: left, idle, right, accelerate, decelerate\n",
    "        if len(keys) > 150000:\n",
    "            print(\"Warning: The number of states is too large, consider reducing the number of states\")\n",
    "        if print_stats:\n",
    "            print(f\"Number of states: {len(keys)}\")   \n",
    "        self.Q = {key: 0 for key in keys}\n",
    "\n",
    "    def epsilon_greedy(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(5)\n",
    "        else:\n",
    "            values = [self.Q[(state, action)] for action in range(5)]\n",
    "            return np.argmax(values)\n",
    "        \n",
    "    def train(self, m=50, verbose=0): \n",
    "        env = gym.make('highway-v0', render_mode=None, config=self.config)\n",
    "        obs, info = env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        state = self.get_state()\n",
    "        action = self.epsilon_greedy(state)\n",
    "        q_explored = 0\n",
    "        for i in tqdm(range(m)):\n",
    "            env.reset()\n",
    "            cum_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                reward = fix_reward(reward, done=done)\n",
    "                next_state = self.get_state()\n",
    "                next_action = self.epsilon_greedy(next_state)\n",
    "\n",
    "                print(next_state, decode_meta_action(next_action), reward) if verbose > 2 else None\n",
    "                if self.Q[(state, action)] == 0:\n",
    "                    q_explored += 1\n",
    "                cum_reward += reward\n",
    "\n",
    "                self.Q[(state, action)] += self.alpha*(reward + self.gamma*self.Q[(next_state, next_action)] - self.Q[(state, action)])\n",
    "                state, action = next_state, next_action\n",
    "                self.current_obs = next_obs\n",
    "            print(f\"Episode {i+1} completed, cumulative reward: {cum_reward}\") if verbose > 0 else None\n",
    "            print(f\"Q explored: {100*q_explored/len(self.Q)}\") if verbose > 1 else None\n",
    "        env.close()\n",
    "\n",
    "    def test(self):\n",
    "        env = gym.make('highway-v0', render_mode=self.render_mode, config=self.config)\n",
    "        obs, info = env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        state = self.get_state(self.state_type)\n",
    "        action = self.policy_Q(state)\n",
    "        while not done:\n",
    "            next_obs, reward, done, truncate, info = env.step(action)\n",
    "            next_state = self.get_state(self.state_type)\n",
    "            next_action = self.policy_Q(next_state)\n",
    "            state, action = next_state, next_action\n",
    "            self.current_obs = next_obs\n",
    "            print(next_state)\n",
    "        env.close()\n",
    "        return info[\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- grid_size=[[-50, 50], [-50, 50]],  # X controls the lane-width, Y controls how far ahead\n",
    "- grid_step=[1, 1],\n",
    "- n_closest=3,\n",
    "- ss_bins=[5,6],\n",
    "- crop_dist=[[-10,10], [-10,25]],\n",
    "- policy=None,\n",
    "- sim_frequency=15,\n",
    "- policy_frequency=5,\n",
    "- render_mode = 'human',\n",
    "- seed = 50,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 720\n"
     ]
    }
   ],
   "source": [
    "a = Sarsa(print_stats=True, epsilon=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49916acc6fe3459aa90c885a02637451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 completed, cumulative reward: -992.5979704320507\n",
      "Episode 2 completed, cumulative reward: -990.2825113242836\n",
      "Episode 3 completed, cumulative reward: -990.9409494892124\n",
      "Episode 4 completed, cumulative reward: -990.5974050429357\n",
      "Episode 5 completed, cumulative reward: -966.0344252934487\n",
      "Episode 6 completed, cumulative reward: -987.2215439237193\n",
      "Episode 7 completed, cumulative reward: -944.4623444411777\n",
      "Episode 8 completed, cumulative reward: -973.4765640393324\n",
      "Episode 9 completed, cumulative reward: -979.0561081281122\n",
      "Episode 10 completed, cumulative reward: -996.1908800502778\n",
      "Episode 11 completed, cumulative reward: -990.9925324148519\n",
      "Episode 12 completed, cumulative reward: -983.9203834708939\n",
      "Episode 13 completed, cumulative reward: -993.3190179108218\n",
      "Episode 14 completed, cumulative reward: -982.7329815915542\n",
      "Episode 15 completed, cumulative reward: -988.52928711729\n",
      "Episode 16 completed, cumulative reward: -987.4232627071336\n",
      "Episode 17 completed, cumulative reward: -991.1652561658939\n",
      "Episode 18 completed, cumulative reward: -999.1047846907369\n",
      "Episode 19 completed, cumulative reward: -987.2495371652759\n",
      "Episode 20 completed, cumulative reward: -978.4890073823035\n",
      "Episode 21 completed, cumulative reward: -960.2886848156414\n",
      "Episode 22 completed, cumulative reward: -988.3400694146486\n",
      "Episode 23 completed, cumulative reward: -988.1088254143647\n",
      "Episode 24 completed, cumulative reward: -982.0425653484363\n",
      "Episode 25 completed, cumulative reward: -857.4740233583392\n",
      "Episode 26 completed, cumulative reward: -992.8795885482135\n",
      "Episode 27 completed, cumulative reward: -997.1668900062875\n",
      "Episode 28 completed, cumulative reward: -972.9202262199218\n",
      "Episode 29 completed, cumulative reward: -999.1047846907369\n",
      "Episode 30 completed, cumulative reward: -949.3611323184554\n",
      "Episode 31 completed, cumulative reward: -993.5828765839515\n",
      "Episode 32 completed, cumulative reward: -994.9541160773239\n",
      "Episode 33 completed, cumulative reward: -989.9104157603302\n",
      "Episode 34 completed, cumulative reward: -961.6792559964482\n",
      "Episode 35 completed, cumulative reward: -982.2358003020862\n",
      "Episode 36 completed, cumulative reward: -988.0529324427707\n",
      "Episode 37 completed, cumulative reward: -988.2293320712095\n",
      "Episode 38 completed, cumulative reward: -984.7604796926123\n",
      "Episode 39 completed, cumulative reward: -998.5196292811055\n",
      "Episode 40 completed, cumulative reward: -990.330010019337\n",
      "Episode 41 completed, cumulative reward: -980.2620891431777\n",
      "Episode 42 completed, cumulative reward: -992.37347294925\n",
      "Episode 43 completed, cumulative reward: -989.994061001659\n",
      "Episode 44 completed, cumulative reward: -969.9483260175848\n",
      "Episode 45 completed, cumulative reward: -976.721607205835\n",
      "Episode 46 completed, cumulative reward: -994.2621404341169\n",
      "Episode 47 completed, cumulative reward: -989.1560587072688\n",
      "Episode 48 completed, cumulative reward: -987.1750528627041\n",
      "Episode 49 completed, cumulative reward: -971.0194055270224\n",
      "Episode 50 completed, cumulative reward: -971.0618780556077\n"
     ]
    }
   ],
   "source": [
    "a.train(m=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.52777777777778\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for keys, values in a.Q.items():\n",
    "    if values != 0:\n",
    "        count += 1\n",
    "\n",
    "print(100*count/len(a.Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 30, 12, 12)\n",
      "(5, 30, 12, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, -5, 12)\n",
      "(30, 30, -5, 12)\n",
      "(30, 30, -12, 12)\n",
      "(30, 30, -12, 12)\n",
      "(30, 30, -12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, 5, 12)\n",
      "(30, 30, -5, 12)\n",
      "(30, 30, -12, 12)\n",
      "(30, 30, -12, 12)\n",
      "(30, 30, -12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(30, 30, 12, 12)\n",
      "(10, 30, 12, 12)\n",
      "(10, 30, 12, 12)\n",
      "(10, 30, 12, 12)\n",
      "(10, 30, 12, 12)\n",
      "(5, 30, 12, 12)\n",
      "(5, 30, 12, 12)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[44], line 327\u001b[0m, in \u001b[0;36mSarsa.test\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mprint\u001b[39m(next_state)\n\u001b[0;32m    326\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'score'"
     ]
    }
   ],
   "source": [
    "a.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_occupancy = OcupancyGrid(render_mode=None)\n",
    "# print(new_occupancy.x_bins, new_occupancy.y_bins, len(new_occupancy.states))\n",
    "# new_occupancy.test_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________\n",
    "### Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_car_positions(obs, grid_step=1, grid_size=50):\n",
    "    # car_positions = []\n",
    "    # for i in range(int(grid_size*2 / grid_step)):\n",
    "    #     for j in range(int(grid_size*2 / grid_step)):\n",
    "    #         if obs[0,i,j] == 1:\n",
    "    #             car_positions.append([i*grid_step - grid_size, j*grid_step - grid_size])\n",
    "    positions = np.nonzero(obs[0])\n",
    "    car_positions = list(zip(positions[0]*grid_step - grid_size, positions[1]*grid_step - grid_size))\n",
    "    return car_positions\n",
    "\n",
    "def get_n_closest(obs, grid_step=1, grid_size=50, n=3):\n",
    "    car_positions = return_car_positions(obs, grid_step, grid_size)\n",
    "    distances = [np.linalg.norm(car) for car in car_positions]\n",
    "    closest = np.argsort(distances)[1:n+1]\n",
    "    return [car_positions[i] for i in closest]\n",
    "\n",
    "def get_distances(car_positions):\n",
    "    return [np.linalg.norm(car) for car in car_positions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space is as follows\n",
    "</br></br>\n",
    "ACTIONS_ALL = {\n",
    "        0: 'LANE_LEFT',\n",
    "        1: 'IDLE',\n",
    "        2: 'LANE_RIGHT',\n",
    "        3: 'FASTER',\n",
    "        4: 'SLOWER'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.848857801796104, 22.47220505424423]\n",
      "[10.295630140987, 22.47220505424423]\n",
      "[10.63014581273465, 21.095023109728988]\n",
      "[11.313708498984761, 21.095023109728988]\n",
      "[12.041594578792296, 20.591260281974]\n",
      "[13.601470508735444, 19.697715603592208]\n",
      "[15.264337522473747, 18.384776310850235]\n",
      "[16.1245154965971, 18.027756377319946]\n",
      "[16.492422502470642, 17.88854381999832]\n",
      "[16.278820596099706, 18.788294228055936]\n",
      "[15.132745950421556, 18.788294228055936]\n",
      "[14.560219778561036, 17.88854381999832]\n",
      "[14.560219778561036, 17.88854381999832]\n",
      "[13.601470508735444, 17.88854381999832]\n",
      "[12.649110640673518, 17.88854381999832]\n",
      "[11.40175425099138, 18.788294228055936]\n",
      "[9.486832980505138, 18.788294228055936]\n",
      "[7.615773105863909, 18.384776310850235]\n",
      "[5.385164807134504, 18.973665961010276]\n",
      "[3.605551275463989, 18.439088914585774]\n"
     ]
    }
   ],
   "source": [
    "# Render the environment slow motion and print the observations\n",
    "env = gym.make('highway-v0', render_mode='human', config=occupancyGrid)\n",
    "env.reset(seed = 500)\n",
    "env.render()\n",
    "for _ in range(100):\n",
    "    print(env.action_space.sample())\n",
    "    obs, reward, done, truncate, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "    #print(obs)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
