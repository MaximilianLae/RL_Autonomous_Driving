{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "\n",
    "    # Parametrization bellow cannot be changed\n",
    "    \"lanes_count\" : 10, # The environment must always have 10 lanes\n",
    "    \"vehicles_count\": 50, # The environment must always have 50 other vehicles\n",
    "    \"duration\": 120,  # [s] The environment must terminate never before 120 seconds\n",
    "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.IDMVehicle\", # This is the policy of the other vehicles\n",
    "    \"initial_spacing\": 2, # Initial spacing between vehicles needs to be at most 2\n",
    "\n",
    "    # Refer to refer to https://highway-env.farama.org/observations/ to change observation space type\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\"\n",
    "    },\n",
    "\n",
    "    # Refer to refer to https://highway-env.farama.org/actions/ to change action space type\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteMetaAction\",\n",
    "    },\n",
    "\n",
    "    # Parameterization bellow can be changed (as it refers mostly to the reward system)\n",
    "    # \"collision_reward\": -10,  # The reward received when colliding with a vehicle. (Can be changed)\n",
    "    # \"reward_speed_range\": [20, 30],  # [m/s] The reward for high speed is mapped linearly from this range to [0, HighwayEnv.HIGH_SPEED_REWARD]. (Can be changed)\n",
    "    \"simulation_frequency\": 15, #15,  # [Hz] (Can be changed)\n",
    "    \"policy_frequency\": 5, #5,  # [Hz] (Can be changed)\n",
    "\n",
    "    \"collision_reward\": -1000,  # The reward received when colliding with a vehicle.\n",
    "    \"right_lane_reward\": 0.1,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
    "    # zero for other lanes.\n",
    "    \"high_speed_reward\": 5,  # The reward received when driving at full speed, linearly mapped to zero for\n",
    "    # lower speeds according to config[\"reward_speed_range\"].\n",
    "    \"lane_change_reward\": 0,  # The reward received at each lane change action.\n",
    "    \"reward_speed_range\": [20, 30],\n",
    "    \n",
    "    # Parameters defined bellow are purely for visualiztion purposes! You can alter them as you please\n",
    "    \"screen_width\": 800,  # [px]\n",
    "    \"screen_height\": 600,  # [px]\n",
    "    \"centering_position\": [0.5, 0.5],\n",
    "    \"scaling\": 5,\n",
    "    \"show_trajectories\": True,\n",
    "    \"render_agent\": True,\n",
    "    \"offscreen_rendering\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = configuration.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occupancyGrid = configuration.copy()\n",
    "occupancyGrid[\"observation\"] =  {\n",
    "    \"type\": \"OccupancyGrid\",\n",
    "    # \"vehicles_count\": 50,\n",
    "    \"features\": [\n",
    "                \"presence\",\n",
    "                #\"x\", \"y\", \n",
    "                #\"vx\", \"vy\"\n",
    "                ],\n",
    "    # \"features_range\": {\n",
    "    #      \"x\": [-500, 500],\n",
    "    #      \"y\": [-500, 500],\n",
    "    #     \"vx\": [-20, 20],\n",
    "    #     \"vy\": [-20, 20]\n",
    "    # },\n",
    "    \"grid_size\": [[-100, 100], [-100, 100]],    # X controls how many lanes, Y controls how far ahead\n",
    "    \"grid_step\": [1, 1],\n",
    "    #\"absolute\": False,                     # Not implemented in the library\n",
    "    #\"as_image\": True,\n",
    "    # \"align_to_vehicle_axes\" : True\n",
    "}\n",
    "\n",
    "# The higher the number, the more frequent the policy and the simulation frequencies, the slower the simulation\n",
    "occupancyGrid[\"simulation_frequency\"] = 15\n",
    "occupancyGrid[\"policy_frequency\"] = 5\n",
    "\n",
    "env = gym.make('highway-v0', render_mode='human', config=occupancyGrid)\n",
    "\n",
    "obs, info = env.reset(seed = 30)\n",
    "\n",
    "env.close()\n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccupancyGrid():\n",
    "    def __init__(\n",
    "            self,\n",
    "            grid_size=[[-50, 50], [-50, 50]],  # X controls the lane-width, Y controls how far ahead\n",
    "            grid_step=[1, 1],\n",
    "            n_closest=3,\n",
    "            ss_bins=[5,6],\n",
    "            crop_dist=[[-10,10], [-10,25]],\n",
    "            policy=None,\n",
    "            sim_frequency=15,\n",
    "            policy_frequency=3,\n",
    "            render_mode = 'human',\n",
    "            seed = 50,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Occupancy view class constructor\n",
    "        Arguments:\n",
    "            grid_size: list of lists, the size of the grid in the x and y direction, where x controls the lane-width and y controls how far ahead. Lanes are 5m wide, and the car position is (0,0)\n",
    "            grid_step: list, the step size of the grid in the x and y direction, in meters\n",
    "            n_closest: int, the number of closest cars to consider in the state space\n",
    "            ss_bins: list, the number of bins to divide the x and y directions\n",
    "            crop_dist: list of lists, the distance to crop the x and y directions, above which the values will be clipped\n",
    "            policy: function, the policy to use in the simulation\n",
    "            sim_frequency: int, the frequency of the simulation\n",
    "            policy_frequency: int, the frequency of the policy\n",
    "            render_mode: str, the mode to render the simulation\n",
    "            seed: int, the seed to use in the simulation\n",
    "        \"\"\"\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.grid_step = grid_step\n",
    "        self.config = default_config.copy()\n",
    "        self.config[\"observation\"] =  {\n",
    "            \"type\": \"OccupancyGrid\",\n",
    "            \"features\": [\"presence\"],\n",
    "            \"grid_size\": grid_size,    # X controls how many lanes, Y controls how far ahead\n",
    "            \"grid_step\": grid_step,\n",
    "        }\n",
    "        self.config[\"simulation_frequency\"] = sim_frequency\n",
    "        self.config[\"policy_frequency\"] = policy_frequency\n",
    "        self.render_mode = render_mode\n",
    "        self.seed = seed\n",
    "        self.policy = policy\n",
    "        self.n_closest, self.ss_bins, self.crop_dist = n_closest, ss_bins, crop_dist\n",
    "        self.initialize_states()\n",
    "\n",
    "    def initialize_states(self):\n",
    "        \"\"\"\n",
    "        Initialize the states of the occupancy grid\n",
    "        \"\"\"\n",
    "        # States will be stored in a dictionary, with the key being ((x1,x2,...,xn), (y1,y2,...,yn)), and n is the number of neighbors\n",
    "        # Make ss_bins[0] from the crop_dist[0] and ss_bins[1] from crop_dist[1]\n",
    "        self.x_bins = np.linspace(self.crop_dist[0][0], self.crop_dist[0][1], self.ss_bins[0])\n",
    "        self.y_bins = np.linspace(self.crop_dist[1][0], self.crop_dist[1][1], self.ss_bins[1])\n",
    "\n",
    "        # Each of the nearest neighbors will have a state of the form (x,y). Create the first key of the dictionary in the form (x1,x2,...xn)\n",
    "        x_keys = list(itertools.product(self.x_bins, repeat=self.n_closest))\n",
    "        y_keys = list(itertools.product(self.y_bins, repeat=self.n_closest))\n",
    "        self.states = list(itertools.product(x_keys, y_keys))\n",
    "\n",
    "    def get_car_positions(self):\n",
    "        positions = np.nonzero(self.current_obs[0])\n",
    "        car_positions = np.array([positions[0]*self.grid_step[0] + self.grid_size[0][0], positions[1]*self.grid_step[1] + self.grid_size[1][0]]).T\n",
    "        return car_positions\n",
    "\n",
    "    def get_n_closest(self):\n",
    "        car_positions = self.get_car_positions()\n",
    "        distances = np.linalg.norm(car_positions, axis=1)\n",
    "\n",
    "        # Remove the agent position\n",
    "        closest = np.argsort(distances)[1:self.n_closest+1]\n",
    "        closest_car_positions = car_positions[closest]\n",
    "\n",
    "        # If there are less than n_closest cars, pad the array with the crop_dist values\n",
    "        if len(closest_car_positions) < self.n_closest:\n",
    "            n_missing = self.n_closest - len(closest_car_positions)\n",
    "            closest_car_positions = np.pad(closest_car_positions, ((0, n_missing), (0,0)), 'constant', constant_values=(self.crop_dist[0][0], self.crop_dist[1][0]))\n",
    "\n",
    "        # Values that are \n",
    "        return closest_car_positions\n",
    "    \n",
    "    def get_state(self):\n",
    "        n_closest = self.get_n_closest()\n",
    "        # For the closest cars, get the state\n",
    "        state_x, state_y = [], []\n",
    "        # Get the bin values for each of the x,y positions, and return a tuple with the values\n",
    "        for car in n_closest:\n",
    "            x = np.digitize(car[0], self.x_bins) - 1\n",
    "            y = np.digitize(car[1], self.y_bins) - 1\n",
    "            x_val, y_val = self.x_bins[x], self.y_bins[y]\n",
    "            state_x.append(x_val)\n",
    "            state_y.append(y_val)\n",
    "        state = (tuple(state_x), tuple(state_y))\n",
    "        return tuple(state)\n",
    "\n",
    "    def test_env(self):\n",
    "        \"\"\"\n",
    "        Function to test the environment with a random policy, or with a policy\n",
    "        \"\"\"\n",
    "        env = gym.make('highway-v0', render_mode=self.render_mode, config=self.config)\n",
    "        obs, info = env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        while not done:\n",
    "            start = time.time()\n",
    "            if self.policy is None:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = self.policy()\n",
    "            obs, reward, done, truncate, info = env.step(action)\n",
    "            self.current_obs = obs\n",
    "            # print(self.get_state())\n",
    "            # time.sleep(2)\n",
    "            end = time.time()\n",
    "            print(f\"Time taken: {end-start}\")\n",
    "        env.close()\n",
    "        return info[\"score\"]\n",
    "\n",
    "\n",
    "# ------------------- SARSA -------------------    \n",
    "class Sarsa(OccupancyGrid):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha=0.6,\n",
    "        gamma=0.95,\n",
    "        m=50,\n",
    "        epsilon=0.5,\n",
    "        print_stats=False,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        SARSA class constructor\n",
    "        Arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            m: int, the number of episodes to train the agent for\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.initialize_Q()\n",
    "        self.alpha, self.gamma, self.m, self.epsilon = alpha, gamma, m, epsilon\n",
    "\n",
    "    def policy_Q(self, state):\n",
    "        values = [self.Q[(state, action)] for action in range(5)]\n",
    "        return np.argmax(values)   \n",
    "\n",
    "    def initialize_Q(self):\n",
    "        # Combine the possible states with the possible actions\n",
    "        keys = list(itertools.product(self.states, range(5)))       # 5 possible actions, 0-4: left, idle, right, accelerate, decelerate\n",
    "        if len(keys) > 150000:\n",
    "            print(\"Warning: The number of states is too large, consider reducing the number of states\")\n",
    "        self.Q = {key: 0 for key in keys}\n",
    "\n",
    "    def epsilon_greedy(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(5)\n",
    "        else:\n",
    "            values = [self.Q[(state, action)] for action in range(5)]\n",
    "            return np.argmax(values)\n",
    "        \n",
    "    def train(self): \n",
    "        env = gym.make('highway-v0', render_mode=None, config=self.config)\n",
    "        obs, info = env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        state = self.get_state()\n",
    "        action = self.epsilon_greedy(state)\n",
    "        for i in tqdm(range(self.m)):\n",
    "            env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                next_state = self.get_state()\n",
    "                next_action = self.epsilon_greedy(next_state)\n",
    "                self.Q[(state, action)] += self.alpha*(reward + self.gamma*self.Q[(next_state, next_action)] - self.Q[(state, action)])\n",
    "                state, action = next_state, next_action\n",
    "                self.current_obs = next_obs\n",
    "        env.close()\n",
    "\n",
    "    def test(self):\n",
    "        env = gym.make('highway-v0', render_mode=self.render_mode, config=self.config)\n",
    "        obs, info = env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        state = self.get_state()\n",
    "        action = self.policy_Q(state)\n",
    "        while not done:\n",
    "            next_obs, reward, done, truncate, info = env.step(action)\n",
    "            next_state = self.get_state()\n",
    "            next_action = self.policy_Q(next_state)\n",
    "            state, action = next_state, next_action\n",
    "            self.current_obs = next_obs\n",
    "            print(next_state)\n",
    "        env.close()\n",
    "        return info[\"score\"]\n",
    "\n",
    "    def run(self, n_episodes):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- grid_size=[[-50, 50], [-50, 50]],  # X controls the lane-width, Y controls how far ahead\n",
    "- grid_step=[1, 1],\n",
    "- n_closest=3,\n",
    "- ss_bins=[5,6],\n",
    "- crop_dist=[[-10,10], [-10,25]],\n",
    "- policy=None,\n",
    "- sim_frequency=15,\n",
    "- policy_frequency=5,\n",
    "- render_mode = 'human',\n",
    "- seed = 50,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-15.  -9.  -3.   3.   9.  15.] [-10.  -5.   0.   5.  10.  15.  20.] 210\n"
     ]
    }
   ],
   "source": [
    "copy_Q = a.Q.copy()\n",
    "a = Sarsa(n_closest=1, ss_bins=[6,7], crop_dist=[[-15, 15], [-10, 20]])\n",
    "a.Q = copy_Q.copy()\n",
    "print(a.x_bins, a.y_bins, len(a.Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((-3.0,), (10.0,))\n",
      "((-3.0,), (5.0,))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((3.0,), (5.0,))\n",
      "((9.0,), (5.0,))\n",
      "((9.0,), (5.0,))\n",
      "((9.0,), (5.0,))\n",
      "((9.0,), (0.0,))\n",
      "((9.0,), (0.0,))\n",
      "((15.0,), (0.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (15.0,))\n",
      "((-9.0,), (15.0,))\n",
      "((-9.0,), (15.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (10.0,))\n",
      "((-9.0,), (5.0,))\n",
      "((-9.0,), (5.0,))\n",
      "((-9.0,), (0.0,))\n",
      "((-9.0,), (-5.0,))\n",
      "((-9.0,), (-5.0,))\n",
      "((-9.0,), (-10.0,))\n",
      "((-9.0,), (20.0,))\n",
      "((-9.0,), (5.0,))\n",
      "((-9.0,), (0.0,))\n",
      "((-9.0,), (0.0,))\n",
      "((-9.0,), (-5.0,))\n",
      "((-9.0,), (-10.0,))\n",
      "((-9.0,), (-10.0,))\n",
      "((3.0,), (5.0,))\n",
      "((3.0,), (5.0,))\n",
      "((9.0,), (0.0,))\n",
      "((9.0,), (-5.0,))\n",
      "((9.0,), (-5.0,))\n",
      "((9.0,), (-10.0,))\n",
      "((3.0,), (-10.0,))\n",
      "((3.0,), (20.0,))\n",
      "((3.0,), (20.0,))\n",
      "((3.0,), (20.0,))\n",
      "((9.0,), (15.0,))\n",
      "((3.0,), (10.0,))\n",
      "((3.0,), (10.0,))\n",
      "((3.0,), (5.0,))\n",
      "((-3.0,), (5.0,))\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[48], line 197\u001b[0m, in \u001b[0;36mSarsa.test\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28mprint\u001b[39m(next_state)\n\u001b[0;32m    196\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'score'"
     ]
    }
   ],
   "source": [
    "a.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.61904761904762\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for keys, values in a.Q.items():\n",
    "    if values != 0:\n",
    "        count += 1\n",
    "\n",
    "print(100*count/len(a.Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f08b9d04e2441b0a1ccac896ceba930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[36], line 174\u001b[0m, in \u001b[0;36mSarsa.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    172\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m--> 174\u001b[0m     next_obs, reward, done, truncate, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state()\n\u001b[0;32m    176\u001b[0m     next_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_greedy(next_state)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:257\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_control\u001b[39m\u001b[38;5;124m\"\u001b[39m] \\\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:324\u001b[0m, in \u001b[0;36mRoad.act\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decide the actions of each entity on the road.\"\"\"\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[1;32m--> 324\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:100\u001b[0m, in \u001b[0;36mIDMVehicle.act\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     97\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Longitudinal: IDM\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m front_vehicle, rear_vehicle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneighbour_vehicles\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlane_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macceleration(ego_vehicle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    102\u001b[0m                                            front_vehicle\u001b[38;5;241m=\u001b[39mfront_vehicle,\n\u001b[0;32m    103\u001b[0m                                            rear_vehicle\u001b[38;5;241m=\u001b[39mrear_vehicle)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# When changing lane, check both current and target lanes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:361\u001b[0m, in \u001b[0;36mRoad.neighbour_vehicles\u001b[1;34m(self, vehicle, lane_index)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vehicle \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, Landmark):  \u001b[38;5;66;03m# self.network.is_connected_road(v.lane_index,\u001b[39;00m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;66;03m# lane_index, same_lane=True):\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m         s_v, lat_v \u001b[38;5;241m=\u001b[39m \u001b[43mlane\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lane\u001b[38;5;241m.\u001b[39mon_lane(v\u001b[38;5;241m.\u001b[39mposition, s_v, lat_v, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    363\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:190\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    188\u001b[0m delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m    189\u001b[0m longitudinal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection)\n\u001b[1;32m--> 190\u001b[0m lateral \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_occupancy = OcupancyGrid(render_mode=None)\n",
    "# print(new_occupancy.x_bins, new_occupancy.y_bins, len(new_occupancy.states))\n",
    "# new_occupancy.test_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________\n",
    "### Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_car_positions(obs, grid_step=1, grid_size=50):\n",
    "    # car_positions = []\n",
    "    # for i in range(int(grid_size*2 / grid_step)):\n",
    "    #     for j in range(int(grid_size*2 / grid_step)):\n",
    "    #         if obs[0,i,j] == 1:\n",
    "    #             car_positions.append([i*grid_step - grid_size, j*grid_step - grid_size])\n",
    "    positions = np.nonzero(obs[0])\n",
    "    car_positions = list(zip(positions[0]*grid_step - grid_size, positions[1]*grid_step - grid_size))\n",
    "    return car_positions\n",
    "\n",
    "def get_n_closest(obs, grid_step=1, grid_size=50, n=3):\n",
    "    car_positions = return_car_positions(obs, grid_step, grid_size)\n",
    "    distances = [np.linalg.norm(car) for car in car_positions]\n",
    "    closest = np.argsort(distances)[1:n+1]\n",
    "    return [car_positions[i] for i in closest]\n",
    "\n",
    "def get_distances(car_positions):\n",
    "    return [np.linalg.norm(car) for car in car_positions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space is as follows\n",
    "</br></br>\n",
    "ACTIONS_ALL = {\n",
    "        0: 'LANE_LEFT',\n",
    "        1: 'IDLE',\n",
    "        2: 'LANE_RIGHT',\n",
    "        3: 'FASTER',\n",
    "        4: 'SLOWER'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.848857801796104, 22.47220505424423]\n",
      "[10.295630140987, 22.47220505424423]\n",
      "[10.63014581273465, 21.095023109728988]\n",
      "[11.313708498984761, 21.095023109728988]\n",
      "[12.041594578792296, 20.591260281974]\n",
      "[13.601470508735444, 19.697715603592208]\n",
      "[15.264337522473747, 18.384776310850235]\n",
      "[16.1245154965971, 18.027756377319946]\n",
      "[16.492422502470642, 17.88854381999832]\n",
      "[16.278820596099706, 18.788294228055936]\n",
      "[15.132745950421556, 18.788294228055936]\n",
      "[14.560219778561036, 17.88854381999832]\n",
      "[14.560219778561036, 17.88854381999832]\n",
      "[13.601470508735444, 17.88854381999832]\n",
      "[12.649110640673518, 17.88854381999832]\n",
      "[11.40175425099138, 18.788294228055936]\n",
      "[9.486832980505138, 18.788294228055936]\n",
      "[7.615773105863909, 18.384776310850235]\n",
      "[5.385164807134504, 18.973665961010276]\n",
      "[3.605551275463989, 18.439088914585774]\n"
     ]
    }
   ],
   "source": [
    "# Render the environment slow motion and print the observations\n",
    "env = gym.make('highway-v0', render_mode='human', config=occupancyGrid)\n",
    "env.reset(seed = 500)\n",
    "env.render()\n",
    "for _ in range(100):\n",
    "    print(env.action_space.sample())\n",
    "    obs, reward, done, truncate, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "    #print(obs)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
