{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "\n",
    "    # Parametrization bellow cannot be changed\n",
    "    \"lanes_count\" : 10, # The environment must always have 10 lanes\n",
    "    \"vehicles_count\": 50, # The environment must always have 50 other vehicles\n",
    "    \"duration\": 120,  # [s] The environment must terminate never before 120 seconds\n",
    "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.IDMVehicle\", # This is the policy of the other vehicles\n",
    "    \"initial_spacing\": 2, # Initial spacing between vehicles needs to be at most 2\n",
    "\n",
    "    # Refer to refer to https://highway-env.farama.org/observations/ to change observation space type\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\"\n",
    "    },\n",
    "\n",
    "    # Refer to refer to https://highway-env.farama.org/actions/ to change action space type\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteMetaAction\",\n",
    "    },\n",
    "\n",
    "    # Parameterization bellow can be changed (as it refers mostly to the reward system)\n",
    "    # \"collision_reward\": -10,  # The reward received when colliding with a vehicle. (Can be changed)\n",
    "    # \"reward_speed_range\": [20, 30],  # [m/s] The reward for high speed is mapped linearly from this range to [0, HighwayEnv.HIGH_SPEED_REWARD]. (Can be changed)\n",
    "    \"simulation_frequency\": 15, #15,  # [Hz] (Can be changed)\n",
    "    \"policy_frequency\": 5, #5,  # [Hz] (Can be changed)\n",
    "\n",
    "    \"collision_reward\": -100,  # The reward received when colliding with a vehicle.\n",
    "    \"right_lane_reward\": 0.1,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
    "    # zero for other lanes.\n",
    "    \"high_speed_reward\": 100,  # The reward received when driving at full speed, linearly mapped to zero for\n",
    "    # lower speeds according to config[\"reward_speed_range\"].\n",
    "    \"lane_change_reward\": 0,  # The reward received at each lane change action.\n",
    "    \"reward_speed_range\": [20, 30],\n",
    "    \n",
    "    # Parameters defined bellow are purely for visualiztion purposes! You can alter them as you please\n",
    "    \"screen_width\": 800,  # [px]\n",
    "    \"screen_height\": 600,  # [px]\n",
    "    \"centering_position\": [0.5, 0.5],\n",
    "    \"scaling\": 5,\n",
    "    \"show_trajectories\": True,\n",
    "    \"render_agent\": True,\n",
    "    \"offscreen_rendering\": False\n",
    "}\n",
    "\n",
    "default_config = configuration.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 91.46669937980411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occupancyGrid = configuration.copy()\n",
    "occupancyGrid[\"observation\"] =  {\n",
    "    \"type\": \"OccupancyGrid\",\n",
    "    # \"vehicles_count\": 50,\n",
    "    \"features\": [\n",
    "                \"presence\",\n",
    "                #\"x\", \"y\", \n",
    "                #\"vx\", \"vy\"\n",
    "                ],\n",
    "    # \"features_range\": {\n",
    "    #      \"x\": [-500, 500],\n",
    "    #      \"y\": [-500, 500],\n",
    "    #     \"vx\": [-20, 20],\n",
    "    #     \"vy\": [-20, 20]\n",
    "    # },\n",
    "    \"grid_size\": [[-100, 100], [-100, 100]],    # X controls how many lanes, Y controls how far ahead\n",
    "    \"grid_step\": [1, 1],\n",
    "    #\"absolute\": False,                     # Not implemented in the library\n",
    "    #\"as_image\": True,\n",
    "    # \"align_to_vehicle_axes\" : True\n",
    "}\n",
    "\n",
    "# The higher the number, the more frequent the policy and the simulation frequencies, the slower the simulation\n",
    "occupancyGrid[\"simulation_frequency\"] = 15\n",
    "occupancyGrid[\"policy_frequency\"] = 1\n",
    "occupancyGrid[\"normalize_reward\"] = False\n",
    "\n",
    "env = gym.make('highway-v0', render_mode='human', config=occupancyGrid)\n",
    "\n",
    "obs, info = env.reset(seed = 30)\n",
    "# Do one step\n",
    "action = 3\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "print(\"Reward:\", reward)\n",
    "\n",
    "env.close()\n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- grid_size=[[-50, 50], [-50, 50]],  # X controls the lane-width, Y controls how far ahead\n",
    "- grid_step=[1, 1],\n",
    "- n_closest=3,\n",
    "- ss_bins=[5,6],\n",
    "- crop_dist=[[-10,10], [-10,25]],\n",
    "- policy=None,\n",
    "- sim_frequency=15,\n",
    "- policy_frequency=5,\n",
    "- render_mode = 'human',\n",
    "- seed = 50,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! TODO !!!\n",
    "\n",
    "- Use kinematics to substitute the current occupancy grid class\n",
    "\n",
    "- He cannot know when he cant turn left or right because the lane is the final one\n",
    "\n",
    "- Change the occupancy class to use occupancy grid with 5m grid size, 3 lanes and -30, 30 ahead\n",
    "\n",
    "- Make a plot history \n",
    "\n",
    "- Check how many times each state is visited\n",
    "\n",
    "- Check the action distribution, so as to see if slowing down is the most chosen action and the one with the best Q value\n",
    "\n",
    "- Change the reward for colision, so that the agent goes faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! IDEA WITH KINEMATICS !!!\n",
    "- State space in this maner: (danger ahead, danger left, danger right, danger behind, lane position, (maybe) speed)\n",
    "- Lane position can be 0 if in the middle, 1 if in the right, -1 if in the left\n",
    "\n",
    "- We need speed, because if the speed is too fast, it might not be able to turn in time\n",
    "- Actually, we could just increase the safety distance, and then we wouldn't need speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________\n",
    "### Using kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 83.56591  ,  28.       ,  25.       ,   0.       ],\n",
       "       [  9.4535885,   8.       ,  -1.5146654,   0.       ],\n",
       "       [ 20.187185 , -24.       ,  -1.9328905,   0.       ],\n",
       "       [ 31.133034 , -24.       ,  -1.129222 ,   0.       ],\n",
       "       [ 41.896053 ,   0.       ,  -1.7400944,   0.       ],\n",
       "       [ 51.945835 ,  -8.       ,  -3.565016 ,   0.       ],\n",
       "       [ 62.276524 , -16.       ,  -3.3216567,   0.       ],\n",
       "       [ 72.60146  ,   8.       ,  -1.0905089,   0.       ],\n",
       "       [ 82.24522  ,  -8.       ,  -2.182405 ,   0.       ],\n",
       "       [ 91.352806 , -20.       ,  -3.5367248,   0.       ],\n",
       "       [100.35392  , -16.       ,  -3.2140508,   0.       ],\n",
       "       [109.795944 ,  -4.       ,  -3.2532372,   0.       ],\n",
       "       [119.59274  ,   0.       ,  -1.6719042,   0.       ],\n",
       "       [129.31784  ,   4.       ,  -2.6289418,   0.       ],\n",
       "       [138.54662  ,  -4.       ,  -2.0985248,   0.       ],\n",
       "       [149.64609  , -16.       ,  -1.698277 ,   0.       ],\n",
       "       [159.69713  ,   0.       ,  -1.612952 ,   0.       ],\n",
       "       [169.55093  ,   4.       ,  -2.264635 ,   0.       ],\n",
       "       [179.66725  ,   0.       ,  -3.794033 ,   0.       ],\n",
       "       [190.1403   ,   4.       ,  -2.7294507,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kinematics = configuration.copy()\n",
    "kinematics[\"observation\"] =  {\n",
    "    \"type\": \"Kinematics\",\n",
    "    \"vehicles_count\": 50,\n",
    "    \"features\": [\"x\", \"y\", \"vx\", \"vy\"],\n",
    "    # \"features_range\": {\n",
    "    #     \"x\": [-40, 40],\n",
    "    #     \"y\": [-40, 40],\n",
    "    #     \"vx\": [-200, 200],\n",
    "    #     \"vy\": [-200, 200]\n",
    "    # }, \n",
    "    \"absolute\": False,\n",
    "    \"normalize\": False,\n",
    "}\n",
    "\n",
    "# The higher the number, the more frequent the policy and the simulation frequencies, the slower the simulation\n",
    "kinematics[\"simulation_frequency\"] = 20\n",
    "kinematics[\"policy_frequency\"] = 5\n",
    "\n",
    "env = gym.make('highway-v0', render_mode='human', config=kinematics)\n",
    "\n",
    "obs, info = env.reset(seed = 10)\n",
    "\n",
    "env.close()\n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make the dictionary less strict\n",
    "\n",
    "optimum_q_kinematics = {\n",
    "    (0,0,0,0,0,0) : 2, \n",
    "    (0,0,0,0,1,0) : 3,\n",
    "    (0,0,0,0,-1,0) : 2, \n",
    "    (1,0,0,0,0,0) : 2,\n",
    "    (1,0,0,0,1,0) : 0,\n",
    "    (1,0,0,0,-1,0) : 2,\n",
    "    (0,0,1,0,0,0) : 2,\n",
    "    (0,0,1,0,1,0) : 3,\n",
    "    (0,0,0,1,0,0) : 3,\n",
    "    (0,0,0,1,-1,0) : 3,\n",
    "    (1,0,1,1,0,0) : 4,\n",
    "    (0,0,1,1,0,0) : 3,\n",
    "    (1,0,0,1,0,0) : 0,\n",
    "    (1,0,0,1,-1,0) : 4,\n",
    "    (1,0,1,0,0,0) : 2,\n",
    "    (1,0,1,0,1,0) : 4,\n",
    "\n",
    "    (0,0,0,0,0,1) : 3,    # !! ALL 3s could have to be substituted with 4s \n",
    "    (0,0,0,0,1,1) : 3, \n",
    "    (0,0,0,0,-1,1) : 3,  # Could have only turned right before, so faster \n",
    "    (1,0,0,0,0,1) : 3,   # Car in front and just turned, so the car will soon be on my left or right\n",
    "    (1,0,0,0,1,1) : 3,  \n",
    "    (1,0,0,0,-1,1) : 3,\n",
    "    (0,0,1,0,0,1) : 3,\n",
    "    (0,0,1,0,1,1) : 3,\n",
    "    (0,0,0,1,0,1) : 3,\n",
    "    (0,0,0,1,-1,1) : 4,   # Car on the right, couldnt have turned left, so soon will be in front\n",
    "    (1,0,1,1,0,1) : 4,    # Terrible, brace\n",
    "    (0,0,1,1,0,1) : 4,    # Terrible \n",
    "    (1,0,0,1,0,1) : 3,    # I have turned left\n",
    "    (1,0,0,1,-1,1) : 4,   # On the right and in front, have turned right, terrible\n",
    "    (1,0,1,0,0,1) : 3,    # On the left and in front, have turned right\n",
    "    (1,0,1,0,1,1) : 4,    # On the left, cant turn right, could only have turned left, terrible\n",
    "    #(0,1,0,0,0) : 2,\n",
    "    #(0,1,0,0,1) : 3,\n",
    "    #(0,1,0,0,-1) : 2,\n",
    "    # (0,0,1,0,-1) : 2,\n",
    "    # (0,0,0,1,1) : 3,\n",
    "    #(1,1,0,0,0) : 2,\n",
    "    #(1,1,0,0,1) : 0,\n",
    "    #(1,1,0,0,-1) : 2,\n",
    "    #(1,1,1,0,0) : 2,\n",
    "    #(1,1,1,0,1) : 2,\n",
    "    # (1,1,1,0,-1) : 1,\n",
    "    #(1,1,1,1,0) : 1,\n",
    "    # (1,1,1,1,1) : 1,\n",
    "    # (1,1,1,1,-1) : 1,\n",
    "    #(1,1,0,1,0) : 0,\n",
    "    # (1,1,0,1,1) : 0,\n",
    "    #(1,1,0,1,-1) : 1,\n",
    "    # (1,0,1,1,1) : 1,\n",
    "    # (1,0,1,1,-1) : 1,\n",
    "    # (0,1,1,1,0) : 3,\n",
    "    # (0,1,1,1,1) : 3,\n",
    "    # (0,1,1,1,-1) : 3,\n",
    "    # (0,0,1,1,1) : 3,\n",
    "    # (0,0,1,1,-1) : 3,\n",
    "    # (0,1,0,1,0) : 3,\n",
    "    # (0,1,0,1,1) : 3,\n",
    "    # (0,1,0,1,-1) : 3,\n",
    "    # (0,1,1,0,0) : 2,\n",
    "    # (0,1,1,0,1) : 3,\n",
    "    # (0,1,1,0,-1) : 2,\n",
    "    # (1,0,0,1,1) : 0,\n",
    "    #(1,0,1,0,-1) : 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_reward(reward, position, action, to_right_reward=5, to_right_skewness=2, change_lane_reward=-0.5):\n",
    "    \"\"\"\n",
    "    This function is used to correct the reward function, which is not correctly outputted by the environment\n",
    "    Params: \n",
    "        reward: float, the reward to fix    \n",
    "        position: tuple, the position of the car\n",
    "        action: int, the action taken by the car\n",
    "        to_right_reward: float, the reward to give to the driver \n",
    "        to_right_skewness: float, the skewness to apply to the to_right_reward\n",
    "        change_lane_reward: float, the reward to give when changing lanes\n",
    "    \"\"\"\n",
    "    \n",
    "    lane = position[1]\n",
    "    lane_value = to_right_reward * ((lane/36)**to_right_skewness)\n",
    "    lane_change = 1 if action in [0, 2] else 0\n",
    "    reward = reward + lane_value + lane_change * change_lane_reward    \n",
    "    return reward\n",
    "\n",
    "def decode_meta_action(action):\n",
    "    \"\"\"\n",
    "    Function to output the corresponding action in text-form\n",
    "    \"\"\"\n",
    "    assert action in range(5), \"The action must be between 0 and 4\"\n",
    "    if action == 0:\n",
    "        return \"LANE_LEFT\"\n",
    "    elif action == 1:\n",
    "        return \"IDLE\"\n",
    "    elif action == 2:\n",
    "        return \"LANE_RIGHT\"\n",
    "    elif action == 3:\n",
    "        return \"FASTER\"\n",
    "    elif action == 4:\n",
    "        return \"SLOWER\"\n",
    "    \n",
    "def decode_danger(state):\n",
    "    \"\"\"\n",
    "    Function to decode the danger state\n",
    "    \"\"\"\n",
    "    state_meaning = ['front', 'back', 'left', 'right']\n",
    "    to_return = ''\n",
    "    for i in range(3): \n",
    "        if state[i] == 1:\n",
    "            if to_return == '':\n",
    "                to_return = 'Danger in '\n",
    "            to_return += state_meaning[i] + ', '\n",
    "    if to_return == '': \n",
    "        to_return = 'No danger'\n",
    "    if state[4] == -1:\n",
    "        to_return += '. Cant turn left'\n",
    "    elif state[4] == 1:\n",
    "        to_return += '. Cant turn right'\n",
    "    return to_return\n",
    "    \n",
    "\n",
    "def decode_Q(Q): \n",
    "    \"\"\"\n",
    "    Function to decode the Q-values\n",
    "    \"\"\"\n",
    "    return {(decode_danger(key[0]), decode_meta_action(key[1])) : value for key, value in Q.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "class ObservationType: \n",
    "    def __init__(self, \n",
    "                sim_frequency=10,\n",
    "                policy_frequency=2,\n",
    "                render_mode='human',\n",
    "                seed=None,\n",
    "                colision_reward=-20, high_speed_reward=5, reward_speed_range=[20, 30], to_right_reward=5, to_right_skewness=2, change_lane_reward=-0.5):\n",
    "\n",
    "        \"\"\"\n",
    "        Constructor for the ObservationType class\n",
    "        Arguments:\n",
    "            sim_frequency: int, the frequency of the simulation\n",
    "            policy_frequency: int, the frequency of the policy\n",
    "            render_mode: str, the mode to render the simulation\n",
    "            seed: int, the seed to use in the simulation\n",
    "            colision_reward: float, the reward to give when a colision occurs\n",
    "            high_speed_reward: float, the reward to give when driving at high speed\n",
    "            reward_speed_range: list, the range of speeds to give the high speed reward\n",
    "            to_right_reward: float, the reward to give when driving to the right, mapped polynomially with to_right_skewness\n",
    "            to_right_skewness: float, the skewness to apply to the to_right_reward\n",
    "            change_lane_reward: float, the reward to give when changing lanes\n",
    "        \"\"\"\n",
    "        self.config = default_config.copy()\n",
    "\n",
    "        self.config.update({\n",
    "            \"duration\": 50,\n",
    "            \"simulation_frequency\": sim_frequency,\n",
    "            \"policy_frequency\": policy_frequency,\n",
    "            \"collision_reward\": colision_reward,\n",
    "            \"high_speed_reward\": high_speed_reward,\n",
    "            \"reward_speed_range\": reward_speed_range\n",
    "        })\n",
    "        self.config['normalize_reward'] = False\n",
    "        self.seed = seed\n",
    "        self.render_mode = render_mode\n",
    "        self.to_right_reward, self.to_right_skewness, self.change_lane_reward = to_right_reward, to_right_skewness, change_lane_reward\n",
    "\n",
    "\n",
    "class Kinematics(ObservationType):\n",
    "    def __init__(self, \n",
    "                 seed=None, \n",
    "                 state_type='danger',\n",
    "                 policy=None,\n",
    "                 crop=100, lane_tolerance=2.5, danger_threshold_x=10, danger_threshold_y=15, x_speed_coef=1, y_speed_coef=1,\n",
    "                 special_Q=False, past_action_len=2,\n",
    "                 **kwargs):\n",
    "\n",
    "            \"\"\"\n",
    "            Kinematics class constructor\n",
    "            Arguments:\n",
    "                seed: int, the seed to use in the test environment\n",
    "                state_type: str, the type of state to use. Options are 'n_neighbours' or 'danger'\n",
    "                policy: function, the policy to use in the simulation\n",
    "                crop: int, the crop distance to use in the state\n",
    "                lane_tolerance: int, the tolerance to use in the lane\n",
    "                danger_threshold_x: float, the threshold to use in the x direction for the danger state\n",
    "                danger_threshold_y: float, the threshold to use in the y direction for the danger state\n",
    "                x_speed_coef: float, the coefficient to use in the x direction for the danger state\n",
    "                y_speed_coef: float, the coefficient to use in the y direction for the danger state\n",
    "                special_Q: Q function includes the past action\n",
    "                past_action_len : int, the length of the past actions to consider for the turns done\n",
    "            \n",
    "            Other Arguments (for Observation Type):\n",
    "                sim_frequency: int, the frequency of the simulation\n",
    "                policy_frequency: int, the frequency of the policy\n",
    "                render_mode: str, the mode to render the simulation\n",
    "                seed: int, the seed to use in the simulation\n",
    "                colision_reward: float, the reward to give when a colision occurs\n",
    "                high_speed_reward: float, the reward to give when driving at high speed\n",
    "                reward_speed_range: list, the range of speeds to give the high speed reward\n",
    "                to_right_reward: float, the reward to give when driving to the right, mapped polynomially with to_right_skewness\n",
    "                to_right_skewness: float, the skewness to apply to the to_right_reward\n",
    "                change_lane_reward: float, the reward to give when changing lanes\n",
    "            \"\"\"\n",
    "            super().__init__(**kwargs)\n",
    "\n",
    "            self.config[\"observation\"] =  {\n",
    "                \"type\": \"Kinematics\",\n",
    "                \"vehicles_count\": 50,\n",
    "                \"features\": [\"x\", \"y\", \"vx\", \"vy\"],\n",
    "                \"absolute\": False,\n",
    "                \"normalize\": False,\n",
    "            }\n",
    "\n",
    "            self.policy = policy\n",
    "\n",
    "            with gym.make('highway-v0', render_mode=self.render_mode, config=self.config) as env:\n",
    "                self.env = env\n",
    "                obs, info = env.reset(seed = self.seed)\n",
    "                self.current_obs = obs\n",
    "                ones = np.ones(past_action_len)\n",
    "                self.past_actions = deque(ones,maxlen=past_action_len)\n",
    "\n",
    "            self.state_type = state_type\n",
    "            self.special_Q = special_Q\n",
    "            self.crop, self.lane_tolerance, self.danger_threshold_x, self.danger_threshold_y, self.x_speed_coef, self.y_speed_coef = crop, lane_tolerance, danger_threshold_x, danger_threshold_y, x_speed_coef, y_speed_coef\n",
    "            self.initialize_states()\n",
    "\n",
    "    def initialize_states(self):\n",
    "        \"\"\"\n",
    "        Initialize the states of the occupancy grid\n",
    "        \"\"\"\n",
    "        # If the state type is danger, we need to initialize the states\n",
    "        if self.state_type == 'danger':\n",
    "            self.state_size = 16  # Removing impossible combinations from the 48 total ones \n",
    "            # The states will be the possible combinations of 0s and 1s for the 4 features + {-1,0,1} for the lane \n",
    "            a = list(itertools.product([0, 1], repeat=4))\n",
    "            # Now we need to add the product of {-1,0,1}\n",
    "            a = list(itertools.product(a, [-1, 0, 1]))\n",
    "            flattened = [(*x, y) for x, y in a]\n",
    "            if self.special_Q:\n",
    "                self.state_size *= 2\n",
    "                turn_states = list(itertools.product([0,1], repeat=1))\n",
    "                flattened = list(itertools.product(flattened, turn_states))\n",
    "                flattened = [(*x, *y) for x, y in flattened]\n",
    "            self.states = flattened\n",
    "\n",
    "        elif self.state_type == 'binned': \n",
    "            pass\n",
    "\n",
    "    def get_state(self, return_values=False, decode=False, debug=False):\n",
    "        \"\"\"\n",
    "        Get the state of the environment\n",
    "        Arguments:\n",
    "            self.state_type: str, the type of state to get. Options are 'n_neighbours' or 'danger' :\n",
    "                n_neighbours: the state is the n closest neighbours in the form (x1,x2,...,xn), (y1,y2,...,yn)\n",
    "                danger: the state is an array with 4 binary variables representing whether there is danger ahead, behind, on the left or right lanes\n",
    "            return_values: bool, whether to return the values of the state\n",
    "            decode: bool, whether to decode the state\n",
    "            debug: bool, whether to print the debug information\n",
    "        \"\"\"\n",
    "        assert self.state_type in ['n_neighbours', 'danger'], \"The type of state must be either 'n_neighbours' or 'lane-wise'\"\n",
    "        if self.state_type == 'n_neighbours':\n",
    "            state = self.state_n_neighbours(return_values)\n",
    "        elif self.state_type == 'danger':\n",
    "            state = self.state_danger(return_values, debug)\n",
    "        return state\n",
    "\n",
    "    def state_danger(self, return_values=False, debug=False):\n",
    "        \"\"\"\n",
    "        Get the state of the environment\n",
    "        \"\"\"\n",
    "        def get_sign(num): \n",
    "            sign = num/np.abs(num)\n",
    "            return sign\n",
    "\n",
    "        lane = self.current_obs[0,1]\n",
    "        observation = self.current_obs[1:][:,0:4]\n",
    "        observation = observation[~np.all(observation == 0, axis=1)]\n",
    "\n",
    "        # Lane observations\n",
    "        same_lane = observation[np.abs(observation[:,1]) <= self.lane_tolerance]\n",
    "        lane_front = same_lane[(same_lane[:,0] > 0)][:3]\n",
    "        lane_back = same_lane[(same_lane[:,0] < 0)][:3]\n",
    "\n",
    "        # For the left and right lanes we consider 2 lanes, instead of just one \n",
    "        left_lanes = observation[(observation[:,1] >= -8 - self.lane_tolerance) & (observation[:,1] <= -4 + self.lane_tolerance)][:3]\n",
    "        right_lanes = observation[(observation[:,1] <= 8 + self.lane_tolerance) & (observation[:,1] >= 4 - self.lane_tolerance)][:3]\n",
    "\n",
    "        # Calculating the adjusted distances\n",
    "        front_dist = lane_front[0,0] if len(lane_front) > 0 else self.crop \n",
    "        front_speed_diff = lane_front[0,2] if len(lane_front) > 0 else 0\n",
    "        front_adj_dist = front_dist + self.x_speed_coef*front_speed_diff   # The more the front speed diff, the harder it is to get to the car in the front, so adjusted distance is higher\n",
    "\n",
    "        back_dist = -lane_back[0,0] if len(lane_back) > 0 else self.crop\n",
    "        back_speed_diff = lane_back[0,2] - 5 if len(lane_back) > 0 else 0\n",
    "        back_adj_dist = back_dist - self.x_speed_coef*back_speed_diff    # The faster the car in the back is driving, the more dangerous it is, so the adjusted distance is lower\n",
    "\n",
    "        left_signs = [get_sign(left_lanes[i,0]+3) for i in range(len(left_lanes))]\n",
    "        left_adj_dists = (left_lanes[:,1]<-5.5)*5 + np.abs(left_lanes[:,0]) + self.x_speed_coef*left_lanes[:,2]*left_signs - (self.y_speed_coef*left_lanes[:,3])*((1+np.array(left_signs))/2)\n",
    "        left_adj_dist = np.min(left_adj_dists) if len(left_adj_dists) > 0 else self.crop\n",
    "\n",
    "        right_signs = [get_sign(right_lanes[i,0]+3) for i in range(len(right_lanes))]\n",
    "        right_adj_dists = (right_lanes[:,1]>5.5)*5 + np.abs(right_lanes[:,0]) + self.x_speed_coef*right_lanes[:,2]*right_signs + (self.y_speed_coef*right_lanes[:,3])*((1+np.array(right_signs))/2)\n",
    "        right_adj_dist = np.min(right_adj_dists) if len(right_adj_dists) > 0 else self.crop\n",
    "\n",
    "        turn_possibility = -1 if lane < 2.5 else 1 if lane > 35.5 else 0\n",
    "        values = np.array([front_adj_dist, back_adj_dist, left_adj_dist, right_adj_dist])\n",
    "\n",
    "        if debug:\n",
    "            print('--------------------------------------------')\n",
    "            print('Position:', self.current_obs[0])\n",
    "            print('Front:', front_dist, front_speed_diff)\n",
    "            print('Back:', back_dist, back_speed_diff)\n",
    "            print('Left:', left_lanes[:,0], left_lanes[:,1], left_lanes[:,2], left_lanes[:,3], left_adj_dists)\n",
    "            print('Right:', right_lanes[:,0], right_lanes[:,1], right_lanes[:,2], right_lanes[:,3], right_adj_dists)\n",
    "            print('Adjusted:', values)\n",
    "\n",
    "        # Use the danger threshold to make 0 or 1 \n",
    "        if not return_values: \n",
    "            values_x = np.where(values[:2] < self.danger_threshold_x, 1, 0)\n",
    "            values_y = np.where(values[2:] < self.danger_threshold_y, 1, 0)\n",
    "            values = np.append(values_x, values_y)\n",
    "        \n",
    "        values = np.append(values, turn_possibility)\n",
    "\n",
    "        if self.special_Q:\n",
    "            past_actions = np.array(self.past_actions)\n",
    "            # We dont want actions that cancel themselves out, so we need to assing opposite values for turning left and right and then to check if the sum of the actions is different from 0\n",
    "            mask = np.where(past_actions == 0, -1, 0) + np.where(past_actions == 2, 1, 0)\n",
    "            turn_states = [1] if np.sum(mask) != 0 else [0]\n",
    "            values = np.append(values, turn_states)\n",
    "        return tuple(values)\n",
    "\n",
    "    def get_n_closest(self):\n",
    "        \"\"\"\n",
    "        Get the n closest cars to the agent\n",
    "        Returns:\n",
    "            closest_car_positions: np.array, the positions of the n closest cars to the agent. If there are less than n_closest cars, the array is padded with the crop_dist values\n",
    "        \"\"\"\n",
    "\n",
    "        car_positions = self.get_car_positions()\n",
    "        distances = np.linalg.norm(car_positions, axis=1)\n",
    "\n",
    "        # Remove the agent position\n",
    "        closest = np.argsort(distances)[1:self.n_closest+1]\n",
    "        closest_car_positions = car_positions[closest]\n",
    "\n",
    "        # If there are less than n_closest cars, pad the array with the crop_dist values\n",
    "        if len(closest_car_positions) < self.n_closest:\n",
    "            n_missing = self.n_closest - len(closest_car_positions)\n",
    "            closest_car_positions = np.pad(closest_car_positions, ((0, n_missing), (0,0)), 'constant', constant_values=(self.crop_dist[0][0], self.crop_dist[1][0]))\n",
    "\n",
    "        # Values that are \n",
    "        return closest_car_positions\n",
    "    \n",
    "\n",
    "    def state_n_neighbours(self):\n",
    "        \"\"\"\n",
    "        Get the state of the environment in a neighbour-wise manner\n",
    "        Returns:\n",
    "            state: tuple, the state of the environment in the form (x1,x2,...,xn), (y1,y2,...,yn)\n",
    "        \"\"\"\n",
    "        n_closest = self.get_n_closest()\n",
    "        # For the closest cars, get the state\n",
    "        state_x, state_y = [], []\n",
    "        # Get the bin values for each of the x,y positions, and return a tuple with the values\n",
    "        for car in n_closest:\n",
    "            x = np.digitize(car[0], self.x_bins) - 1\n",
    "            y = np.digitize(car[1], self.y_bins) - 1\n",
    "            x_val, y_val = self.x_bins[x], self.y_bins[y]\n",
    "            state_x.append(x_val)\n",
    "            state_y.append(y_val)\n",
    "        state = (tuple(state_x), tuple(state_y))\n",
    "        return tuple(state)\n",
    "    \n",
    "    @staticmethod\n",
    "    def bin_values(values, bins, digitize=False):\n",
    "        \"\"\"\n",
    "        Function to bin the values\n",
    "        Arguments:\n",
    "            values: np.array, the values to bin\n",
    "            bins: list, the bins to use\n",
    "                Example: bins = [[5,10,15,30], [5,10,30], [8,14,20], [8,14,20]], for x,y,vx,vy\n",
    "            digitize: bool, whether to digitize the values. If set to False, the values will be returned as the bin index\n",
    "        Returns:\n",
    "            binned_values: np.array, the binned values\n",
    "        \"\"\"\n",
    "        # Check if there are as much bins as values\n",
    "        if len(values.shape) == 1:\n",
    "            assert len(bins) == len(values), \"The number of bins must be equal to the number of values\"\n",
    "            binned_values = []\n",
    "            if digitize:\n",
    "                binned_values = [np.digitize(values[i], bins[i]) for i in range(len(bins))]\n",
    "                return np.array(binned_values)\n",
    "\n",
    "            binned_values = [bins[i][np.digitize(values[i], bins[i])-1] for i in range(len(bins))]\n",
    "            return np.array(binned_values)\n",
    "\n",
    "        # For a NxM matrix, with N>1\n",
    "        assert len(bins) == values.shape[1], \"The number of bins must be equal to the number of values\"\n",
    "        binned_values = []\n",
    "        for i in range(len(values)):\n",
    "            if digitize:\n",
    "                binned_values.append([np.digitize(values[i,j], bins[j]) for j in range(len(bins))])\n",
    "            else:\n",
    "                binned_values.append([bins[j][np.digitize(values[i,j], bins[j])-1] for j in range(len(bins))])\n",
    "        return np.array(binned_values)\n",
    "\n",
    "    \n",
    "    def test_env(self, sleep_time=0.1, custom_probs=None, show_values=False, manual=False, debug=False):\n",
    "        \"\"\"\n",
    "        Function to test the environment with a random policy, or with a policy\n",
    "        \"\"\" \n",
    "        def sample_action(probs=[0.2,0.2,0.2,0.2,0.2]):\n",
    "            return np.random.choice([0,1,2,3,4], p=probs)\n",
    "\n",
    "        print('You have chosen the manual control of the car') if manual else None \n",
    "\n",
    "        obs, info = self.env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        step, action, done = 0, None, False\n",
    "        while not done:\n",
    "            # start = time.time()\n",
    "            if manual: \n",
    "                while action is None:\n",
    "                    try:\n",
    "                        action = int(input(\">\"))\n",
    "                    except:\n",
    "                        continue\n",
    "            elif custom_probs is not None:\n",
    "                action = sample_action(custom_probs)\n",
    "            elif self.policy is None:\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                action = self.policy()\n",
    "            obs, reward, done, truncate, info = self.env.step(action)\n",
    "            print(obs[0]) if debug else None\n",
    "            speed = obs[0,2]\n",
    "            reward = fix_reward(reward, obs[0], action, self.to_right_reward, self.to_right_skewness, self.change_lane_reward)\n",
    "            print('\\n', step, self.get_state(), speed, decode_meta_action(action), reward)\n",
    "            print(self.get_state(return_values=True, debug=debug)) if show_values else None\n",
    "            time.sleep(sleep_time)\n",
    "            # end = time.time()\n",
    "            # print(f\"Time taken: {end-start}\")\n",
    "            self.current_obs = obs\n",
    "            self.past_actions.append(action)\n",
    "            step += 1\n",
    "            action = None\n",
    "        self.env.close()\n",
    "\n",
    "    def test_danger_threshold(self, iterations=1000):\n",
    "        def sample_stress(): \n",
    "            probs = [0.2,0.2,0.2,0.2,0.2]\n",
    "            return np.random.choice([0,1,2,3,4], p=probs)\n",
    "\n",
    "        data_x, data_y = [], []\n",
    "        sleep_time = 0\n",
    "        self.env = gym.make('highway-v0', render_mode=None, config=self.config)\n",
    "        obs, info = self.env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        for i in tqdm(range(iterations)):\n",
    "            done = False\n",
    "            self.env.reset(seed = np.random.randint(10000))\n",
    "            step = 0\n",
    "            while not done:\n",
    "                obs, reward, done, truncate, info = self.env.step(sample_stress())\n",
    "                state = self.get_state(return_values=True)       # Last state before crash\n",
    "                print(state)\n",
    "                if done:\n",
    "                    min_index = np.argmin(state)                           # Closest adj distance, meaning the car against which we crashed \n",
    "                    data_x.append(state[min_index]) if min_index < 2 else data_y.append(state[min_index])\n",
    "                    print(np.max(data_x), np.max(data_y)) if len(data_x) > 5 and len(data_y) > 5 else print(data_x, data_y)\n",
    "                self.current_obs = obs\n",
    "                step += 1\n",
    "        self.env.close()\n",
    "        return data_x, data_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algorithm(Kinematics):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha=0.75,\n",
    "        gamma=0.95,\n",
    "        epsilon=0.6,\n",
    "        epsilon_decay=1, min_epsilon=0.05,\n",
    "        print_stats=False,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Algorithm class constructor\n",
    "        Arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            epsilon_decay: float, the decay value for epsilon. If set to 1, the epsilon will not decay\n",
    "            min_epsilon: float, the minimum value for epsilon. If the epsilon is lower than this value, it will not decay\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \n",
    "        Other Arguments (for the Kinematics observation):\n",
    "            seed: int, the seed to use in the test environment\n",
    "            state_type: str, the type of state to use. Options are 'n_neighbours' or 'danger'\n",
    "            policy: function, the policy to use in the simulation\n",
    "            crop: int, the crop distance to use in the state\n",
    "            lane_tolerance: int, the tolerance to use in the lane\n",
    "            danger_threshold_x: float, the threshold to use in the x direction for the danger state\n",
    "            danger_threshold_y: float, the threshold to use in the y direction for the danger state\n",
    "            x_speed_coef: float, the coefficient to use in the x direction for the danger state\n",
    "            y_speed_coef: float, the coefficient to use in the y direction for the danger state\n",
    "            special_Q: Q function includes the past action\n",
    "\n",
    "        Other Arguments (for Observation Type):\n",
    "            sim_frequency: int, the frequency of the simulation\n",
    "            policy_frequency: int, the frequency of the policy\n",
    "            render_mode: str, the mode to render the simulation\n",
    "            seed: int, the seed to use in the simulation\n",
    "            colision_reward: float, the reward to give when a colision occurs\n",
    "            high_speed_reward: float, the reward to give when driving at high speed\n",
    "            reward_speed_range: list, the range of speeds to give the high speed reward\n",
    "            to_right_reward: float, the reward to give when driving to the right, mapped polynomially with to_right_skewness\n",
    "            to_right_skewness: float, the skewness to apply to the to_right_reward\n",
    "            change_lane_reward: float, the reward to give when changing lanes\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.initialize_Q(print_stats)\n",
    "        self.alpha, self.gamma, self.epsilon = alpha, gamma, epsilon\n",
    "        self.epsilon_decay, self.min_epsilon = epsilon_decay, min_epsilon\n",
    "        self.Q_stats = self.Q.copy()\n",
    "        self.rewards_hist = []\n",
    "\n",
    "    def policy_Q(self, state):\n",
    "        values = [self.Q[(state, action)] for action in range(5)]\n",
    "        return np.argmax(values)   \n",
    "    \n",
    "    def initialize_Q(self, print_stats = False):\n",
    "        # Combine the possible states with the possible actions\n",
    "        keys = list(itertools.product(self.states, range(5)))       # 5 possible past actions, 0-4: left, idle, right, accelerate, decelerate\n",
    "        if print_stats:\n",
    "            print(f\"Number of states: {self.state_size * 5}\")   \n",
    "        self.Q = {key: 0 for key in keys}\n",
    "\n",
    "    def epsilon_greedy(self, state, smart=True):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            frequencies = [self.Q_stats[(state,action)] for action in range(5)]\n",
    "            # Find the 3 most uncommon actions and choose one of them\n",
    "            return np.random.choice(np.argsort(frequencies)[:3]), 1\n",
    "        else:\n",
    "            values = [self.Q[(state, action)] for action in range(5)]\n",
    "            return np.argmax(values), 0\n",
    "\n",
    "    def decay_epsilon(self, episode):\n",
    "        if self.epsilon > self.min_epsilon:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def test(self, sleep_time=1, time_after_crash=10, max_steps=200):\n",
    "        with gym.make('highway-v0', render_mode='human', config=self.config) as env:\n",
    "            obs, info = env.reset(seed = self.seed)\n",
    "            self.current_obs = obs\n",
    "            done,steps = False,0\n",
    "            state = self.get_state()\n",
    "            action = self.policy_Q(state)\n",
    "            while not done and steps < max_steps:\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                reward = fix_reward(reward, next_obs[0], action, self.to_right_reward, self.to_right_skewness, self.change_lane_reward)\n",
    "                print(state, decode_meta_action(action), reward)\n",
    "\n",
    "                # Update state after env.step\n",
    "                self.current_obs = next_obs\n",
    "                state = self.get_state()\n",
    "                action = self.policy_Q(state)\n",
    "\n",
    "                # If we cant turn left or right, the car wont be turning, so dont append a turning signal\n",
    "                if (state[4] == -1 and action == 0) or (state[4] == 1 and action == 2):\n",
    "                    self.past_actions.append(1) \n",
    "                else:\n",
    "                    self.past_actions.append(action)\n",
    "                time.sleep(sleep_time)\n",
    "                steps+=1\n",
    "            time.sleep(time_after_crash)\n",
    "\n",
    "    def get_state_visits(self, state=None):\n",
    "        state_visits = {state: np.sum([self.Q_stats[(state, action)] for action in range(5)]) for state in self.states}\n",
    "        state_visits = {k:100*v/np.sum(list(state_visits.values())) for k, v in sorted(state_visits.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return state_visits if state is None else state_visits[state]\n",
    "\n",
    "    def plot_rewards_history(self, smoothing=10, relative=True):\n",
    "        if relative == False:\n",
    "            data = [reward[0] for reward in self.rewards_hist]\n",
    "        else: \n",
    "            plt.axhline(y=0, color='r', linestyle='--', label='Zero')\n",
    "            data = [reward[0]/reward[1] for reward in self.rewards_hist]\n",
    "\n",
    "        plt.plot(data, label='Original')\n",
    "        smoothing_window = np.ones(smoothing) / smoothing\n",
    "        padded_rewards = np.concatenate((np.zeros(smoothing - 1), data))\n",
    "        smoothed_rewards = np.convolve(padded_rewards, smoothing_window, mode='valid')\n",
    "        plt.plot(smoothed_rewards, label='Smoothed')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Cumulative reward')\n",
    "        plt.title('Cumulative reward per episode')\n",
    "        plt.show()\n",
    "\n",
    "    def search_Q(self, state, decode=True):\n",
    "        assert len(state) in {5,6}, \"The state must be a tuple of 5 or 6 values\"\n",
    "        Q_vals = {decode_meta_action(a) : self.Q[state, a] for a in range(5)} if decode else {a: self.Q[state, a] for a in range(5)}\n",
    "        # Order a \n",
    "        Q_vals = {k: v for k, v in sorted(Q_vals.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return Q_vals\n",
    "\n",
    "    def save(self, directory=\"saved_models\", prefix=\"model\", name=None):\n",
    "        \"\"\"Saves the entire model object to a pickle file.\"\"\"\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M\")\n",
    "        if name is None:\n",
    "            filename = f\"{prefix}_{self.algorithm_type}_{timestamp}.pkl\"\n",
    "        else: \n",
    "            filename = f\"{prefix}_{self.algorithm_type}_{name}.pkl\"\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(self, f) \n",
    "\n",
    "    @staticmethod\n",
    "    def load(filepath):\n",
    "        \"\"\"Loads the entire model object from a pickle file.\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            loaded_model = pickle.load(f)\n",
    "        return loaded_model \n",
    "\n",
    "    def q_measure(self): \n",
    "        score = 0\n",
    "        # Get the best actions for each state\n",
    "        q_actions = {state: np.argmax([self.Q[(state, action)] for action in range(5)]) for state in self.states}\n",
    "        for state in optimum_q_kinematics.keys():\n",
    "            if q_actions[state] == optimum_q_kinematics[state]:\n",
    "                score+=1\n",
    "        return 100*(score/len(optimum_q_kinematics))\n",
    "\n",
    "class Sarsa(Algorithm):\n",
    "    def __init__(\n",
    "        self,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        SARSA class constructor\n",
    "        Algorithm arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            m: int, the number of episodes to train the agent for\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.algorithm_type = 'Sarsa'\n",
    "        \n",
    "    def train(self, m=100, max_steps=200, verbose=0, render=False): \n",
    "        render_mode = 'human' if render else None\n",
    "        env = gym.make('highway-v0', render_mode=render_mode, config=self.config) \n",
    "        q_explored = np.count_nonzero(list(self.Q.values()))\n",
    "        for i in tqdm(range(m)):\n",
    "            # Reset the environment and get the initial state and action for that state\n",
    "            obs, info = env.reset(seed = np.random.randint(1000))\n",
    "            self.current_obs = obs\n",
    "            done, state, action = False, self.get_state(), self.epsilon_greedy(state)\n",
    "            cum_reward, steps, last_state, done = 0,0, None, False\n",
    "\n",
    "            while not done and steps < max_steps:\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "\n",
    "                # Update the state and action after the environment step\n",
    "                self.current_obs = next_obs\n",
    "                next_state = self.get_state()\n",
    "                next_action,_ = self.epsilon_greedy(next_state)\n",
    "                next_reward = fix_reward(reward, next_obs[0], action, self.to_right_reward, self.to_right_skewness, self.change_lane_reward)\n",
    "\n",
    "                print(steps, next_state, decode_meta_action(next_action), reward) if verbose > 2 else None\n",
    "                if self.Q[(state, action)] == 0:\n",
    "                    q_explored += 1\n",
    "                self.Q_stats[(state, action)] += 1\n",
    "                \n",
    "                # Update the Q value for the state and action pair \n",
    "                self.Q[(state, action)] += self.alpha*(next_reward + self.gamma*self.Q[(next_state, next_action)] - self.Q[(state, action)])\n",
    "\n",
    "                if done:\n",
    "                    last_state = state\n",
    "\n",
    "                state, action = next_state, next_action\n",
    "\n",
    "                # If we cant turn left or right, the car wont be turning, so dont append a turning signal\n",
    "                if (state[4] == -1 and action == 0) or (state[4] == 1 and action == 2):\n",
    "                    self.past_actions.append(1) \n",
    "                else:\n",
    "                    self.past_actions.append(action)\n",
    "                cum_reward += next_reward\n",
    "                steps += 1\n",
    "            self.rewards_hist.append((cum_reward, steps))\n",
    "            print(f\"Episode {i+1} completed on state {last_state} with cumulative reward: {cum_reward}\") if verbose > 0 else None\n",
    "            print(f\"Q explored: {100*q_explored/(self.state_size*5)}. Epsilon: {self.epsilon}\") if verbose > 1 else None\n",
    "            self.decay_epsilon(i)\n",
    "        env.close()\n",
    "    \n",
    "\n",
    "class Q_learning(Algorithm):\n",
    "    def __init__(\n",
    "        self,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Q-learning class constructor\n",
    "        Arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            m: int, the number of episodes to train the agent for\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.algorithm_type = 'Q_learning'\n",
    "\n",
    "    def train(self, m=100, max_steps=200, verbose=0, render=False, show_progress=25):\n",
    "        render_mode = 'human' if render else None\n",
    "        env = gym.make('highway-v0', render_mode=render_mode, config=self.config) \n",
    "        q_explored = np.count_nonzero(list(self.Q.values()))\n",
    "        for i in tqdm(range(m)):\n",
    "            # For each episode, reset the environment and get the initial state\n",
    "            obs, info = env.reset(seed = np.random.randint(1000))\n",
    "            self.current_obs = obs\n",
    "            state = self.get_state()\n",
    "            cum_reward,steps, last_state, done = 0,0, None, False\n",
    "\n",
    "            while not done and steps < max_steps:\n",
    "                # Get the action to make in the current state, and step the environment\n",
    "                action, _ = self.epsilon_greedy(state)\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                next_reward = fix_reward(reward, next_obs[0], action, self.to_right_reward, self.to_right_skewness, self.change_lane_reward)\n",
    "                self.current_obs = next_obs\n",
    "                next_state = self.get_state()\n",
    "\n",
    "                print(f'{steps}:', state, decode_meta_action(action), next_reward) if verbose > 2 else None\n",
    "                if done:\n",
    "                    last_state = state\n",
    "\n",
    "                if self.Q[(state, action)] == 0:\n",
    "                    q_explored += 1\n",
    "                cum_reward += reward\n",
    "                \n",
    "                self.Q[(state, action)] += self.alpha*(next_reward + self.gamma*np.max([self.Q[(next_state, a)] for a in range(5)]) - self.Q[(state,action)])\n",
    "                self.Q_stats[(state, action)] += 1\n",
    "                state = next_state\n",
    "\n",
    "                # If we cant turn left or right, the car wont be turning, so dont append a turning signal\n",
    "                if (state[4] == -1 and action == 0) or (state[4] == 1 and action == 2):\n",
    "                    self.past_actions.append(1) \n",
    "                else:\n",
    "                    self.past_actions.append(action)\n",
    "                steps += 1\n",
    "                print(f'Q value for {state, decode_meta_action(action)} updated from {self.Q[(state,action)]} to {self.Q[(state,action)]}') if verbose > 3 else None\n",
    "\n",
    "            self.rewards_hist.append((cum_reward, steps))\n",
    "            last_state = 'Terminal' if steps >= max_steps else last_state\n",
    "            print(f\"Episode {i+1} completed on state {last_state} with cumulative reward: {cum_reward}\") if verbose > 0 else None\n",
    "            print(f\"Q explored: {100*q_explored/(self.state_size*5)}. Epsilon: {self.epsilon}\") if verbose > 1 else None\n",
    "\n",
    "            # For measuring how accurate the Q function is \n",
    "            if self.special_Q: \n",
    "                q_qual = self.q_measure()\n",
    "                print(f\"Q measure: {q_qual} %\") if verbose > 1 else None\n",
    "            self.decay_epsilon(i)\n",
    "            if i % show_progress == 0:\n",
    "                self.test(sleep_time=0, time_after_crash=1, max_steps=max_steps)\n",
    "            if render: \n",
    "                env = gym.make('highway-v0', render_mode=render_mode, config=self.config) \n",
    "        env.close()\n",
    "\n",
    "\n",
    "class MonteCarlo(Algorithm):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"Constructor for the MonteCarlo algorithm class.\"\"\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.algorithm_type = 'Q_learning'\n",
    "        self.returns = {}  # Store returns for each (state, action) pair\n",
    "        self.visits = {}   # Store visit counts for each (state, action) pair\n",
    "\n",
    "    def train(self, m=100, max_steps=200, verbose=0):\n",
    "        \"\"\"Train the MonteCarlo agent.\"\"\"\n",
    "        env = gym.make('highway-v0', render_mode=None, config=self.config)\n",
    "\n",
    "        for i in tqdm(range(m)):\n",
    "            episode_history = []\n",
    "            cum_reward, steps = 0,0\n",
    "\n",
    "            obs, info = env.reset(seed=np.random.randint(1000))\n",
    "            self.current_obs = obs\n",
    "            done = False\n",
    "            state = self.get_state()\n",
    "\n",
    "            while not done and steps < max_steps:\n",
    "                action, random = self.epsilon_greedy(state) \n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                if done:\n",
    "                    self.hit_random.append(random)\n",
    "\n",
    "                reward = fix_reward(reward, next_obs[0], self.to_right_reward, self.to_right_skewness, self.change_lane_reward, self.current_obs[0])\n",
    "                next_state = self.get_state()\n",
    "\n",
    "                episode_history.append((state, action, reward))\n",
    "\n",
    "                cum_reward += reward\n",
    "                state = next_state\n",
    "                self.current_obs = next_obs\n",
    "                steps += 1\n",
    "\n",
    "            G = 0\n",
    "            for state, action, reward in reversed(episode_history):\n",
    "                G = self.gamma * G + reward \n",
    "                if (state, action) not in [(s, a) for s, a, _ in episode_history[:-1]]: # First visit\n",
    "                    self.returns[(state, action)] = self.returns.get((state, action), 0) + G\n",
    "                    self.visits[(state, action)] = self.visits.get((state, action), 0) + 1\n",
    "                    self.Q[(state, action)] = self.returns[(state, action)] / self.visits[(state, action)]\n",
    "\n",
    "            self.rewards_hist.append((cum_reward, steps))\n",
    "            print(f\"Episode {i+1} completed with cumulative reward: {cum_reward}. Epsilon: {self.epsilon}\") if verbose > 0 else None\n",
    "\n",
    "            self.decay_epsilon(i)\n",
    "            \n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO : IMPLEMENT SPECIAL Q!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kin = Kinematics(seed=10, \n",
    "                state_type='danger', \n",
    "                render_mode='human', \n",
    "                sim_frequency=20, \n",
    "                policy_frequency=5, \n",
    "                danger_threshold_x=12.5, \n",
    "                danger_threshold_y=12.5,   # Changed to 12.5 from 10, bc the driver takes time to notice other cars on the right \n",
    "                x_speed_coef=1,\n",
    "                y_speed_coef=1,\n",
    "                colision_reward=-50,\n",
    "                high_speed_reward=0,\n",
    "                reward_speed_range=[20, 30],\n",
    "                to_right_reward=5, \n",
    "                to_right_skewness=2, \n",
    "                change_lane_reward=-1,\n",
    "                lane_tolerance=2)\n",
    "\n",
    "kin.test_env(show_values=True, manual=True,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 160\n"
     ]
    }
   ],
   "source": [
    "# TODO learn the best danger thresholds !!\n",
    "\n",
    "Q = Q_learning(print_stats=True, \n",
    "        epsilon=1, \n",
    "        epsilon_decay=0.97,\n",
    "        min_epsilon=0.15,\n",
    "        alpha=0.1, \n",
    "        gamma=0.99, \n",
    "        state_type='danger', \n",
    "        policy_frequency=2,    # 2\n",
    "        sim_frequency=20,     # 10 \n",
    "        danger_threshold_x=15, \n",
    "        danger_threshold_y=12.5,   # Changed to 12.5 from 10, bc the driver takes time to notice other cars on the right \n",
    "        x_speed_coef=1.25,\n",
    "        y_speed_coef=0.5,\n",
    "        lane_tolerance=2,\n",
    "        colision_reward=-100,\n",
    "        high_speed_reward=15,\n",
    "        reward_speed_range=[20, 30],\n",
    "        to_right_reward=15, \n",
    "        to_right_skewness=6,         # 2 Skew more ? Increase to_right_reward and skew? \n",
    "        change_lane_reward=-1,     # -0.25 Changing lanes too often (0,0,0,0,1) best action is lane left. Maybe skew more ? \n",
    "        special_Q=True, \n",
    "        past_action_len=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_save = Q.Q.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.Q = Q_save.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eeae95a96794da09c7306ac02fdba88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 completed on state (1, 0, 0, 0, -1, 1) with cumulative reward: 39.514026335142134\n",
      "Q explored: 6.875. Epsilon: 1\n",
      "Q measure: 9.375 %\n",
      "(0, 0, 0, 0, 0, 0) FASTER 11.91213030588613\n",
      "(0, 0, 0, 0, 0, 0) FASTER 13.737805960384366\n",
      "(0, 0, 0, 1, 0, 0) LANE_LEFT 13.039147122601845\n",
      "(0, 0, 1, 0, 0, 1) LANE_LEFT 12.90844113777977\n",
      "(0, 0, 0, 0, 0, 1) LANE_LEFT 12.870874270121545\n",
      "(0, 0, 0, 0, 0, 1) LANE_LEFT 13.818505756755162\n",
      "(0, 0, 0, 0, -1, 1) FASTER 14.97126807967644\n",
      "(0, 0, 0, 1, -1, 1) LANE_LEFT 13.992130794744854\n",
      "(0, 0, 0, 1, -1, 1) LANE_LEFT 13.996980206249182\n",
      "(0, 0, 0, 1, -1, 0) LANE_LEFT 13.998750163777302\n",
      "(0, 0, 0, 1, -1, 0) LANE_LEFT 13.999477146580144\n",
      "(0, 0, 0, 1, -1, 0) LANE_LEFT 13.999781002787028\n",
      "(0, 0, 0, 0, -1, 0) LANE_LEFT 13.999908262138149\n",
      "(0, 0, 0, 0, -1, 0) LANE_LEFT 13.999961570675792\n",
      "(0, 0, 0, 0, -1, 0) LANE_LEFT 13.999983901806992\n",
      "(0, 0, 0, 0, -1, 0) LANE_LEFT 13.999993256404355\n",
      "(0, 0, 0, 0, -1, 0) LANE_LEFT 13.999997175081567\n",
      "(0, 0, 0, 0, -1, 0) LANE_LEFT 13.999998816630683\n",
      "(0, 0, 0, 0, -1, 0) LANE_LEFT 13.99999950428199\n",
      "(0, 0, 0, 0, -1, 0) LANE_LEFT 13.9999997923418\n",
      "(0, 0, 0, 1, -1, 0) LANE_LEFT 13.999999913011173\n",
      "(0, 0, 0, 1, -1, 0) LANE_LEFT 13.99999996356004\n",
      "(0, 0, 0, 1, -1, 0) LANE_LEFT 13.99999998473516\n",
      "(0, 0, 0, 1, -1, 0) LANE_LEFT 13.999999993605499\n",
      "(0, 0, 0, 1, -1, 0) LANE_LEFT 13.999999997321321\n",
      "(0, 0, 0, 1, -1, 0) LANE_LEFT 13.99999999887789\n",
      "(0, 0, 0, 0, -1, 0) LANE_LEFT 13.999999999529944\n",
      "(0, 0, 0, 0, -1, 0) LANE_LEFT 13.99999999980309\n",
      "(0, 0, 0, 0, -1, 0) LANE_LEFT 13.999999999917517\n",
      "(0, 0, 0, 0, -1, 0) LANE_LEFT 13.999999999965446\n",
      "(0, 0, 0, 0, -1, 0) LANE_LEFT 13.999999999985526\n",
      "(0, 0, 0, 0, -1, 0) LANE_LEFT 13.999999999993936\n",
      "(1, 0, 0, 1, -1, 0) LANE_LEFT 13.999999999997458\n",
      "(1, 0, 0, 1, -1, 0) LANE_LEFT 13.99999999999894\n",
      "(1, 0, 0, 1, -1, 0) LANE_LEFT 13.999999999999563\n",
      "(1, 0, 0, 1, -1, 0) LANE_LEFT 13.999999999999813\n",
      "(1, 0, 0, 0, -1, 0) LANE_LEFT -96.1798578125001\n",
      "Episode 2 completed on state (0, 0, 1, 0, 0, 0) with cumulative reward: -81.49683486872192\n",
      "Q explored: 8.75. Epsilon: 0.97\n",
      "Q measure: 9.375 %\n",
      "Episode 3 completed on state (0, 0, 1, 1, 0, 1) with cumulative reward: 277.08798318037924\n",
      "Q explored: 23.75. Epsilon: 0.9409\n",
      "Q measure: 18.75 %\n",
      "Episode 4 completed on state (1, 0, 0, 1, 0, 1) with cumulative reward: 43.88791187902622\n",
      "Q explored: 28.125. Epsilon: 0.912673\n",
      "Q measure: 15.625 %\n",
      "Episode 5 completed on state (1, 0, 1, 1, 0, 0) with cumulative reward: 267.68112230173085\n",
      "Q explored: 40.625. Epsilon: 0.8852928099999999\n",
      "Q measure: 9.375 %\n",
      "Episode 6 completed on state (0, 0, 0, 1, 0, 1) with cumulative reward: 238.76381416894884\n",
      "Q explored: 46.25. Epsilon: 0.8587340256999999\n",
      "Q measure: 15.625 %\n",
      "Episode 7 completed on state (0, 0, 1, 0, 1, 1) with cumulative reward: 39.29408680139174\n",
      "Q explored: 47.5. Epsilon: 0.8329720049289999\n",
      "Q measure: 15.625 %\n",
      "Episode 8 completed on state (0, 0, 0, 1, 0, 0) with cumulative reward: -67.33411483639844\n",
      "Q explored: 48.125. Epsilon: 0.8079828447811299\n",
      "Q measure: 15.625 %\n",
      "Episode 9 completed on state (1, 0, 0, 0, 0, 1) with cumulative reward: -21.92822677457596\n",
      "Q explored: 48.75. Epsilon: 0.783743359437696\n",
      "Q measure: 15.625 %\n",
      "Episode 10 completed on state (0, 0, 1, 0, 0, 1) with cumulative reward: 26.024675505419424\n",
      "Q explored: 50.0. Epsilon: 0.7602310586545651\n",
      "Q measure: 21.875 %\n",
      "Episode 11 completed on state (1, 0, 0, 0, 0, 0) with cumulative reward: -26.59314866748558\n",
      "Q explored: 51.25. Epsilon: 0.7374241268949281\n",
      "Q measure: 21.875 %\n",
      "Episode 12 completed on state (0, 0, 1, 0, 0, 0) with cumulative reward: 12.982660504570944\n",
      "Q explored: 53.75. Epsilon: 0.7153014030880802\n",
      "Q measure: 21.875 %\n",
      "Episode 13 completed on state (0, 0, 1, 0, 0, 1) with cumulative reward: -35.49136386688501\n",
      "Q explored: 53.75. Epsilon: 0.6938423609954378\n",
      "Q measure: 21.875 %\n",
      "Episode 14 completed on state (1, 0, 1, 0, 0, 0) with cumulative reward: -62.71824707200503\n",
      "Q explored: 53.75. Epsilon: 0.6730270901655747\n",
      "Q measure: 21.875 %\n",
      "Episode 15 completed on state (1, 0, 0, 0, 0, 0) with cumulative reward: 294.9427237908104\n",
      "Q explored: 55.625. Epsilon: 0.6528362774606075\n",
      "Q measure: 18.75 %\n",
      "Episode 16 completed on state (1, 0, 0, 1, -1, 0) with cumulative reward: 33.7730776624731\n",
      "Q explored: 56.25. Epsilon: 0.6332511891367892\n",
      "Q measure: 18.75 %\n",
      "Episode 17 completed on state (1, 0, 0, 0, 0, 0) with cumulative reward: 134.5644917260837\n",
      "Q explored: 56.25. Epsilon: 0.6142536534626856\n",
      "Q measure: 21.875 %\n",
      "Episode 18 completed on state (0, 0, 1, 0, 0, 0) with cumulative reward: 241.62713965360908\n",
      "Q explored: 56.25. Epsilon: 0.595826043858805\n",
      "Q measure: 28.125 %\n",
      "Episode 19 completed on state (0, 0, 1, 0, 0, 1) with cumulative reward: -51.88808586316068\n",
      "Q explored: 56.875. Epsilon: 0.5779512625430409\n",
      "Q measure: 28.125 %\n",
      "Episode 20 completed on state (0, 0, 1, 1, 0, 1) with cumulative reward: -80.43835735891957\n",
      "Q explored: 56.875. Epsilon: 0.5606127246667496\n",
      "Q measure: 28.125 %\n",
      "Episode 21 completed on state (0, 0, 1, 1, 0, 1) with cumulative reward: 5.96778386924592\n",
      "Q explored: 57.5. Epsilon: 0.5437943429267471\n",
      "Q measure: 25.0 %\n",
      "Episode 22 completed on state (1, 0, 0, 0, 0, 1) with cumulative reward: -36.55732595338974\n",
      "Q explored: 57.5. Epsilon: 0.5274805126389447\n",
      "Q measure: 25.0 %\n",
      "Episode 23 completed on state (1, 0, 1, 0, 0, 1) with cumulative reward: 527.5235231923488\n",
      "Q explored: 60.0. Epsilon: 0.5116560972597763\n",
      "Q measure: 28.125 %\n",
      "Episode 24 completed on state (1, 0, 0, 0, 0, 1) with cumulative reward: 18.09016700475874\n",
      "Q explored: 60.625. Epsilon: 0.496306414341983\n",
      "Q measure: 25.0 %\n",
      "Episode 25 completed on state (0, 0, 1, 1, 0, 0) with cumulative reward: -6.152884045839485\n",
      "Q explored: 61.25. Epsilon: 0.48141722191172354\n",
      "Q measure: 25.0 %\n",
      "Episode 26 completed on state (1, 0, 0, 1, 0, 1) with cumulative reward: 75.67925952942689\n",
      "Q explored: 62.5. Epsilon: 0.4669747052543718\n",
      "Q measure: 28.125 %\n",
      "Episode 27 completed on state (0, 0, 1, 1, 0, 0) with cumulative reward: -5.640808265066227\n",
      "Q explored: 62.5. Epsilon: 0.4529654640967406\n",
      "Q measure: 28.125 %\n",
      "Episode 28 completed on state (0, 0, 1, 1, 0, 1) with cumulative reward: 763.7017713356266\n",
      "Q explored: 65.0. Epsilon: 0.4393765001738384\n",
      "Q measure: 25.0 %\n",
      "Episode 29 completed on state (0, 0, 1, 1, 0, 1) with cumulative reward: -64.63398439757015\n",
      "Q explored: 65.0. Epsilon: 0.42619520516862325\n",
      "Q measure: 25.0 %\n",
      "Episode 30 completed on state (1, 0, 0, 0, 0, 1) with cumulative reward: 51.80135932556074\n",
      "Q explored: 65.0. Epsilon: 0.4134093490135645\n",
      "Q measure: 25.0 %\n",
      "Episode 31 completed on state (1, 0, 0, 1, 0, 0) with cumulative reward: 353.71041703150536\n",
      "Q explored: 65.625. Epsilon: 0.4010070685431576\n",
      "Q measure: 25.0 %\n",
      "Episode 32 completed on state (1, 0, 1, 0, 0, 1) with cumulative reward: 1345.0491999789351\n",
      "Q explored: 70.0. Epsilon: 0.38897685648686287\n",
      "Q measure: 28.125 %\n",
      "Episode 33 completed on state (0, 0, 1, 0, 1, 1) with cumulative reward: 166.5526209578203\n",
      "Q explored: 70.0. Epsilon: 0.377307550792257\n",
      "Q measure: 28.125 %\n",
      "Episode 34 completed on state (0, 0, 0, 1, 0, 0) with cumulative reward: 143.12160996621958\n",
      "Q explored: 70.0. Epsilon: 0.36598832426848926\n",
      "Q measure: 28.125 %\n",
      "Episode 35 completed on state (0, 0, 1, 0, 0, 0) with cumulative reward: 82.21746655222944\n",
      "Q explored: 70.0. Epsilon: 0.35500867454043455\n",
      "Q measure: 28.125 %\n",
      "Episode 36 completed on state (1, 0, 1, 0, 0, 1) with cumulative reward: 205.6714238278123\n",
      "Q explored: 70.625. Epsilon: 0.3443584143042215\n",
      "Q measure: 31.25 %\n",
      "Episode 37 completed on state (1, 0, 0, 1, 0, 1) with cumulative reward: 459.9762895898408\n",
      "Q explored: 71.25. Epsilon: 0.3340276618750948\n",
      "Q measure: 31.25 %\n",
      "Episode 38 completed on state (1, 0, 0, 1, 0, 1) with cumulative reward: 148.23740624636366\n",
      "Q explored: 71.25. Epsilon: 0.32400683201884195\n",
      "Q measure: 31.25 %\n",
      "Episode 39 completed on state (1, 0, 1, 0, 0, 0) with cumulative reward: 465.4012384088138\n",
      "Q explored: 71.25. Epsilon: 0.3142866270582767\n",
      "Q measure: 31.25 %\n",
      "Episode 40 completed on state (1, 0, 1, 1, 0, 1) with cumulative reward: 1492.826146685573\n",
      "Q explored: 72.5. Epsilon: 0.3048580282465284\n",
      "Q measure: 31.25 %\n",
      "Episode 41 completed on state (0, 0, 1, 1, 0, 0) with cumulative reward: 3.169489947483612\n",
      "Q explored: 72.5. Epsilon: 0.2957122873991326\n",
      "Q measure: 31.25 %\n",
      "Episode 42 completed on state (1, 0, 0, 0, 0, 0) with cumulative reward: 24.67931035552894\n",
      "Q explored: 72.5. Epsilon: 0.2868409187771586\n",
      "Q measure: 31.25 %\n",
      "Episode 43 completed on state (1, 0, 0, 1, -1, 1) with cumulative reward: 88.39030313563286\n",
      "Q explored: 75.0. Epsilon: 0.27823569121384384\n",
      "Q measure: 31.25 %\n",
      "Episode 44 completed on state (0, 0, 0, 1, -1, 0) with cumulative reward: 322.86916720547214\n",
      "Q explored: 76.25. Epsilon: 0.2698886204774285\n",
      "Q measure: 31.25 %\n",
      "Episode 45 completed on state (1, 0, 1, 0, 0, 1) with cumulative reward: 1351.9239891435682\n",
      "Q explored: 76.875. Epsilon: 0.26179196186310566\n",
      "Q measure: 34.375 %\n",
      "Episode 46 completed on state (0, 0, 1, 1, 0, 1) with cumulative reward: 851.2003824360766\n",
      "Q explored: 76.875. Epsilon: 0.2539382030072125\n",
      "Q measure: 34.375 %\n",
      "Episode 47 completed on state (0, 0, 1, 1, 0, 1) with cumulative reward: -65.64044397288069\n",
      "Q explored: 76.875. Epsilon: 0.2463200569169961\n",
      "Q measure: 34.375 %\n",
      "Episode 48 completed on state (1, 0, 0, 0, -1, 0) with cumulative reward: 35.14062379477615\n",
      "Q explored: 77.5. Epsilon: 0.23893045520948622\n",
      "Q measure: 34.375 %\n",
      "Episode 49 completed on state (1, 0, 0, 0, -1, 0) with cumulative reward: 246.08729620740144\n",
      "Q explored: 78.125. Epsilon: 0.23176254155320164\n",
      "Q measure: 37.5 %\n",
      "Episode 50 completed on state (1, 0, 0, 1, -1, 0) with cumulative reward: 18.817854029681257\n",
      "Q explored: 79.375. Epsilon: 0.2248096653066056\n",
      "Q measure: 37.5 %\n",
      "Episode 51 completed on state Terminal with cumulative reward: 2532.132310760127\n",
      "Q explored: 79.375. Epsilon: 0.21806537534740741\n",
      "Q measure: 37.5 %\n",
      "(0, 0, 0, 0, 0, 1) FASTER 15.25665886126669\n",
      "(0, 0, 1, 0, 0, 1) FASTER 17.082334515764927\n",
      "(0, 0, 1, 1, 0, 0) FASTER 17.84711714545049\n",
      "(0, 0, 1, 1, 0, 0) FASTER 18.167487562412383\n",
      "(0, 0, 0, 1, 0, 0) FASTER 18.30169197564092\n",
      "(0, 0, 1, 1, 0, 0) FASTER 18.35791072611363\n",
      "(0, 0, 1, 0, 0, 0) LANE_RIGHT 18.767012198397666\n",
      "(0, 0, 0, 0, 0, 1) FASTER 21.610068795993623\n",
      "(0, 0, 0, 0, 0, 1) FASTER 22.256976771819414\n",
      "(0, 0, 0, 0, 0, 1) FASTER 22.432281898525147\n",
      "(0, 0, 0, 0, 0, 0) IDLE 22.4753691326957\n",
      "(0, 0, 0, 1, 0, 0) FASTER 22.485247647620085\n",
      "(0, 0, 0, 1, 0, 0) FASTER 22.487384161646816\n",
      "(0, 0, 0, 1, 0, 0) FASTER 22.487823839053622\n",
      "(0, 0, 1, 1, 0, 0) FASTER 22.48791497031511\n",
      "(0, 0, 1, 1, 0, 0) FASTER 22.487934909381067\n",
      "(0, 0, 1, 0, 0, 0) LANE_RIGHT 24.505809244809853\n",
      "(0, 0, 1, 0, 0, 1) FASTER 28.582345280634605\n",
      "(0, 0, 1, 0, 0, 1) FASTER 29.69388235188798\n",
      "(0, 0, 1, 0, 1, 1) IDLE 30.00222954783514\n",
      "(1, 0, 1, 0, 1, 0) LANE_LEFT 24.104371040654506\n",
      "(0, 0, 0, 1, 0, 1) FASTER 23.28739049726319\n",
      "(0, 0, 0, 1, 0, 1) FASTER 22.707665601839718\n",
      "(0, 0, 0, 1, 0, 1) FASTER 22.54170966159876\n",
      "(1, 0, 0, 1, 0, 0) FASTER 22.499953005711024\n",
      "(1, 0, 0, 1, 0, 0) FASTER 22.490417509263846\n",
      "(1, 0, 0, 0, 0, 0) LANE_LEFT 18.569044896801696\n",
      "(0, 0, 0, 1, 0, 1) FASTER 18.776563108252777\n",
      "(0, 0, 0, 1, 0, 1) FASTER 18.508269971053252\n",
      "(0, 0, 0, 1, 0, 1) FASTER 18.425840958108573\n",
      "(0, 0, 0, 1, 0, 0) FASTER 18.40459205493228\n",
      "(0, 0, 0, 0, 0, 0) IDLE 18.399707327095555\n",
      "(1, 0, 0, 0, 0, 0) LANE_LEFT 15.714696860356305\n",
      "(0, 0, 0, 1, 0, 1) FASTER 16.520589779342902\n",
      "(0, 0, 1, 1, 0, 1) IDLE 16.430837034627597\n",
      "(0, 0, 1, 1, 0, 1) IDLE 16.39597566619743\n",
      "(0, 0, 1, 1, 0, 0) FASTER 16.386372709303785\n",
      "(0, 0, 1, 1, 0, 0) FASTER 16.38412570351408\n",
      "(0, 0, 1, 1, 0, 0) FASTER 16.383649563939297\n",
      "(0, 0, 1, 0, 0, 0) LANE_RIGHT 15.825663580039578\n",
      "(0, 0, 0, 0, 0, 1) FASTER 17.91839326963428\n",
      "(0, 0, 0, 0, 0, 1) FASTER 18.27835414456698\n",
      "(0, 0, 0, 1, 0, 1) FASTER 18.370300800883147\n",
      "(0, 0, 1, 1, 0, 0) FASTER 18.392241420449643\n",
      "(0, 0, 1, 1, 0, 0) FASTER 18.39716662581435\n",
      "(0, 0, 1, 1, 0, 0) FASTER 18.3981991073046\n",
      "(0, 0, 1, 1, 0, 0) FASTER 18.398397302509157\n",
      "(0, 0, 1, 1, 0, 0) FASTER 18.398432591510222\n",
      "(0, 0, 0, 1, 0, 0) FASTER 18.39843802040035\n",
      "(0, 0, 0, 0, 0, 0) IDLE 18.3984380204011\n",
      "(0, 0, 0, 0, 0, 0) IDLE 18.3984380204011\n",
      "(0, 0, 0, 0, 0, 0) IDLE 18.3984380204011\n",
      "(1, 0, 0, 0, 0, 0) LANE_LEFT 15.714833079862288\n",
      "(0, 0, 0, 1, 0, 1) FASTER 16.5205912357609\n",
      "(1, 0, 0, 1, 0, 1) FASTER 16.43083554932879\n",
      "(1, 0, 0, 1, 0, 1) FASTER 16.395975702806954\n",
      "(1, 0, 0, 1, 0, 0) FASTER 16.386372709692036\n",
      "(1, 0, 0, 1, 0, 0) FASTER -88.00340466575234\n",
      "Episode 52 completed on state (1, 0, 0, 1, 0, 0) with cumulative reward: 472.47143555587854\n",
      "Q explored: 80.625. Epsilon: 0.2115234140869852\n",
      "Q measure: 37.5 %\n",
      "Episode 53 completed on state (0, 0, 0, 1, 0, 1) with cumulative reward: 583.7968713220035\n",
      "Q explored: 80.625. Epsilon: 0.20517771166437562\n",
      "Q measure: 37.5 %\n",
      "Episode 54 completed on state (1, 0, 0, 1, 0, 0) with cumulative reward: 186.90999670822242\n",
      "Q explored: 81.25. Epsilon: 0.19902238031444436\n",
      "Q measure: 37.5 %\n",
      "Episode 55 completed on state (0, 0, 0, 1, 0, 0) with cumulative reward: -88.00739478055834\n",
      "Q explored: 81.25. Epsilon: 0.19305170890501103\n",
      "Q measure: 37.5 %\n",
      "Episode 56 completed on state (1, 0, 1, 1, 0, 1) with cumulative reward: 586.2867098406082\n",
      "Q explored: 81.875. Epsilon: 0.1872601576378607\n",
      "Q measure: 37.5 %\n",
      "Episode 57 completed on state (0, 0, 1, 0, 0, 1) with cumulative reward: 1142.0049035572933\n",
      "Q explored: 82.5. Epsilon: 0.18164235290872485\n",
      "Q measure: 37.5 %\n",
      "Episode 58 completed on state (1, 0, 0, 1, 0, 1) with cumulative reward: 745.1671736744748\n",
      "Q explored: 82.5. Epsilon: 0.1761930823214631\n",
      "Q measure: 37.5 %\n",
      "Episode 59 completed on state (1, 0, 1, 1, 0, 0) with cumulative reward: 83.64554734460154\n",
      "Q explored: 82.5. Epsilon: 0.17090728985181922\n",
      "Q measure: 37.5 %\n",
      "Episode 60 completed on state (1, 0, 0, 1, 0, 0) with cumulative reward: 441.7406649046694\n",
      "Q explored: 82.5. Epsilon: 0.16578007115626464\n",
      "Q measure: 37.5 %\n",
      "Episode 61 completed on state (1, 0, 1, 0, 1, 0) with cumulative reward: 179.1447805956892\n",
      "Q explored: 84.375. Epsilon: 0.1608066690215767\n",
      "Q measure: 37.5 %\n",
      "Episode 62 completed on state (1, 0, 0, 1, 0, 0) with cumulative reward: 543.7074505714179\n",
      "Q explored: 85.0. Epsilon: 0.1559824689509294\n",
      "Q measure: 40.625 %\n",
      "Episode 63 completed on state (0, 0, 1, 0, 0, 1) with cumulative reward: 271.5428105103306\n",
      "Q explored: 85.625. Epsilon: 0.1513029948824015\n",
      "Q measure: 40.625 %\n",
      "Episode 64 completed on state (1, 0, 0, 0, 0, 0) with cumulative reward: 328.72483669596465\n",
      "Q explored: 85.625. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 65 completed on state (1, 0, 0, 1, -1, 0) with cumulative reward: 421.21877271586965\n",
      "Q explored: 86.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 66 completed on state (0, 0, 0, 1, 0, 0) with cumulative reward: 750.8720060878812\n",
      "Q explored: 86.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 67 completed on state (1, 0, 0, 1, 0, 1) with cumulative reward: 333.16747762380817\n",
      "Q explored: 86.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 68 completed on state Terminal with cumulative reward: 2859.7574154941\n",
      "Q explored: 86.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 69 completed on state (1, 0, 0, 1, 0, 0) with cumulative reward: 283.49530228545746\n",
      "Q explored: 86.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 70 completed on state Terminal with cumulative reward: 2879.8375409783866\n",
      "Q explored: 86.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 71 completed on state (0, 0, 0, 1, 0, 0) with cumulative reward: 1294.5043775099875\n",
      "Q explored: 86.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 72 completed on state (1, 0, 1, 0, 0, 1) with cumulative reward: -4.200703394396967\n",
      "Q explored: 86.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 73 completed on state (1, 0, 1, 1, 0, 1) with cumulative reward: 317.9446067165667\n",
      "Q explored: 86.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 74 completed on state (1, 0, 0, 1, 0, 0) with cumulative reward: 1440.3760577809696\n",
      "Q explored: 86.875. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 75 completed on state (0, 0, 1, 1, 0, 0) with cumulative reward: 214.52620276125077\n",
      "Q explored: 86.875. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 76 completed on state (0, 0, 1, 1, -1, 1) with cumulative reward: 69.23344161711323\n",
      "Q explored: 87.5. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 77 completed on state (1, 0, 1, 1, 0, 0) with cumulative reward: 246.62048702798683\n",
      "Q explored: 87.5. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 78 completed on state (1, 0, 0, 0, -1, 0) with cumulative reward: 160.68922333152244\n",
      "Q explored: 88.75. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 79 completed on state (1, 0, 1, 0, 1, 1) with cumulative reward: 351.6360133673774\n",
      "Q explored: 88.75. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 80 completed on state (1, 0, 1, 1, 0, 0) with cumulative reward: 259.18480747769735\n",
      "Q explored: 88.75. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 81 completed on state (0, 0, 1, 0, 0, 1) with cumulative reward: -52.60360240081343\n",
      "Q explored: 88.75. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 82 completed on state (1, 0, 0, 0, 0, 1) with cumulative reward: -88.00844582580112\n",
      "Q explored: 88.75. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 83 completed on state (1, 0, 0, 1, 0, 0) with cumulative reward: 867.8226165902006\n",
      "Q explored: 88.75. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 84 completed on state (0, 0, 1, 0, 1, 0) with cumulative reward: 782.2094928919512\n",
      "Q explored: 88.75. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 85 completed on state (0, 0, 0, 1, 0, 0) with cumulative reward: 104.63995181592887\n",
      "Q explored: 88.75. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 86 completed on state (1, 0, 1, 0, 0, 1) with cumulative reward: 1831.895705038778\n",
      "Q explored: 88.75. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 87 completed on state (0, 0, 0, 1, 0, 1) with cumulative reward: 276.3877946293756\n",
      "Q explored: 88.75. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 88 completed on state (0, 0, 0, 1, 0, 0) with cumulative reward: 532.1153762438811\n",
      "Q explored: 88.75. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 89 completed on state (1, 0, 0, 1, 0, 0) with cumulative reward: 1634.8712896275292\n",
      "Q explored: 88.75. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 90 completed on state (1, 0, 1, 0, 0, 0) with cumulative reward: 1038.2934746581166\n",
      "Q explored: 88.75. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 91 completed on state Terminal with cumulative reward: 1542.0807524929544\n",
      "Q explored: 88.75. Epsilon: 0.14676390503592945\n",
      "Q measure: 40.625 %\n",
      "Episode 92 completed on state (1, 0, 0, 1, -1, 1) with cumulative reward: 524.4874742948241\n",
      "Q explored: 90.0. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 93 completed on state (1, 0, 0, 0, 0, 1) with cumulative reward: 478.53098490532784\n",
      "Q explored: 90.0. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 94 completed on state (0, 0, 0, 1, -1, 0) with cumulative reward: 1100.5229761839666\n",
      "Q explored: 90.625. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 95 completed on state (1, 0, 1, 1, 0, 0) with cumulative reward: 487.39148979829054\n",
      "Q explored: 90.625. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 96 completed on state (1, 0, 0, 1, 0, 0) with cumulative reward: 608.5379746795462\n",
      "Q explored: 90.625. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 97 completed on state (1, 0, 0, 1, -1, 0) with cumulative reward: 187.5641355093639\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 98 completed on state (0, 0, 0, 1, 0, 1) with cumulative reward: 567.500175323412\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 99 completed on state (1, 0, 1, 0, 0, 0) with cumulative reward: 248.24418891827372\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 100 completed on state (1, 0, 1, 0, 0, 0) with cumulative reward: 612.7279759282659\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 101 completed on state Terminal with cumulative reward: 2662.387191074348\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "(1, 0, 0, 0, 0, 1) LANE_LEFT 6.084418365809767\n",
      "(0, 0, 0, 1, 0, 1) FASTER 11.834631869273304\n",
      "(0, 0, 0, 1, 0, 1) FASTER 13.732559360293878\n",
      "(0, 0, 1, 1, 0, 1) IDLE 14.50253601670545\n",
      "(0, 0, 1, 1, 0, 0) FASTER 14.823014023106053\n",
      "(0, 0, 1, 1, 0, 0) FASTER 14.957177099805108\n",
      "(0, 0, 1, 1, 0, 0) FASTER 15.013384651584872\n",
      "(0, 0, 1, 0, 0, 0) LANE_RIGHT 13.650273467944233\n",
      "(0, 0, 1, 0, 0, 1) FASTER 15.055170103935943\n",
      "(0, 0, 0, 0, 0, 1) FASTER 15.143465753491354\n",
      "(0, 0, 0, 0, 0, 1) FASTER 15.156662399195872\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.159130338357842\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.159757109885529\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.159948261083123\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.160013773469755\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.1600382148815\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.160047858941503\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.160051818964895\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.16005346051401\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.160054148165317\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.160054436225128\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.1600545568945\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.160054607443367\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.160054628618488\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.160054637488827\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.160054641204649\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.160054642761217\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.160054643413272\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.160054643686419\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.160054643800844\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.160054643848774\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.160054643868854\n",
      "(0, 0, 1, 0, 0, 0) LANE_RIGHT 13.866755179542947\n",
      "(0, 0, 0, 0, 0, 1) FASTER 15.348659311331106\n",
      "(0, 0, 1, 1, 0, 1) IDLE 15.468841612224375\n",
      "(0, 0, 1, 1, 0, 1) IDLE 15.490971318737751\n",
      "(0, 0, 1, 1, 0, 0) FASTER 15.495400440941198\n",
      "(0, 0, 1, 1, 0, 0) FASTER 15.496336071572452\n",
      "(0, 0, 1, 1, 0, 0) FASTER 15.49652900414867\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.49656589497844\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.496572457778555\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.496573467232638\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.496573467233395\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.496573467233395\n",
      "(1, 0, 0, 1, 0, 0) LANE_LEFT 13.839797376551036\n",
      "(0, 0, 0, 1, 0, 1) FASTER 15.114722826459372\n",
      "(0, 0, 0, 1, 0, 1) FASTER 15.160450178350947\n",
      "(0, 0, 0, 1, 0, 1) FASTER 15.161293044782543\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.160405330451054\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.160130817669904\n",
      "(1, 0, 0, 1, 0, 0) LANE_LEFT 13.651502204735017\n",
      "(0, 0, 0, 1, 0, 1) FASTER 14.98689623396075\n",
      "(0, 0, 0, 1, 0, 1) FASTER 15.048840459686476\n",
      "(0, 0, 1, 1, 0, 1) IDLE 15.053850334172044\n",
      "(0, 0, 1, 1, 0, 0) FASTER 15.053972843143109\n",
      "(0, 0, 1, 1, 0, 0) FASTER 15.053926582698274\n",
      "(0, 0, 1, 0, 0, 0) LANE_RIGHT 13.667722172557918\n",
      "(0, 0, 1, 0, 0, 1) FASTER 15.062296717645644\n",
      "(0, 0, 1, 0, 0, 1) FASTER 15.146440542283141\n",
      "(0, 0, 1, 1, 0, 1) IDLE 15.157909179827305\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.15965284328999\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.159976020323146\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.160039998796107\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.160052202789199\n",
      "(0, 0, 0, 0, 0, 0) IDLE 15.160054313074337\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.160054602537116\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.160054643883296\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.160054643883296\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.160054643883296\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.160054643883296\n",
      "(0, 0, 0, 1, 0, 0) FASTER 15.160054643883296\n",
      "(1, 0, 0, 0, 0, 0) LANE_LEFT 13.65179384399\n",
      "(1, 0, 1, 0, 0, 1) FASTER 14.986918276078688\n",
      "(1, 0, 1, 1, 0, 1) FASTER 15.048841488081013\n",
      "(1, 0, 1, 1, 0, 1) FASTER 15.053850360821977\n",
      "(1, 0, 1, 1, 0, 0) SLOWER -91.06500291322766\n",
      "Episode 102 completed on state (1, 0, 0, 1, 0, 1) with cumulative reward: 96.22149002648302\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 103 completed on state (1, 0, 0, 0, 0, 1) with cumulative reward: 103.1143471087975\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 104 completed on state Terminal with cumulative reward: 2779.7085653901577\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 105 completed on state (1, 0, 1, 0, 0, 1) with cumulative reward: 604.7345533699926\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 106 completed on state (1, 0, 0, 0, -1, 0) with cumulative reward: 327.6844260175544\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 107 completed on state (1, 0, 1, 1, 0, 1) with cumulative reward: 150.68539300644716\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 108 completed on state Terminal with cumulative reward: 2104.618127205964\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 109 completed on state (1, 0, 0, 0, 0, 1) with cumulative reward: 12.467304659814559\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 110 completed on state (1, 0, 1, 1, 0, 0) with cumulative reward: 1002.8241564541836\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 111 completed on state (1, 0, 1, 0, 0, 1) with cumulative reward: -52.030253296726684\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 112 completed on state (1, 0, 0, 1, 0, 0) with cumulative reward: 392.6620326227567\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 113 completed on state (0, 0, 1, 1, -1, 0) with cumulative reward: 1239.1062772759637\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 114 completed on state (1, 0, 0, 1, 0, 0) with cumulative reward: 411.0789722793542\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 115 completed on state (1, 0, 1, 0, 0, 1) with cumulative reward: 200.8069265471658\n",
      "Q explored: 91.25. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 116 completed on state Terminal with cumulative reward: 2672.9986580272043\n",
      "Q explored: 91.875. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 117 completed on state (1, 0, 0, 0, 0, 1) with cumulative reward: 110.80859683488589\n",
      "Q explored: 91.875. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 118 completed on state (1, 0, 1, 1, 0, 0) with cumulative reward: 138.69181180879718\n",
      "Q explored: 91.875. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 119 completed on state (0, 0, 0, 1, 0, 0) with cumulative reward: -72.20115676714185\n",
      "Q explored: 91.875. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 120 completed on state (1, 0, 1, 1, 0, 1) with cumulative reward: 14.219521558466909\n",
      "Q explored: 91.875. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 121 completed on state Terminal with cumulative reward: 2745.2888417902623\n",
      "Q explored: 91.875. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n",
      "Episode 122 completed on state Terminal with cumulative reward: 2703.225986248889\n",
      "Q explored: 91.875. Epsilon: 0.14676390503592945\n",
      "Q measure: 43.75 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[82], line 256\u001b[0m, in \u001b[0;36mQ_learning.train\u001b[1;34m(self, m, max_steps, verbose, render, show_progress)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m steps \u001b[38;5;241m<\u001b[39m max_steps:\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;66;03m# Get the action to make in the current state, and step the environment\u001b[39;00m\n\u001b[0;32m    255\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_greedy(state)\n\u001b[1;32m--> 256\u001b[0m     next_obs, reward, done, truncate, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m     next_reward \u001b[38;5;241m=\u001b[39m fix_reward(reward, next_obs[\u001b[38;5;241m0\u001b[39m], action, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_right_reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_right_skewness, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchange_lane_reward)\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_obs \u001b[38;5;241m=\u001b[39m next_obs\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:258\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mact()\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimulation_frequency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:333\u001b[0m, in \u001b[0;36mRoad.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03mStep the dynamics of each entity on the road.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03m:param dt: timestep [s]\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[1;32m--> 333\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:124\u001b[0m, in \u001b[0;36mIDMVehicle.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03mStep the simulation.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m:param dt: timestep\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dt\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:133\u001b[0m, in \u001b[0;36mVehicle.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheading \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msin(beta) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLENGTH \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m dt\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m dt\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:151\u001b[0m, in \u001b[0;36mVehicle.on_state_update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mget_lane(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mrecord_history:\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mappendleft(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_from\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:75\u001b[0m, in \u001b[0;36mIDMVehicle.create_from\u001b[1;34m(cls, vehicle)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_from\u001b[39m(\u001b[38;5;28mcls\u001b[39m, vehicle: ControlledVehicle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIDMVehicle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    Create a new vehicle from an existing one.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    :return: a new vehicle at the same dynamical state\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheading\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_lane_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_lane_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_speed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_speed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43mroute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvehicle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m v\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:58\u001b[0m, in \u001b[0;36mIDMVehicle.__init__\u001b[1;34m(self, road, position, heading, speed, target_lane_index, target_speed, route, enable_lane_change, timer)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     49\u001b[0m              road: Road,\n\u001b[0;32m     50\u001b[0m              position: Vector,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m              enable_lane_change: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     57\u001b[0m              timer: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lane_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_speed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_lane_change \u001b[38;5;241m=\u001b[39m enable_lane_change\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer \u001b[38;5;241m=\u001b[39m timer \u001b[38;5;129;01mor\u001b[39;00m (np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLANE_CHANGE_DELAY\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\controller.py:42\u001b[0m, in \u001b[0;36mControlledVehicle.__init__\u001b[1;34m(self, road, position, heading, speed, target_lane_index, target_speed, route)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     35\u001b[0m              road: Road,\n\u001b[0;32m     36\u001b[0m              position: Vector,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m              target_speed: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m              route: Route \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_lane_index \u001b[38;5;241m=\u001b[39m target_lane_index \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_speed \u001b[38;5;241m=\u001b[39m target_speed \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:40\u001b[0m, in \u001b[0;36mVehicle.__init__\u001b[1;34m(self, road, position, heading, speed, predition_type)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     35\u001b[0m              road: Road,\n\u001b[0;32m     36\u001b[0m              position: Vector,\n\u001b[0;32m     37\u001b[0m              heading: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     38\u001b[0m              speed: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     39\u001b[0m              predition_type: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant_steering\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_type \u001b[38;5;241m=\u001b[39m predition_type\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\objects.py:36\u001b[0m, in \u001b[0;36mRoadObject.__init__\u001b[1;34m(self, road, position, heading, speed)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheading \u001b[38;5;241m=\u001b[39m heading\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m=\u001b[39m speed\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_closest_lane_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheading\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mget_lane(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Enable collision with other collidables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:61\u001b[0m, in \u001b[0;36mRoadNetwork.get_closest_lane_index\u001b[1;34m(self, position, heading)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _to, lanes \u001b[38;5;129;01min\u001b[39;00m to_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _id, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lanes):\n\u001b[1;32m---> 61\u001b[0m             distances\u001b[38;5;241m.\u001b[39mappend(\u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance_with_heading\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     62\u001b[0m             indexes\u001b[38;5;241m.\u001b[39mappend((_from, _to, _id))\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexes[\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmin(distances))]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:125\u001b[0m, in \u001b[0;36mAbstractLane.distance_with_heading\u001b[1;34m(self, position, heading, heading_weight)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m heading \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance(position)\n\u001b[1;32m--> 125\u001b[0m s, r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m angle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_angle(heading, s))\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(r) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(s \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m \u001b[38;5;241m-\u001b[39m s, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m heading_weight\u001b[38;5;241m*\u001b[39mangle\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:189\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlocal_coordinates\u001b[39m(\u001b[38;5;28mself\u001b[39m, position: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    188\u001b[0m     delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[1;32m--> 189\u001b[0m     longitudinal \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m     lateral \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection_lateral)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Q.train(m=250, max_steps=200, verbose=2, show_progress=50, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Q.plot_rewards_history(relative=False, smoothing=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.load('saved_models/model_Q_learning_20240611-1522')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0, 0, 0, 0, 0) IDLE 5.0129527176058435\n",
      "(1, 0, 0, 0, 0, 0) IDLE 5.0129527176058435\n",
      "(1, 0, 0, 0, 0, 0) IDLE 5.0129527176058435\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 2.1074721570288073\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.8903550540299834\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.38049996757293936\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.16691968959834633\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.07745008077931978\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.03997091379751247\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.02427074503416357\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.017693883298752627\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.014938810347706027\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.013784699577110121\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.01330123808825498\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.013098714190933197\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.013013876142957986\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012978337154821634\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012963449734515325\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.01295721333627306\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012954600884798356\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012953506518721486\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.01295304808451711\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.01295285604464333\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012952775598394556\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012952741899150272\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.01295272778240339\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.01295272186884397\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012952719391629092\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012952718353916953\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012952717919214013\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.01295271773711612\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012952717660832253\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012952717628879146\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.01295271761549252\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012952717609886338\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012952717607537995\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.01295271760655034\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012952717606134673\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012952717605967695\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012952717605896641\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012952717605864666\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012952717605864666\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012952717605864666\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.012952717605864666\n",
      "(0, 0, 0, 0, 0, 0) LANE_RIGHT -0.9671833500443016\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 0.04330200054802535\n",
      "(0, 0, 0, 1, 0, 1) IDLE 0.04864967830959023\n",
      "(0, 0, 0, 1, 0, 1) IDLE 0.05066543537521951\n",
      "(0, 0, 0, 1, 0, 1) IDLE 0.051352315531241643\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.05157868938850725\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.05165246503796317\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.05167642761563716\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.051684195949094826\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.05168671782706291\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.0516875327303172\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.051687799685058856\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.05168788398693309\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.05168791208759802\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.05168791911277096\n",
      "(0, 0, 0, 0, 0, 0) LANE_RIGHT -0.888956363527986\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 0.15302947051988766\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 0.172110338388478\n",
      "(0, 0, 0, 1, 0, 1) IDLE 0.17903733372615652\n",
      "(0, 0, 0, 1, 0, 1) IDLE 0.1813691522313794\n",
      "(0, 0, 0, 1, 0, 0) SLOWER 0.1821345905657013\n",
      "(0, 0, 0, 0, 0, 0) LANE_RIGHT -0.6636890039205425\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 0.4445230598218162\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 0.4910654951824562\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 0.5076521858737336\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 0.5132020336422155\n",
      "(0, 0, 0, 0, 0, 0) LANE_RIGHT -0.15732555390828662\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 1.0651202543066733\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 1.1577499669179314\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 1.1903988220334072\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 1.2012835841377636\n",
      "(0, 0, 0, 0, 0, 0) LANE_RIGHT 0.8100056513038889\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 2.206990995396242\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 2.368817011348158\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 2.4254381738188906\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 2.4442690856534424\n",
      "(0, 0, 0, 0, 0, 0) LANE_RIGHT 2.460963363656412\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 4.105935860679163\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 4.364919294447437\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 4.455062607039707\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 4.484990418165972\n",
      "(0, 0, 0, 0, 0, 0) LANE_RIGHT 5.062806222531452\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 7.042117334073557\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 7.430965913889484\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 7.565782898713221\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 7.61048450889422\n",
      "(0, 0, 0, 0, 0, 0) LANE_RIGHT 8.926996918077817\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 11.339900472674339\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 11.896062433288789\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 12.088308381730366\n",
      "(0, 0, 0, 0, 1, 1) LANE_LEFT 8.458236780779249\n",
      "(0, 0, 0, 0, 1, 1) LANE_LEFT 5.253000961300219\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 5.0634231999681125\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 4.681211592737553\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 4.55827424966003\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 4.518526119569341\n",
      "(0, 0, 0, 0, 0, 0) LANE_RIGHT 5.076445473604598\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 7.047050471764977\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 7.432626193450722\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 7.566327817672356\n",
      "(0, 0, 0, 0, 0, 1) SLOWER 7.610662173460081\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_after_crash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[53], line 85\u001b[0m, in \u001b[0;36mAlgorithm.test\u001b[1;34m(self, sleep_time, time_after_crash, max_steps)\u001b[0m\n\u001b[0;32m     83\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_Q(state)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m steps \u001b[38;5;241m<\u001b[39m max_steps:\n\u001b[1;32m---> 85\u001b[0m     next_obs, reward, done, truncate, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     reward \u001b[38;5;241m=\u001b[39m fix_reward(reward, next_obs[\u001b[38;5;241m0\u001b[39m], action, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_right_reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_right_skewness, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchange_lane_reward)\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(state, decode_meta_action(action), reward)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:258\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mact()\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimulation_frequency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:333\u001b[0m, in \u001b[0;36mRoad.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03mStep the dynamics of each entity on the road.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03m:param dt: timestep [s]\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[1;32m--> 333\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:124\u001b[0m, in \u001b[0;36mIDMVehicle.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03mStep the simulation.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m:param dt: timestep\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dt\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:133\u001b[0m, in \u001b[0;36mVehicle.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheading \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msin(beta) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLENGTH \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m dt\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m dt\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:151\u001b[0m, in \u001b[0;36mVehicle.on_state_update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mget_lane(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mrecord_history:\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mappendleft(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_from\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:75\u001b[0m, in \u001b[0;36mIDMVehicle.create_from\u001b[1;34m(cls, vehicle)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_from\u001b[39m(\u001b[38;5;28mcls\u001b[39m, vehicle: ControlledVehicle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIDMVehicle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    Create a new vehicle from an existing one.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    :return: a new vehicle at the same dynamical state\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheading\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_lane_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_lane_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_speed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_speed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43mroute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvehicle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m v\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:58\u001b[0m, in \u001b[0;36mIDMVehicle.__init__\u001b[1;34m(self, road, position, heading, speed, target_lane_index, target_speed, route, enable_lane_change, timer)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     49\u001b[0m              road: Road,\n\u001b[0;32m     50\u001b[0m              position: Vector,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m              enable_lane_change: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     57\u001b[0m              timer: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lane_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_speed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_lane_change \u001b[38;5;241m=\u001b[39m enable_lane_change\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer \u001b[38;5;241m=\u001b[39m timer \u001b[38;5;129;01mor\u001b[39;00m (np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLANE_CHANGE_DELAY\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\controller.py:42\u001b[0m, in \u001b[0;36mControlledVehicle.__init__\u001b[1;34m(self, road, position, heading, speed, target_lane_index, target_speed, route)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     35\u001b[0m              road: Road,\n\u001b[0;32m     36\u001b[0m              position: Vector,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m              target_speed: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m              route: Route \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_lane_index \u001b[38;5;241m=\u001b[39m target_lane_index \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_speed \u001b[38;5;241m=\u001b[39m target_speed \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:40\u001b[0m, in \u001b[0;36mVehicle.__init__\u001b[1;34m(self, road, position, heading, speed, predition_type)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     35\u001b[0m              road: Road,\n\u001b[0;32m     36\u001b[0m              position: Vector,\n\u001b[0;32m     37\u001b[0m              heading: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     38\u001b[0m              speed: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     39\u001b[0m              predition_type: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant_steering\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_type \u001b[38;5;241m=\u001b[39m predition_type\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\objects.py:36\u001b[0m, in \u001b[0;36mRoadObject.__init__\u001b[1;34m(self, road, position, heading, speed)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheading \u001b[38;5;241m=\u001b[39m heading\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m=\u001b[39m speed\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_closest_lane_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheading\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mget_lane(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Enable collision with other collidables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:61\u001b[0m, in \u001b[0;36mRoadNetwork.get_closest_lane_index\u001b[1;34m(self, position, heading)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _to, lanes \u001b[38;5;129;01min\u001b[39;00m to_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _id, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lanes):\n\u001b[1;32m---> 61\u001b[0m             distances\u001b[38;5;241m.\u001b[39mappend(\u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance_with_heading\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     62\u001b[0m             indexes\u001b[38;5;241m.\u001b[39mappend((_from, _to, _id))\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexes[\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmin(distances))]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:125\u001b[0m, in \u001b[0;36mAbstractLane.distance_with_heading\u001b[1;34m(self, position, heading, heading_weight)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m heading \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance(position)\n\u001b[1;32m--> 125\u001b[0m s, r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m angle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_angle(heading, s))\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(r) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(s \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m \u001b[38;5;241m-\u001b[39m s, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m heading_weight\u001b[38;5;241m*\u001b[39mangle\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:190\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    188\u001b[0m delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m    189\u001b[0m longitudinal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection)\n\u001b[1;32m--> 190\u001b[0m lateral \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Q.test(sleep_time=0, time_after_crash=1, max_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LANE_LEFT': 30.895279442255937,\n",
       " 'LANE_RIGHT': 3.9442164269434232,\n",
       " 'IDLE': 0,\n",
       " 'FASTER': 0,\n",
       " 'SLOWER': 0}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.search_Q((0, 0, 1, 0, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0, 0, 0, 0): 3,\n",
       " (0, 0, 0, 0, 1): 3,\n",
       " (0, 0, 0, 0, -1): 2,\n",
       " (1, 0, 0, 0, 0): 2,\n",
       " (1, 0, 0, 0, 1): 0,\n",
       " (1, 0, 0, 0, -1): 2,\n",
       " (0, 0, 1, 0, 0): 2,\n",
       " (0, 0, 1, 0, 1): 3,\n",
       " (0, 0, 0, 1, 0): 3,\n",
       " (0, 0, 0, 1, -1): 3,\n",
       " (1, 0, 1, 1, 0): 4,\n",
       " (0, 0, 1, 1, 0): 3,\n",
       " (1, 0, 0, 1, 0): 0,\n",
       " (1, 0, 0, 1, -1): 4,\n",
       " (1, 0, 1, 0, 0): 2,\n",
       " (1, 0, 1, 0, 1): 4}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimum_q_kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0, 0, 0)  -> mine: IDLE 358.85,  optimum: LANE_RIGHT 302.91\n",
      "(0, 0, 0, 0, 1, 0)  -> mine: LANE_LEFT 260.75,  optimum: FASTER 3.48\n",
      "(0, 0, 0, 0, -1, 0)  -> mine: IDLE 249.59,  optimum: LANE_RIGHT 131.71\n",
      "(1, 0, 0, 0, 0, 0)  -> mine: LANE_LEFT 273.77,  optimum: LANE_RIGHT 48.15\n",
      "(1, 0, 0, 0, 1, 0)  -> mine: LANE_LEFT 27.41,  optimum: LANE_LEFT 27.41\n",
      "(1, 0, 0, 0, -1, 0)  -> mine: LANE_RIGHT 93.05,  optimum: LANE_RIGHT 93.05\n",
      "(0, 0, 1, 0, 0, 0)  -> mine: LANE_RIGHT 322.25,  optimum: LANE_RIGHT 322.25\n",
      "(0, 0, 1, 0, 1, 0)  -> mine: LANE_RIGHT 174.73,  optimum: FASTER 38.72\n",
      "(0, 0, 0, 1, 0, 0)  -> mine: FASTER 329.96,  optimum: FASTER 329.96\n",
      "(0, 0, 0, 1, -1, 0)  -> mine: IDLE 249.82,  optimum: FASTER 95.72\n",
      "(1, 0, 1, 1, 0, 0)  -> mine: SLOWER 195.85,  optimum: SLOWER 195.85\n",
      "(0, 0, 1, 1, 0, 0)  -> mine: FASTER 297.74,  optimum: FASTER 297.74\n",
      "(1, 0, 0, 1, 0, 0)  -> mine: LANE_LEFT 259.37,  optimum: LANE_LEFT 259.37\n",
      "(1, 0, 0, 1, -1, 0)  -> mine: LANE_RIGHT 13.05,  optimum: SLOWER 1.56\n",
      "(1, 0, 1, 0, 0, 0)  -> mine: LANE_RIGHT 242.66,  optimum: LANE_RIGHT 242.66\n",
      "(1, 0, 1, 0, 1, 0)  -> mine: LANE_LEFT 92.88,  optimum: SLOWER 0\n",
      "(0, 0, 0, 0, 0, 1)  -> mine: FASTER 321.23,  optimum: FASTER 321.23\n",
      "(0, 0, 0, 0, 1, 1)  -> mine: LANE_LEFT 282.25,  optimum: FASTER 36.5\n",
      "(0, 0, 0, 0, -1, 1)  -> mine: IDLE 106.12,  optimum: FASTER 1.92\n",
      "(1, 0, 0, 0, 0, 1)  -> mine: LANE_LEFT 261.21,  optimum: FASTER 39.66\n",
      "(1, 0, 0, 0, 1, 1)  -> mine: LANE_LEFT 19.21,  optimum: FASTER 0\n",
      "(1, 0, 0, 0, -1, 1)  -> mine: LANE_RIGHT 39.6,  optimum: FASTER 2.46\n",
      "(0, 0, 1, 0, 0, 1)  -> mine: FASTER 293.72,  optimum: FASTER 293.72\n",
      "(0, 0, 1, 0, 1, 1)  -> mine: IDLE 180.0,  optimum: FASTER 25.71\n",
      "(0, 0, 0, 1, 0, 1)  -> mine: FASTER 318.28,  optimum: FASTER 318.28\n",
      "(0, 0, 0, 1, -1, 1)  -> mine: FASTER 73.81,  optimum: SLOWER 13.96\n",
      "(1, 0, 1, 1, 0, 1)  -> mine: FASTER 136.64,  optimum: SLOWER 18.85\n",
      "(0, 0, 1, 1, 0, 1)  -> mine: IDLE 302.24,  optimum: SLOWER 120.61\n",
      "(1, 0, 0, 1, 0, 1)  -> mine: FASTER 186.83,  optimum: FASTER 186.83\n",
      "(1, 0, 0, 1, -1, 1)  -> mine: SLOWER 3.8,  optimum: SLOWER 3.8\n",
      "(1, 0, 1, 0, 0, 1)  -> mine: FASTER 102.89,  optimum: FASTER 102.89\n",
      "(1, 0, 1, 0, 1, 1)  -> mine: LANE_LEFT 20.01,  optimum: SLOWER 0\n"
     ]
    }
   ],
   "source": [
    "q_actions = {state: np.argmax([Q.Q[(state, action)] for action in range(5)]) for state in Q.states}\n",
    "for state in optimum_q_kinematics.keys():\n",
    "    print(state, f' -> mine: {decode_meta_action(q_actions[state])} { round(Q.search_Q(state)[decode_meta_action(q_actions[state])],2) }, '\n",
    "          , f'optimum: {decode_meta_action(optimum_q_kinematics[state])} { round(Q.search_Q(state)[decode_meta_action(optimum_q_kinematics[state])],2) }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IDLE': 39.23231392942805,\n",
       " 'FASTER': 25.106315564406323,\n",
       " 'LANE_RIGHT': 24.587116718189687,\n",
       " 'SLOWER': 19.83407492435054,\n",
       " 'LANE_LEFT': 16.890837515325952}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.search_Q((0, 0, 0, 0, -1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0, 0, 0, 0, 1) IDLE 3.761111261845088 (1, 0, 0, 0, 0, 1)\n",
      "(1, 0, 0, 1, 0, 1) LANE_LEFT 3.761111261845088 (1, 0, 0, 1, 0, 1)\n",
      "(1, 0, 0, 1, 0, 0) SLOWER 3.4349486331035033 (1, 0, 0, 1, 0, 0)\n",
      "(1, 0, 0, 1, 0, 4) IDLE 2.034246882425425 (1, 0, 0, 1, 0, 4)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 1.2467552983975043 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 0.7596423392365345 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 0.45774182542647246 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 0.27374056859702556 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 0.16302745099819393 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 0.09689302960626112 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 0.05753151778386645 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 0.03414507269538891 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 0.020261171028729485 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 0.012021620534089728 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 0.00713254839998978 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 0.004231741033485115 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 0.0025106733738304854 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 0.0014895666586083323 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 0.0008837492408728664 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 0.0005243217773580611 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 0.0003110760828288761 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 0.00018455902862779539 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 0.00010949743545207724 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 6.496397489552663e-05 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 3.8542619519965626e-05 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 2.286703539056134e-05 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 1.3566833627187691e-05 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 8.04909650131691e-06 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 4.77546612920321e-06 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 2.8332467811864603e-06 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 1, -1, 1) IDLE 1.6809431979680767e-06 (0, 0, 0, 1, -1, 1)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 9.972904795318982e-07 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 5.91684658601821e-07 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 3.510418888907907e-07 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 2.0827041424809067e-07 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 1.23565211396226e-07 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 7.331027518375777e-08 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 4.3494413404232546e-08 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 2.580489422143728e-08 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 1.5309844059174793e-08 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 9.083208851734526e-09 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 5.388995916177919e-09 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 3.1972504643817956e-09 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 1.8969039672356303e-09 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 1.1254170928509666e-09 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 6.677005615074449e-10 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 3.9614178604097106e-10 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 2.3502799706420774e-10 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 1.3944045917924086e-10 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 8.272849072454846e-11 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 4.908073947262892e-11 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 2.9118041311448906e-11 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 1.7276846620006836e-11 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 1.0250467141759145e-11 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 6.080469461267057e-12 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 3.610445276081009e-12 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 2.1396218130576017e-12 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 1.2709833185908792e-12 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 7.540634783254063e-13 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 4.4497738826976274e-13 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 2.611244553918368e-13 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 1.545430450278218e-13 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 9.325873406851315e-14 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 5.595524044110789e-14 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 3.4638958368304884e-14 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 1.865174681370263e-14 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 1.5987211554602254e-14 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 1.5987211554602254e-14 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 1.5987211554602254e-14 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 1.5987211554602254e-14 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 1.5987211554602254e-14 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 1.5987211554602254e-14 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 1.5987211554602254e-14 (0, 0, 0, 0, -1, 0)\n",
      "(0, 0, 0, 0, -1, 1) LANE_LEFT 1.5987211554602254e-14 (0, 0, 0, 0, -1, 1)\n",
      "(0, 0, 0, 0, -1, 0) IDLE 1.5987211554602254e-14 (0, 0, 0, 0, -1, 0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_after_crash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[53], line 86\u001b[0m, in \u001b[0;36mAlgorithm.test\u001b[1;34m(self, sleep_time, time_after_crash, max_steps)\u001b[0m\n\u001b[0;32m     84\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_Q(state)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m steps \u001b[38;5;241m<\u001b[39m max_steps:\n\u001b[1;32m---> 86\u001b[0m     next_obs, reward, done, truncate, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state()\n\u001b[0;32m     88\u001b[0m     next_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_Q(next_state)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:264\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;241m<\u001b[39m frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# Last frame will be rendered through env.render() as usual\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_automatic_rendering\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_auto_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:325\u001b[0m, in \u001b[0;36mAbstractEnv._automatic_rendering\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_video_wrapper\u001b[38;5;241m.\u001b[39mvideo_recorder\u001b[38;5;241m.\u001b[39mcapture_frame()\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:287\u001b[0m, in \u001b[0;36mAbstractEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;241m=\u001b[39m EnvViewer(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_auto_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39moffscreen:\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39mhandle_events()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\graphics.py:137\u001b[0m, in \u001b[0;36mEnvViewer.display\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal_time_rendering\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 137\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSAVE_IMAGES \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirectory:\n\u001b[0;32m    140\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim_surface, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirectory \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhighway-env_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Q.test(sleep_time=0, time_after_crash=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the optimum Q kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = Q_learning(print_stats=True, \n",
    "        epsilon=1, \n",
    "        epsilon_decay=0.96,\n",
    "        min_epsilon=0.1,\n",
    "        alpha=0.1, \n",
    "        gamma=0.98, \n",
    "        state_type='danger', \n",
    "        policy_frequency=2,    # 2\n",
    "        sim_frequency=20,     # 10 \n",
    "        danger_threshold_x=15,  #15\n",
    "        danger_threshold_y=13,  #15 \n",
    "        x_speed_coef=1, #1.5\n",
    "        y_speed_coef=0.5, #1.5\n",
    "        colision_reward=-100,\n",
    "        high_speed_reward=5,\n",
    "        reward_speed_range=[20, 30],\n",
    "        to_right_reward=5, \n",
    "        to_right_skewness=20,\n",
    "        change_lane_reward=-2.5, \n",
    "        lane_tolerance=2, \n",
    "        special_Q=True)   \n",
    "\n",
    "Q.Q = {}\n",
    "for key, value in optimum_q_kinematics.items():\n",
    "    for i in range(6): \n",
    "        if i == value:\n",
    "            Q.Q[key, i] = 1\n",
    "        else:\n",
    "            Q.Q[key, i] = 0\n",
    "\n",
    "Q.test(sleep_time=0.1, time_after_crash=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation testing kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.03333333333333\n",
      "20.127211934156396\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[157], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m      6\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m----> 7\u001b[0m     obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     cum_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Print non-zero obs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:258\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mact()\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimulation_frequency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:333\u001b[0m, in \u001b[0;36mRoad.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03mStep the dynamics of each entity on the road.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03m:param dt: timestep [s]\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[1;32m--> 333\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:124\u001b[0m, in \u001b[0;36mIDMVehicle.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03mStep the simulation.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m:param dt: timestep\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dt\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:133\u001b[0m, in \u001b[0;36mVehicle.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheading \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msin(beta) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLENGTH \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m dt\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m dt\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:148\u001b[0m, in \u001b[0;36mVehicle.on_state_update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_state_update\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad:\n\u001b[1;32m--> 148\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_closest_lane_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheading\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mget_lane(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index)\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mrecord_history:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:61\u001b[0m, in \u001b[0;36mRoadNetwork.get_closest_lane_index\u001b[1;34m(self, position, heading)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _to, lanes \u001b[38;5;129;01min\u001b[39;00m to_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _id, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lanes):\n\u001b[1;32m---> 61\u001b[0m             distances\u001b[38;5;241m.\u001b[39mappend(\u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance_with_heading\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     62\u001b[0m             indexes\u001b[38;5;241m.\u001b[39mappend((_from, _to, _id))\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexes[\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmin(distances))]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:125\u001b[0m, in \u001b[0;36mAbstractLane.distance_with_heading\u001b[1;34m(self, position, heading, heading_weight)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m heading \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance(position)\n\u001b[1;32m--> 125\u001b[0m s, r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m angle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_angle(heading, s))\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(r) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(s \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m \u001b[38;5;241m-\u001b[39m s, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m heading_weight\u001b[38;5;241m*\u001b[39mangle\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:190\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    188\u001b[0m delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m    189\u001b[0m longitudinal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection)\n\u001b[1;32m--> 190\u001b[0m lateral \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate an episode of the environment and show the rewards and cumulative rewards\n",
    "cum_reward = 0\n",
    "with gym.make(\"highway-v0\", config=kinematics, render_mode='human') as env:\n",
    "    obs = env.reset()\n",
    "    for _ in range(100):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "        # Print non-zero obs\n",
    "        obs_non = obs[~np.all(obs == 0, axis=1)]\n",
    "        print(reward)\n",
    "        if done:\n",
    "            break\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time to collision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.  0.  0.5 1. ]\n",
      " [0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.5 1.  1. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "[[0.5 1.  1.  0.5 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "[[0.  1.  1.  0.5 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 1.  1.  0.  0. ]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [1.  1.  0.5 0.  0. ]]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[0.  0.  0.  0.  0.5]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.5 1.  1.  0. ]]\n",
      "[[0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 1.  1.  0.  0. ]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [1.  1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n"
     ]
    }
   ],
   "source": [
    "class TimeToCollision:\n",
    "    def __init__(horizon=5,\n",
    "                policy_frequency=1,\n",
    "                simulation_frequency=10):\n",
    "\n",
    "        self.config = default_config.copy()\n",
    "        self.config[\"observation\"] =  {\n",
    "        \"type\": \"TimeToCollision\",\n",
    "        \"horizon\": horizon}\n",
    "        self.config['policy_frequency'] = policy_frequency\n",
    "        self.config['simulation_frequency'] = simulation_frequency\n",
    "\n",
    "    def get_state(self, env): \n",
    "        grid = env.vehicle.speed_index\n",
    "        return self.current_obs[grid]\n",
    "\n",
    "    def test_env(self):\n",
    "        with gym.make(\"highway-v0\", config=self.config, render_mode='human') as env:\n",
    "            obs = env.reset()\n",
    "            for _ in range(1000):\n",
    "                action = env.action_space.sample()\n",
    "                obs, reward, done, truncated, info = env.step(action)\n",
    "                print(self.get_state, reward)\n",
    "                if done:\n",
    "                    break\n",
    "                time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
