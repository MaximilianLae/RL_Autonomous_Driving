{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "\n",
    "    # Parametrization bellow cannot be changed\n",
    "    \"lanes_count\" : 10, # The environment must always have 10 lanes\n",
    "    \"vehicles_count\": 50, # The environment must always have 50 other vehicles\n",
    "    \"duration\": 120,  # [s] The environment must terminate never before 120 seconds\n",
    "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.IDMVehicle\", # This is the policy of the other vehicles\n",
    "    \"initial_spacing\": 2, # Initial spacing between vehicles needs to be at most 2\n",
    "\n",
    "    # Refer to refer to https://highway-env.farama.org/observations/ to change observation space type\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\"\n",
    "    },\n",
    "\n",
    "    # Refer to refer to https://highway-env.farama.org/actions/ to change action space type\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteMetaAction\",\n",
    "    },\n",
    "\n",
    "    # Parameterization bellow can be changed (as it refers mostly to the reward system)\n",
    "    # \"collision_reward\": -10,  # The reward received when colliding with a vehicle. (Can be changed)\n",
    "    # \"reward_speed_range\": [20, 30],  # [m/s] The reward for high speed is mapped linearly from this range to [0, HighwayEnv.HIGH_SPEED_REWARD]. (Can be changed)\n",
    "    \"simulation_frequency\": 15, #15,  # [Hz] (Can be changed)\n",
    "    \"policy_frequency\": 5, #5,  # [Hz] (Can be changed)\n",
    "\n",
    "    \"collision_reward\": -100,  # The reward received when colliding with a vehicle.\n",
    "    \"right_lane_reward\": 0.1,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
    "    # zero for other lanes.\n",
    "    \"high_speed_reward\": 100,  # The reward received when driving at full speed, linearly mapped to zero for\n",
    "    # lower speeds according to config[\"reward_speed_range\"].\n",
    "    \"lane_change_reward\": 0,  # The reward received at each lane change action.\n",
    "    \"reward_speed_range\": [20, 30],\n",
    "    \n",
    "    # Parameters defined bellow are purely for visualiztion purposes! You can alter them as you please\n",
    "    \"screen_width\": 800,  # [px]\n",
    "    \"screen_height\": 600,  # [px]\n",
    "    \"centering_position\": [0.5, 0.5],\n",
    "    \"scaling\": 5,\n",
    "    \"show_trajectories\": True,\n",
    "    \"render_agent\": True,\n",
    "    \"offscreen_rendering\": False\n",
    "}\n",
    "\n",
    "default_config = configuration.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 91.46669937980411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occupancyGrid = configuration.copy()\n",
    "occupancyGrid[\"observation\"] =  {\n",
    "    \"type\": \"OccupancyGrid\",\n",
    "    # \"vehicles_count\": 50,\n",
    "    \"features\": [\n",
    "                \"presence\",\n",
    "                #\"x\", \"y\", \n",
    "                #\"vx\", \"vy\"\n",
    "                ],\n",
    "    # \"features_range\": {\n",
    "    #      \"x\": [-500, 500],\n",
    "    #      \"y\": [-500, 500],\n",
    "    #     \"vx\": [-20, 20],\n",
    "    #     \"vy\": [-20, 20]\n",
    "    # },\n",
    "    \"grid_size\": [[-100, 100], [-100, 100]],    # X controls how many lanes, Y controls how far ahead\n",
    "    \"grid_step\": [1, 1],\n",
    "    #\"absolute\": False,                     # Not implemented in the library\n",
    "    #\"as_image\": True,\n",
    "    # \"align_to_vehicle_axes\" : True\n",
    "}\n",
    "\n",
    "# The higher the number, the more frequent the policy and the simulation frequencies, the slower the simulation\n",
    "occupancyGrid[\"simulation_frequency\"] = 15\n",
    "occupancyGrid[\"policy_frequency\"] = 1\n",
    "occupancyGrid[\"normalize_reward\"] = False\n",
    "\n",
    "env = gym.make('highway-v0', render_mode='human', config=occupancyGrid)\n",
    "\n",
    "obs, info = env.reset(seed = 30)\n",
    "# Do one step\n",
    "action = 3\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "print(\"Reward:\", reward)\n",
    "\n",
    "env.close()\n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.06666666666667 50.06666666666667\n",
      "91.52225493535967 141.58892160202635\n",
      "98.60652722998472 240.19544883201107\n",
      "99.81714601602411 340.0125948480352\n",
      "-99.93333333333334 240.07926151470184\n"
     ]
    }
   ],
   "source": [
    "# Generate an episode of the environment and show the rewards and cumulative rewards\n",
    "cum_reward = 0\n",
    "with gym.make(\"highway-v0\", config=occupancyGrid, render_mode='human') as env:\n",
    "    obs = env.reset()\n",
    "    for _ in range(1000):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "        print(reward, cum_reward)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccupancyGrid():\n",
    "    def __init__(\n",
    "            self,\n",
    "            grid_size=[[-50, 50], [-50, 50]],  # X controls the lane-width, Y controls how far ahead\n",
    "            grid_step=[1, 1],\n",
    "            colision_reward=-100,\n",
    "            skew_speed=1,\n",
    "            policy=None,\n",
    "            sim_frequency=10,\n",
    "            policy_frequency=1,\n",
    "            render_mode = 'human',\n",
    "            seed = 50,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Occupancy view class constructor\n",
    "        Arguments:\n",
    "            grid_size: list of lists, the size of the grid in the x and y direction, where x controls the lane-width and y controls how far ahead. Lanes are 5m wide, and the car position is (0,0)\n",
    "            grid_step: list, the step size of the grid in the x and y direction, in meters\n",
    "            colision_reward: float, the reward to give when a colision occurs\n",
    "            skew_speed: float, the skew speed to apply to the reward\n",
    "            policy: function, the policy to use in the simulation\n",
    "            sim_frequency: int, the frequency of the simulation\n",
    "            policy_frequency: int, the frequency of the policy\n",
    "            render_mode: str, the mode to render the simulation\n",
    "            seed: int, the seed to use in the simulation\n",
    "        \"\"\"\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.grid_step = grid_step\n",
    "        self.config = default_config.copy()\n",
    "        self.config[\"observation\"] =  {\n",
    "            \"type\": \"OccupancyGrid\",\n",
    "            \"features\": [\"presence\"],\n",
    "            \"grid_size\": grid_size,    # X controls how many lanes, Y controls how far ahead\n",
    "            \"grid_step\": grid_step,\n",
    "        }\n",
    "        self.config[\"simulation_frequency\"] = sim_frequency\n",
    "        self.config[\"policy_frequency\"] = policy_frequency\n",
    "        self.render_mode = render_mode\n",
    "        self.seed = seed\n",
    "        self.policy = policy\n",
    "        self.colision_reward = colision_reward\n",
    "        self.skew_speed = skew_speed\n",
    "        self.initialize_states()\n",
    "\n",
    "\n",
    "    def initialize_states(self):\n",
    "        \"\"\"\n",
    "        Initialize the states of the occupancy grid\n",
    "        \"\"\"\n",
    "        # Start the environment\n",
    "        with gym.make('highway-v0', render_mode=self.render_mode, config=self.config) as env:\n",
    "            obs, info = env.reset(seed = self.seed)\n",
    "            self.current_obs = obs\n",
    "            self.env = env\n",
    "        \n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Get the state of the environment\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def test_env(self):\n",
    "        \"\"\"\n",
    "        Function to test the environment with a random policy, or with a policy\n",
    "        \"\"\"\n",
    "        obs, info = self.env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        while not done:\n",
    "            # start = time.time()\n",
    "            if self.policy is None:\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                action = self.policy()\n",
    "            obs, reward, done, truncate, info = self.env.step(action)\n",
    "            self.current_obs = obs\n",
    "            print(self.get_state(type='lane-wise', decode=True), decode_meta_action(action), reward)\n",
    "            time.sleep(1)\n",
    "            # end = time.time()\n",
    "            # print(f\"Time taken: {end-start}\")\n",
    "        self.env.close()\n",
    "        return info[\"score\"]\n",
    "\n",
    "# ------------------- SARSA -------------------    \n",
    "class Sarsa(OccupancyGrid):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha=0.75,\n",
    "        gamma=0.95,\n",
    "        epsilon=0.6,\n",
    "        print_stats=False,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        SARSA class constructor\n",
    "        Arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            m: int, the number of episodes to train the agent for\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.initialize_Q(print_stats)\n",
    "        self.alpha, self.gamma, self.epsilon = alpha, gamma, epsilon\n",
    "\n",
    "    def policy_Q(self, state):\n",
    "        values = [self.Q[(state, action)] for action in range(5)]\n",
    "        return np.argmax(values)   \n",
    "\n",
    "    def initialize_Q(self, print_stats = False):\n",
    "        # Combine the possible states with the possible actions\n",
    "        keys = list(itertools.product(self.states, range(5)))       # 5 possible actions, 0-4: left, idle, right, accelerate, decelerate\n",
    "        if len(keys) > 150000:\n",
    "            print(\"Warning: The number of states is too large, consider reducing the number of states\")\n",
    "        if print_stats:\n",
    "            print(f\"Number of states: {len(keys)}\")   \n",
    "        self.Q = {key: 0 for key in keys}\n",
    "\n",
    "    def epsilon_greedy(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(5)\n",
    "        else:\n",
    "            values = [self.Q[(state, action)] for action in range(5)]\n",
    "            return np.argmax(values)\n",
    "        \n",
    "    def train(self, m=100, verbose=0): \n",
    "        env = gym.make('highway-v0', render_mode=None, config=self.config)\n",
    "        obs, info = env.reset(seed = np.random.randint(1000))\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        state = self.get_state()\n",
    "        action = self.epsilon_greedy(state)\n",
    "        q_explored = np.count_nonzero(list(self.Q.values()))\n",
    "        for i in tqdm(range(m)):\n",
    "            env.reset()\n",
    "            cum_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                next_state = self.get_state()\n",
    "                next_action = self.epsilon_greedy(next_state)\n",
    "\n",
    "                print(next_state, decode_meta_action(next_action), reward) if verbose > 2 else None\n",
    "                if self.Q[(state, action)] == 0:\n",
    "                    q_explored += 1\n",
    "                cum_reward += reward\n",
    "\n",
    "                self.Q[(state, action)] += self.alpha*(reward + self.gamma*self.Q[(next_state, next_action)] - self.Q[(state, action)])\n",
    "                state, action = next_state, next_action\n",
    "                self.current_obs = next_obs\n",
    "            print(f\"Episode {i+1} completed, cumulative reward: {cum_reward}\") if verbose > 0 else None\n",
    "            print(f\"Q explored: {100*q_explored/len(self.Q)}\") if verbose > 1 else None\n",
    "        env.close()\n",
    "\n",
    "    def test(self):\n",
    "        env = gym.make('highway-v0', render_mode=self.render_mode, config=self.config)\n",
    "        obs, info = env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        state = self.get_state(self.state_type)\n",
    "        action = self.policy_Q(state)\n",
    "        while not done:\n",
    "            next_obs, reward, done, truncate, info = env.step(action)\n",
    "            next_state = self.get_state(self.state_type)\n",
    "            next_action = self.policy_Q(next_state)\n",
    "            state, action = next_state, next_action\n",
    "            self.current_obs = next_obs\n",
    "            print(next_state)\n",
    "        env.close()\n",
    "        return info[\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- grid_size=[[-50, 50], [-50, 50]],  # X controls the lane-width, Y controls how far ahead\n",
    "- grid_step=[1, 1],\n",
    "- n_closest=3,\n",
    "- ss_bins=[5,6],\n",
    "- crop_dist=[[-10,10], [-10,25]],\n",
    "- policy=None,\n",
    "- sim_frequency=15,\n",
    "- policy_frequency=5,\n",
    "- render_mode = 'human',\n",
    "- seed = 50,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "copyQ = a.Q.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 540\n"
     ]
    }
   ],
   "source": [
    "a = Sarsa(print_stats=True, epsilon=0.6, alpha=0.3, gamma=0.8)\n",
    "a.Q = copyQ.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! TODO !!!\n",
    "\n",
    "- Use kinematics to substitute the current occupancy grid class\n",
    "\n",
    "- He cannot know when he cant turn left or right because the lane is the final one\n",
    "\n",
    "- Change the occupancy class to use occupancy grid with 5m grid size, 3 lanes and -30, 30 ahead\n",
    "\n",
    "- Make a plot history \n",
    "\n",
    "- Check how many times each state is visited\n",
    "\n",
    "- Check the action distribution, so as to see if slowing down is the most chosen action and the one with the best Q value\n",
    "\n",
    "- Change the reward for colision, so that the agent goes faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! IDEA WITH KINEMATICS !!!\n",
    "- State space in this maner: (danger ahead, danger left, danger right, danger behind, lane position, (maybe) speed)\n",
    "- Lane position can be 0 if in the middle, 1 if in the right, -1 if in the left\n",
    "\n",
    "- We need speed, because if the speed is too fast, it might not be able to turn in time\n",
    "- Actually, we could just increase the safety distance, and then we wouldn't need speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c280dc5269724502bed494f225b52c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[106], line 307\u001b[0m, in \u001b[0;36mSarsa.train\u001b[1;34m(self, m, verbose)\u001b[0m\n\u001b[0;32m    305\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m--> 307\u001b[0m     next_obs, reward, done, truncate, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     reward \u001b[38;5;241m=\u001b[39m fix_reward(reward, done\u001b[38;5;241m=\u001b[39mdone, colision_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolision_reward, skew_speed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskew_speed)\n\u001b[0;32m    309\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:257\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_control\u001b[39m\u001b[38;5;124m\"\u001b[39m] \\\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:324\u001b[0m, in \u001b[0;36mRoad.act\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decide the actions of each entity on the road.\"\"\"\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[1;32m--> 324\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:95\u001b[0m, in \u001b[0;36mIDMVehicle.act\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_road()\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_lane_change:\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchange_lane_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteering_control(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_lane_index)\n\u001b[0;32m     97\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:214\u001b[0m, in \u001b[0;36mIDMVehicle.change_lane_policy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# Does the MOBIL model recommend a lane change?\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmobil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlane_index\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_lane_index \u001b[38;5;241m=\u001b[39m lane_index\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:236\u001b[0m, in \u001b[0;36mIDMVehicle.mobil\u001b[1;34m(self, lane_index)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Do I have a planned route for a specific lane which is safe for me to access?\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m old_preceding, old_following \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneighbour_vehicles\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m self_pred_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macceleration(ego_vehicle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, front_vehicle\u001b[38;5;241m=\u001b[39mnew_preceding)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroute \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroute[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# Wrong direction\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:361\u001b[0m, in \u001b[0;36mRoad.neighbour_vehicles\u001b[1;34m(self, vehicle, lane_index)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vehicle \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, Landmark):  \u001b[38;5;66;03m# self.network.is_connected_road(v.lane_index,\u001b[39;00m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;66;03m# lane_index, same_lane=True):\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m         s_v, lat_v \u001b[38;5;241m=\u001b[39m \u001b[43mlane\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lane\u001b[38;5;241m.\u001b[39mon_lane(v\u001b[38;5;241m.\u001b[39mposition, s_v, lat_v, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    363\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:190\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    188\u001b[0m delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m    189\u001b[0m longitudinal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection)\n\u001b[1;32m--> 190\u001b[0m lateral \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a.train(m=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.37037037037037\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for keys, values in a.Q.items():\n",
    "    if values != 0:\n",
    "        count += 1\n",
    "\n",
    "print(100*count/len(a.Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: LANE_LEFT, Q-value: -46.81659543807035\n",
      "Action: IDLE, Q-value: -13.458657418241788\n",
      "Action: LANE_RIGHT, Q-value: -12.65590344059557\n",
      "Action: FASTER, Q-value: -7.4392318919133285\n",
      "Action: SLOWER, Q-value: -8.453300283131032\n"
     ]
    }
   ],
   "source": [
    "# Check the values for Q for this state (15, 30, 18, 18)\n",
    "state = (30, 30, 18, 18)\n",
    "for action in range(5):\n",
    "    print(f\"Action: {decode_meta_action(action)}, Q-value: {a.Q[(state, action)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 30, 18, 18)\n",
      "(30, 30, 18, 6)\n",
      "(30, 30, 18, 18)\n",
      "(15, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(30, 30, 18, 18)\n",
      "(30, 30, 18, 6)\n",
      "(30, 30, 18, 12)\n",
      "(15, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 12)\n",
      "(5, 30, 18, 12)\n",
      "(5, 30, 18, 12)\n",
      "(10, 30, 18, 12)\n",
      "(10, 30, 18, 12)\n",
      "(30, 30, 12, 6)\n",
      "(15, 30, 18, 12)\n",
      "(10, 30, 18, 12)\n",
      "(10, 30, 18, 6)\n",
      "(30, 10, 6, 6)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[106], line 339\u001b[0m, in \u001b[0;36mSarsa.test\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mprint\u001b[39m(next_state)\n\u001b[0;32m    338\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'score'"
     ]
    }
   ],
   "source": [
    "a.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_occupancy = OcupancyGrid(render_mode=None)\n",
    "# print(new_occupancy.x_bins, new_occupancy.y_bins, len(new_occupancy.states))\n",
    "# new_occupancy.test_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________\n",
    "### Using kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 83.56591  ,  28.       ,  25.       ,   0.       ],\n",
       "       [  9.4535885,   8.       ,  -1.5146654,   0.       ],\n",
       "       [ 20.187185 , -24.       ,  -1.9328905,   0.       ],\n",
       "       [ 31.133034 , -24.       ,  -1.129222 ,   0.       ],\n",
       "       [ 41.896053 ,   0.       ,  -1.7400944,   0.       ],\n",
       "       [ 51.945835 ,  -8.       ,  -3.565016 ,   0.       ],\n",
       "       [ 62.276524 , -16.       ,  -3.3216567,   0.       ],\n",
       "       [ 72.60146  ,   8.       ,  -1.0905089,   0.       ],\n",
       "       [ 82.24522  ,  -8.       ,  -2.182405 ,   0.       ],\n",
       "       [ 91.352806 , -20.       ,  -3.5367248,   0.       ],\n",
       "       [100.35392  , -16.       ,  -3.2140508,   0.       ],\n",
       "       [109.795944 ,  -4.       ,  -3.2532372,   0.       ],\n",
       "       [119.59274  ,   0.       ,  -1.6719042,   0.       ],\n",
       "       [129.31784  ,   4.       ,  -2.6289418,   0.       ],\n",
       "       [138.54662  ,  -4.       ,  -2.0985248,   0.       ],\n",
       "       [149.64609  , -16.       ,  -1.698277 ,   0.       ],\n",
       "       [159.69713  ,   0.       ,  -1.612952 ,   0.       ],\n",
       "       [169.55093  ,   4.       ,  -2.264635 ,   0.       ],\n",
       "       [179.66725  ,   0.       ,  -3.794033 ,   0.       ],\n",
       "       [190.1403   ,   4.       ,  -2.7294507,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kinematics = configuration.copy()\n",
    "kinematics[\"observation\"] =  {\n",
    "    \"type\": \"Kinematics\",\n",
    "    \"vehicles_count\": 50,\n",
    "    \"features\": [\"x\", \"y\", \"vx\", \"vy\"],\n",
    "    # \"features_range\": {\n",
    "    #     \"x\": [-40, 40],\n",
    "    #     \"y\": [-40, 40],\n",
    "    #     \"vx\": [-200, 200],\n",
    "    #     \"vy\": [-200, 200]\n",
    "    # }, \n",
    "    \"absolute\": False,\n",
    "    \"normalize\": False,\n",
    "}\n",
    "\n",
    "# The higher the number, the more frequent the policy and the simulation frequencies, the slower the simulation\n",
    "kinematics[\"simulation_frequency\"] = 10\n",
    "kinematics[\"policy_frequency\"] = 2\n",
    "\n",
    "env = gym.make('highway-v0', render_mode='human', config=kinematics)\n",
    "\n",
    "obs, info = env.reset(seed = 10)\n",
    "\n",
    "env.close()\n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_reward(reward, position, to_right_reward=5, to_right_skewness=2, change_lane_reward=-0.5, past_position=None):\n",
    "    \"\"\"\n",
    "    This function is used to correct the reward function, which is not correctly outputted by the environment\n",
    "    Params: \n",
    "        reward: float, the reward to fix    \n",
    "        position: tuple, the position of the car\n",
    "        to_right_reward: float, the reward to give to the driver \n",
    "        to_right_skewness: float, the skewness to apply to the to_right_reward\n",
    "        change_lane_reward: float, the reward to give when changing lanes\n",
    "    \"\"\"\n",
    "    \n",
    "    lane = position[1]\n",
    "    lane_value = to_right_reward * ((lane/36)**to_right_skewness)\n",
    "\n",
    "    if past_position is not None:\n",
    "        past_lane = past_position[1]\n",
    "        if np.abs(past_lane - lane) > 1.5: \n",
    "            reward += change_lane_reward\n",
    "    \n",
    "    return reward + lane_value\n",
    "    \n",
    "\n",
    "def decode_meta_action(action):\n",
    "    \"\"\"\n",
    "    Function to output the corresponding action in text-form\n",
    "    \"\"\"\n",
    "    assert action in range(5), \"The action must be between 0 and 4\"\n",
    "    if action == 0:\n",
    "        return \"LANE_LEFT\"\n",
    "    elif action == 1:\n",
    "        return \"IDLE\"\n",
    "    elif action == 2:\n",
    "        return \"LANE_RIGHT\"\n",
    "    elif action == 3:\n",
    "        return \"FASTER\"\n",
    "    elif action == 4:\n",
    "        return \"SLOWER\"\n",
    "    \n",
    "def decode_danger(state):\n",
    "    \"\"\"\n",
    "    Function to decode the danger state\n",
    "    \"\"\"\n",
    "    state_meaning = ['front', 'back', 'left', 'right']\n",
    "    to_return = ''\n",
    "    for i in range(3): \n",
    "        if state[i] == 1:\n",
    "            if to_return == '':\n",
    "                to_return = 'Danger in '\n",
    "            to_return += state_meaning[i] + ', '\n",
    "    if to_return == '': \n",
    "        to_return = 'No danger'\n",
    "    if state[4] == -1:\n",
    "        to_return += '. Cant turn left'\n",
    "    elif state[4] == 1:\n",
    "        to_return += '. Cant turn right'\n",
    "    return to_return\n",
    "    \n",
    "\n",
    "def decode_Q(Q): \n",
    "    \"\"\"\n",
    "    Function to decode the Q-values\n",
    "    \"\"\"\n",
    "    return {(decode_danger(key[0]), decode_meta_action(key[1])) : value for key, value in Q.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class ObservationType: \n",
    "    def __init__(self, \n",
    "                sim_frequency=10,\n",
    "                policy_frequency=2,\n",
    "                render_mode='human',\n",
    "                seed=None,\n",
    "                colision_reward=-20, high_speed_reward=5, reward_speed_range=[20, 30], to_right_reward=5, to_right_skewness=2, change_lane_reward=-0.5):\n",
    "\n",
    "        \"\"\"\n",
    "        Constructor for the ObservationType class\n",
    "        Arguments:\n",
    "            sim_frequency: int, the frequency of the simulation\n",
    "            policy_frequency: int, the frequency of the policy\n",
    "            render_mode: str, the mode to render the simulation\n",
    "            seed: int, the seed to use in the simulation\n",
    "            colision_reward: float, the reward to give when a colision occurs\n",
    "            high_speed_reward: float, the reward to give when driving at high speed\n",
    "            reward_speed_range: list, the range of speeds to give the high speed reward\n",
    "            to_right_reward: float, the reward to give when driving to the right, mapped polynomially with to_right_skewness\n",
    "            to_right_skewness: float, the skewness to apply to the to_right_reward\n",
    "            change_lane_reward: float, the reward to give when changing lanes\n",
    "        \"\"\"\n",
    "        self.config = default_config.copy()\n",
    "\n",
    "        self.config.update({\n",
    "            \"simulation_frequency\": sim_frequency,\n",
    "            \"policy_frequency\": policy_frequency,\n",
    "            \"collision_reward\": colision_reward,\n",
    "            \"high_speed_reward\": high_speed_reward,\n",
    "            \"reward_speed_range\": reward_speed_range\n",
    "        })\n",
    "        self.config['normalize_reward'] = False\n",
    "        self.seed = seed\n",
    "        self.render_mode = render_mode\n",
    "        self.to_right_reward, self.to_right_skewness, self.change_lane_reward = to_right_reward, to_right_skewness, change_lane_reward\n",
    "\n",
    "\n",
    "class Kinematics(ObservationType):\n",
    "    def __init__(self, \n",
    "                 seed=None, \n",
    "                 state_type='danger',\n",
    "                 policy=None,\n",
    "                 crop=100, lane_tolerance=2, danger_threshold_x=10, danger_threshold_y=15, x_speed_coef=1, y_speed_coef=1,\n",
    "                 **kwargs):\n",
    "\n",
    "            \"\"\"\n",
    "            Kinematics class constructor\n",
    "            Arguments:\n",
    "                seed: int, the seed to use in the test environment\n",
    "                state_type: str, the type of state to use. Options are 'n_neighbours' or 'danger'\n",
    "                policy: function, the policy to use in the simulation\n",
    "                crop: int, the crop distance to use in the state\n",
    "                lane_tolerance: int, the tolerance to use in the lane\n",
    "                danger_threshold_x: float, the threshold to use in the x direction for the danger state\n",
    "                danger_threshold_y: float, the threshold to use in the y direction for the danger state\n",
    "                x_speed_coef: float, the coefficient to use in the x direction for the danger state\n",
    "                y_speed_coef: float, the coefficient to use in the y direction for the danger state\n",
    "            \n",
    "            Other Arguments (for Observation Type):\n",
    "                sim_frequency: int, the frequency of the simulation\n",
    "                policy_frequency: int, the frequency of the policy\n",
    "                render_mode: str, the mode to render the simulation\n",
    "                seed: int, the seed to use in the simulation\n",
    "                colision_reward: float, the reward to give when a colision occurs\n",
    "                high_speed_reward: float, the reward to give when driving at high speed\n",
    "                reward_speed_range: list, the range of speeds to give the high speed reward\n",
    "                to_right_reward: float, the reward to give when driving to the right, mapped polynomially with to_right_skewness\n",
    "                to_right_skewness: float, the skewness to apply to the to_right_reward\n",
    "                change_lane_reward: float, the reward to give when changing lanes\n",
    "            \"\"\"\n",
    "            super().__init__(**kwargs)\n",
    "\n",
    "            self.config[\"observation\"] =  {\n",
    "                \"type\": \"Kinematics\",\n",
    "                \"vehicles_count\": 50,\n",
    "                \"features\": [\"x\", \"y\", \"vx\", \"vy\"],\n",
    "                \"absolute\": False,\n",
    "                \"normalize\": False,\n",
    "            }\n",
    "\n",
    "            self.policy = policy\n",
    "\n",
    "            with gym.make('highway-v0', render_mode=self.render_mode, config=self.config) as env:\n",
    "                self.env = env\n",
    "                obs, info = env.reset(seed = self.seed)\n",
    "                self.current_obs = obs\n",
    "\n",
    "            self.state_type = state_type\n",
    "            self.crop, self.lane_tolerance, self.danger_threshold_x, self.danger_threshold_y, self.x_speed_coef, self.y_speed_coef = crop, lane_tolerance, danger_threshold_x, danger_threshold_y, x_speed_coef, y_speed_coef\n",
    "            self.initialize_states()\n",
    "\n",
    "    def initialize_states(self):\n",
    "        \"\"\"\n",
    "        Initialize the states of the occupancy grid\n",
    "        \"\"\"\n",
    "        # If the state type is danger, we need to initialize the states\n",
    "        if self.state_type == 'danger':\n",
    "            # The states will be the possible combinations of 0s and 1s for the 4 features + {-1,0,1} for the lane \n",
    "            a = list(itertools.product([0, 1], repeat=4))\n",
    "            # Now we need to add the product of {-1,0,1}\n",
    "            a = list(itertools.product(a, [-1, 0, 1]))\n",
    "            flattened = [(*x, y) for x, y in a]\n",
    "            self.states = flattened\n",
    "\n",
    "        elif self.state_type == 'binned': \n",
    "            pass\n",
    "\n",
    "    def get_state(self, decode=False):\n",
    "        \"\"\"\n",
    "        Get the state of the environment\n",
    "        Arguments:\n",
    "            type: str, the type of state to get. Options are 'n_neighbours' or 'danger' :\n",
    "                n_neighbours: the state is the n closest neighbours in the form (x1,x2,...,xn), (y1,y2,...,yn)\n",
    "                danger: the state is an array with 4 binary variables representing whether there is danger ahead, behind, on the left or right lanes\n",
    "            decode: bool, whether to decode the state\n",
    "        \"\"\"\n",
    "        assert self.state_type in ['n_neighbours', 'danger'], \"The type of state must be either 'n_neighbours' or 'lane-wise'\"\n",
    "        if self.state_type == 'n_neighbours':\n",
    "            state = self.state_n_neighbours()\n",
    "        elif self.state_type == 'danger':\n",
    "            state = self.state_danger()\n",
    "        return state\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------------------------------\n",
    "    def get_n_closest(self):\n",
    "        \"\"\"\n",
    "        Get the n closest cars to the agent\n",
    "        Returns:\n",
    "            closest_car_positions: np.array, the positions of the n closest cars to the agent. If there are less than n_closest cars, the array is padded with the crop_dist values\n",
    "        \"\"\"\n",
    "\n",
    "        car_positions = self.get_car_positions()\n",
    "        distances = np.linalg.norm(car_positions, axis=1)\n",
    "\n",
    "        # Remove the agent position\n",
    "        closest = np.argsort(distances)[1:self.n_closest+1]\n",
    "        closest_car_positions = car_positions[closest]\n",
    "\n",
    "        # If there are less than n_closest cars, pad the array with the crop_dist values\n",
    "        if len(closest_car_positions) < self.n_closest:\n",
    "            n_missing = self.n_closest - len(closest_car_positions)\n",
    "            closest_car_positions = np.pad(closest_car_positions, ((0, n_missing), (0,0)), 'constant', constant_values=(self.crop_dist[0][0], self.crop_dist[1][0]))\n",
    "\n",
    "        # Values that are \n",
    "        return closest_car_positions\n",
    "    \n",
    "\n",
    "    def state_n_neighbours(self):\n",
    "        \"\"\"\n",
    "        Get the state of the environment in a neighbour-wise manner\n",
    "        Returns:\n",
    "            state: tuple, the state of the environment in the form (x1,x2,...,xn), (y1,y2,...,yn)\n",
    "        \"\"\"\n",
    "        n_closest = self.get_n_closest()\n",
    "        # For the closest cars, get the state\n",
    "        state_x, state_y = [], []\n",
    "        # Get the bin values for each of the x,y positions, and return a tuple with the values\n",
    "        for car in n_closest:\n",
    "            x = np.digitize(car[0], self.x_bins) - 1\n",
    "            y = np.digitize(car[1], self.y_bins) - 1\n",
    "            x_val, y_val = self.x_bins[x], self.y_bins[y]\n",
    "            state_x.append(x_val)\n",
    "            state_y.append(y_val)\n",
    "        state = (tuple(state_x), tuple(state_y))\n",
    "        return tuple(state)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def state_danger(self):\n",
    "\n",
    "        global values_x\n",
    "        global values_y\n",
    "        global turn_possibility\n",
    "        \"\"\"\n",
    "        Get the state of the environment\n",
    "        \"\"\"\n",
    "        def get_sign(num): \n",
    "            sign = num/np.abs(num)\n",
    "            return sign\n",
    "\n",
    "        lane = self.current_obs[0,1]\n",
    "        observation = self.current_obs[1:][:,0:4]\n",
    "        observation = observation[~np.all(observation == 0, axis=1)]\n",
    "\n",
    "        # The bin 0 indicates (0, 8], which is the safety distance\n",
    "        # bins =  [[0,8,self.crop], [0,8,self.crop], [0,8,self.crop], [0,8,self.crop]] if bins is None else bins\n",
    "        # bins =  [[0,10], [0,10], [0,10], [0,10]] if bins is None else bins\n",
    "\n",
    "        # Lane observations\n",
    "        same_lane = observation[np.abs(observation[:,1]) <= self.lane_tolerance]\n",
    "        lane_front = same_lane[(same_lane[:,0] > 0)]\n",
    "        lane_back = same_lane[(same_lane[:,0] < 0)]\n",
    "\n",
    "        # For the left and right lanes we consider 2 lanes, instead of just one \n",
    "        left_lanes = observation[(observation[:,1] >= -8 - self.lane_tolerance) & (observation[:,1] <= -4 + self.lane_tolerance)]\n",
    "        right_lanes = observation[(observation[:,1] <= 8 + self.lane_tolerance) & (observation[:,1] >= 4 - self.lane_tolerance)]\n",
    "\n",
    "        # Calculating the adjusted distances\n",
    "        front_dist = lane_front[0,0] if len(lane_front) > 0 else self.crop \n",
    "        front_speed_diff = lane_front[0,2] if len(lane_front) > 0 else 0\n",
    "        front_adj_dist = front_dist + self.x_speed_coef*front_speed_diff   # The more the front speed diff, the harder it is to get to the car in the front, so adjusted distance is higher\n",
    "\n",
    "        back_dist = -lane_back[0,0] if len(lane_back) > 0 else self.crop\n",
    "        back_speed_diff = lane_back[0,2] - 5 if len(lane_back) > 0 else 0\n",
    "        back_adj_dist = back_dist - self.x_speed_coef*back_speed_diff    # The faster the car in the back is driving, the more dangerous it is, so the adjusted distance is lower\n",
    "\n",
    "        left_signs = [get_sign(left_lanes[i,0]) for i in range(len(left_lanes))]\n",
    "        left_adj_dists = (np.abs(left_lanes[:,1])-4)*1.5 + left_lanes[:,0] + self.x_speed_coef*left_lanes[:,2]*left_signs - self.y_speed_coef*left_lanes[:,3]\n",
    "        left_adj_dist = np.min(left_adj_dists) if len(left_adj_dists) > 0 else self.crop\n",
    "\n",
    "        right_signs = [get_sign(right_lanes[i,0]) for i in range(len(right_lanes))]\n",
    "        right_adj_dists = (np.abs(right_lanes[:,1])-4)*1.5 + right_lanes[:,0] + self.x_speed_coef*right_lanes[:,2]*right_signs + self.y_speed_coef*right_lanes[:,3]\n",
    "        right_adj_dist = np.min(right_adj_dists) if len(right_adj_dists) > 0 else self.crop\n",
    "\n",
    "        turn_possibility = -1 if lane < 2 else 1 if lane > 34 else 0\n",
    "        values = np.array([front_adj_dist, back_adj_dist, left_adj_dist, right_adj_dist])\n",
    "\n",
    "        # print('------------------/-----------------')\n",
    "        # print(right_lanes)\n",
    "        # print(right_adj_dists)\n",
    "        # print(values)\n",
    "\n",
    "        # Use the danger threshold to make 0 or 1 \n",
    "        values_x = np.where(values[:2] < self.danger_threshold_x, 1, 0)\n",
    "        values_y = np.where(values[2:] < self.danger_threshold_y, 1, 0)\n",
    "\n",
    "        values = np.append(values_x, values_y)\n",
    "        values = np.append(values, turn_possibility)\n",
    "        return tuple(values)\n",
    "    \n",
    "    @staticmethod\n",
    "    def bin_values(values, bins, digitize=False):\n",
    "        \"\"\"\n",
    "        Function to bin the values\n",
    "        Arguments:\n",
    "            values: np.array, the values to bin\n",
    "            bins: list, the bins to use\n",
    "                Example: bins = [[5,10,15,30], [5,10,30], [8,14,20], [8,14,20]], for x,y,vx,vy\n",
    "            digitize: bool, whether to digitize the values. If set to False, the values will be returned as the bin index\n",
    "        Returns:\n",
    "            binned_values: np.array, the binned values\n",
    "        \"\"\"\n",
    "        # Check if there are as much bins as values\n",
    "        if len(values.shape) == 1:\n",
    "            assert len(bins) == len(values), \"The number of bins must be equal to the number of values\"\n",
    "            binned_values = []\n",
    "            if digitize:\n",
    "                binned_values = [np.digitize(values[i], bins[i]) for i in range(len(bins))]\n",
    "                return np.array(binned_values)\n",
    "\n",
    "            binned_values = [bins[i][np.digitize(values[i], bins[i])-1] for i in range(len(bins))]\n",
    "            return np.array(binned_values)\n",
    "\n",
    "        # For a NxM matrix, with N>1\n",
    "        assert len(bins) == values.shape[1], \"The number of bins must be equal to the number of values\"\n",
    "        binned_values = []\n",
    "        for i in range(len(values)):\n",
    "            if digitize:\n",
    "                binned_values.append([np.digitize(values[i,j], bins[j]) for j in range(len(bins))])\n",
    "            else:\n",
    "                binned_values.append([bins[j][np.digitize(values[i,j], bins[j])-1] for j in range(len(bins))])\n",
    "        return np.array(binned_values)\n",
    "\n",
    "    \n",
    "    def test_env(self, sleep_time=0.2):\n",
    "        \"\"\"\n",
    "        Function to test the environment with a random policy, or with a policy\n",
    "        \"\"\" \n",
    "        obs, info = self.env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        while not done:\n",
    "            # start = time.time()\n",
    "            if self.policy is None:\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                action = self.policy()\n",
    "            obs, reward, done, truncate, info = self.env.step(action)\n",
    "            speed = obs[0,2]\n",
    "            reward = fix_reward(reward, obs[0], self.to_right_reward, self.to_right_skewness, self.change_lane_reward, self.current_obs[0])\n",
    "            self.current_obs = obs\n",
    "            print(self.get_state(), speed, decode_meta_action(action), reward)\n",
    "            time.sleep(sleep_time)\n",
    "            # end = time.time()\n",
    "            # print(f\"Time taken: {end-start}\")\n",
    "        self.env.close()\n",
    "\n",
    "        \n",
    "class Algorithm(Kinematics):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha=0.75,\n",
    "        gamma=0.95,\n",
    "        epsilon=0.6,\n",
    "        epsilon_decay=1, min_epsilon=0.05,\n",
    "        print_stats=False,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Algorithm class constructor\n",
    "        Arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            epsilon_decay: float, the decay value for epsilon. If set to 1, the epsilon will not decay\n",
    "            min_epsilon: float, the minimum value for epsilon. If the epsilon is lower than this value, it will not decay\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \n",
    "        Other Arguments (for the Kinematics observation):\n",
    "            seed: int, the seed to use in the test environment\n",
    "            state_type: str, the type of state to use. Options are 'n_neighbours' or 'danger'\n",
    "            policy: function, the policy to use in the simulation\n",
    "            crop: int, the crop distance to use in the state\n",
    "            lane_tolerance: int, the tolerance to use in the lane\n",
    "            danger_threshold_x: float, the threshold to use in the x direction for the danger state\n",
    "            danger_threshold_y: float, the threshold to use in the y direction for the danger state\n",
    "            x_speed_coef: float, the coefficient to use in the x direction for the danger state\n",
    "            y_speed_coef: float, the coefficient to use in the y direction for the danger state\n",
    "\n",
    "        Other Arguments (for Observation Type):\n",
    "            sim_frequency: int, the frequency of the simulation\n",
    "            policy_frequency: int, the frequency of the policy\n",
    "            render_mode: str, the mode to render the simulation\n",
    "            seed: int, the seed to use in the simulation\n",
    "            colision_reward: float, the reward to give when a colision occurs\n",
    "            high_speed_reward: float, the reward to give when driving at high speed\n",
    "            reward_speed_range: list, the range of speeds to give the high speed reward\n",
    "            to_right_reward: float, the reward to give when driving to the right, mapped polynomially with to_right_skewness\n",
    "            to_right_skewness: float, the skewness to apply to the to_right_reward\n",
    "            change_lane_reward: float, the reward to give when changing lanes\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.initialize_Q(print_stats)\n",
    "        self.alpha, self.gamma, self.epsilon = alpha, gamma, epsilon\n",
    "        self.epsilon_decay, self.min_epsilon = epsilon_decay, min_epsilon\n",
    "        self.Q_stats = self.Q.copy()\n",
    "        self.rewards_hist = []\n",
    "\n",
    "    def policy_Q(self, state):\n",
    "        values = [self.Q[(state, action)] for action in range(5)]\n",
    "        return np.argmax(values)   \n",
    "    \n",
    "    def initialize_Q(self, print_stats = False):\n",
    "        # Combine the possible states with the possible actions\n",
    "        keys = list(itertools.product(self.states, range(5)))       # 5 possible actions, 0-4: left, idle, right, accelerate, decelerate\n",
    "        if len(keys) > 150000:\n",
    "            print(\"Warning: The number of states is too large, consider reducing the number of states\")\n",
    "        if print_stats:\n",
    "            print(f\"Number of states: {len(keys)}\")   \n",
    "        self.Q = {key: 0 for key in keys}\n",
    "\n",
    "    def epsilon_greedy(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(5)\n",
    "        else:\n",
    "            values = [self.Q[(state, action)] for action in range(5)]\n",
    "            return np.argmax(values)\n",
    "\n",
    "    def decay_epsilon(self, episode):\n",
    "        if self.epsilon > self.min_epsilon:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def test(self, sleep_time=1, time_after_crash=10):\n",
    "        with gym.make('highway-v0', render_mode='human', config=self.config) as env:\n",
    "            obs, info = env.reset(seed = self.seed)\n",
    "            self.current_obs = obs\n",
    "            done = False\n",
    "            state = self.get_state()\n",
    "            action = self.policy_Q(state)\n",
    "            while not done:\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                next_state = self.get_state()\n",
    "                next_action = self.policy_Q(next_state)\n",
    "                state, action = next_state, next_action\n",
    "                reward = fix_reward(reward, next_obs[0], self.to_right_reward, self.to_right_skewness, self.change_lane_reward, self.current_obs[0])\n",
    "                self.current_obs = next_obs\n",
    "                print(state, decode_meta_action(action), reward, next_state)\n",
    "                past_positon = obs[0]\n",
    "                time.sleep(sleep_time)\n",
    "            time.sleep(time_after_crash)\n",
    "\n",
    "    def get_state_visits(self, state=None):\n",
    "        state_visits = {state: np.sum([self.Q_stats[(state, action)] for action in range(5)]) for state in self.states}\n",
    "        state_visits = {k:100*v/np.sum(list(state_visits.values())) for k, v in sorted(state_visits.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return state_visits if state is None else state_visits[state]\n",
    "\n",
    "    def plot_rewards_history(self, smoothing=10):\n",
    "        plt.plot(self.rewards_hist, label='Original')\n",
    "        plt.plot(np.convolve(self.rewards_hist, np.ones(smoothing)/smoothing, mode='valid'), label='Smoothed')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Cumulative reward')\n",
    "        plt.title('Cumulative reward per episode')\n",
    "        plt.show()\n",
    "\n",
    "    def search_Q(self, state):\n",
    "        assert len(state) == 5, \"The state must be a tuple of 5 values\"\n",
    "        Q_vals = {decode_meta_action(a) : self.Q[state, a] for a in range(5)}\n",
    "        # Order a \n",
    "        Q_vals = {k: v for k, v in sorted(Q_vals.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return Q_vals\n",
    "    \n",
    "\n",
    "class Sarsa(Algorithm):\n",
    "    def __init__(\n",
    "        self,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        SARSA class constructor\n",
    "        Algorithm arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            m: int, the number of episodes to train the agent for\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def train(self, m=100, verbose=0): \n",
    "        env = gym.make('highway-v0', render_mode=None, config=self.config)\n",
    "        obs, info = env.reset(seed = np.random.randint(1000))\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        state = self.get_state()\n",
    "        action = self.epsilon_greedy(state)\n",
    "        q_explored = np.count_nonzero(list(self.Q.values()))\n",
    "        for i in tqdm(range(m)):\n",
    "            last_state = None\n",
    "            env.reset(seed = np.random.randint(1000))\n",
    "            cum_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                reward = fix_reward(reward, next_obs[0], self.to_right_reward, self.to_right_skewness, self.change_lane_reward, self.current_obs[0])\n",
    "                next_state = self.get_state()\n",
    "                next_action = self.epsilon_greedy(next_state)\n",
    "                if done:\n",
    "                    last_state = state\n",
    "\n",
    "                print(next_state, decode_meta_action(next_action), reward) if verbose > 2 else None\n",
    "                if self.Q[(state, action)] == 0:\n",
    "                    q_explored += 1\n",
    "                cum_reward += reward\n",
    "\n",
    "                self.Q[(state, action)] += self.alpha*(reward + self.gamma*self.Q[(next_state, next_action)] - self.Q[(state, action)])\n",
    "                self.Q_stats[(state, action)] += 1\n",
    "                state, action = next_state, next_action\n",
    "                self.current_obs = next_obs\n",
    "            self.rewards_hist.append(cum_reward)\n",
    "            print(f\"Episode {i+1} completed on state {last_state} with cumulative reward: {cum_reward}\") if verbose > 0 else None\n",
    "            print(f\"Q explored: {100*q_explored/len(self.Q)}. Epsilon: {self.epsilon}\") if verbose > 1 else None\n",
    "            self.decay_epsilon(i)\n",
    "        env.close()\n",
    "    \n",
    "\n",
    "class Q_learning(Algorithm):\n",
    "    def __init__(\n",
    "        self,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Q-learning class constructor\n",
    "        Arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            m: int, the number of episodes to train the agent for\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def train(self, m=100, verbose=0):\n",
    "        env = gym.make('highway-v0', render_mode=None, config=self.config)\n",
    "        obs, info = env.reset(seed = np.random.randint(1000))\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        state = self.get_state()\n",
    "        q_explored = np.count_nonzero(list(self.Q.values()))\n",
    "        for i in tqdm(range(m)):\n",
    "            last_state = None\n",
    "            env.reset(seed = np.random.randint(1000))\n",
    "            cum_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.epsilon_greedy(state)\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                reward = fix_reward(reward, next_obs[0], self.to_right_reward, self.to_right_skewness, self.change_lane_reward, self.current_obs[0])\n",
    "                next_state = self.get_state()\n",
    "                if done:\n",
    "                    last_state = state\n",
    "\n",
    "                print(next_state, decode_meta_action(action), reward) if verbose > 2 else None\n",
    "                if self.Q[(state, action)] == 0:\n",
    "                    q_explored += 1\n",
    "                cum_reward += reward\n",
    "\n",
    "                self.Q[(state, action)] += self.alpha*(reward + self.gamma*np.max([self.Q[(next_state, a)] for a in range(5)]) - self.Q[(state, action)])\n",
    "                self.Q_stats[(state, action)] += 1\n",
    "                state = next_state\n",
    "                self.current_obs = next_obs\n",
    "            self.rewards_hist.append(cum_reward)\n",
    "            print(f\"Episode {i+1} completed on state {last_state} with cumulative reward: {cum_reward}\") if verbose > 0 else None\n",
    "            print(f\"Q explored: {100*q_explored/len(self.Q)}. Epsilon: {self.epsilon}\") if verbose > 1 else None\n",
    "            self.decay_epsilon(i)\n",
    "        env.close()\n",
    "\n",
    "\n",
    "class MonteCarlo(Algorithm):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"Constructor for the MonteCarlo algorithm class.\"\"\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.returns = {}  # Store returns for each (state, action) pair\n",
    "        self.visits = {}   # Store visit counts for each (state, action) pair\n",
    "\n",
    "    def train(self, m=100, verbose=0):\n",
    "        \"\"\"Train the MonteCarlo agent.\"\"\"\n",
    "        env = gym.make('highway-v0', render_mode=None, config=self.config)\n",
    "\n",
    "        for i in tqdm(range(m)):\n",
    "            episode_history = []\n",
    "            cum_reward = 0\n",
    "\n",
    "            obs, info = env.reset(seed=np.random.randint(1000))\n",
    "            self.current_obs = obs\n",
    "            done = False\n",
    "            state = self.get_state()\n",
    "\n",
    "            while not done:\n",
    "                action = self.epsilon_greedy(state) \n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                reward = fix_reward(reward, next_obs[0], self.to_right_reward, self.to_right_skewness, self.change_lane_reward, self.current_obs[0])\n",
    "                next_state = self.get_state()\n",
    "\n",
    "                episode_history.append((state, action, reward))\n",
    "\n",
    "                cum_reward += reward\n",
    "                state = next_state\n",
    "                self.current_obs = next_obs\n",
    "\n",
    "            G = 0\n",
    "            for state, action, reward in reversed(episode_history):\n",
    "                G = self.gamma * G + reward \n",
    "                if (state, action) not in [(s, a) for s, a, _ in episode_history[:-1]]: # First visit\n",
    "                    self.returns[(state, action)] = self.returns.get((state, action), 0) + G\n",
    "                    self.visits[(state, action)] = self.visits.get((state, action), 0) + 1\n",
    "                    self.Q[(state, action)] = self.returns[(state, action)] / self.visits[(state, action)]\n",
    "\n",
    "            self.rewards_hist.append(cum_reward)\n",
    "            print(f\"Episode {i+1} completed with cumulative reward: {cum_reward}. Epsilon: {self.epsilon}\") if verbose > 0 else None\n",
    "\n",
    "            self.decay_epsilon(i)\n",
    "            \n",
    "        env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0, 0) 27.990612 FASTER 2.288888888888889\n",
      "(0, 0, 0, 0, 0) 29.192472 FASTER 2.288888888888889\n",
      "(0, 0, 0, 0, 0) 29.358517 LANE_RIGHT 1.7225025588675635\n",
      "(0, 0, 1, 1, 0) 29.738987 LANE_LEFT 2.540472497001288\n",
      "(0, 0, 1, 1, 0) 29.443434 LANE_LEFT 0.9603842224861536\n",
      "(0, 0, 1, 1, 0) 26.927794 SLOWER 0.6989649071890149\n",
      "(0, 0, 1, 0, 0) 28.784813 FASTER 1.6231962503087107\n",
      "(0, 0, 1, 1, 0) 29.176489 LANE_LEFT 0.2729496452499006\n",
      "(0, 0, 1, 1, 0) 26.770985 SLOWER 1.099728192418722\n",
      "(0, 0, 1, 1, 0) 25.725262 IDLE 1.0499388491563615\n",
      "(0, 0, 1, 1, 0) 22.302061 SLOWER 1.0369852523316212\n",
      "(0, 0, 1, 0, 0) 20.519234 LANE_LEFT -0.23326629717852831\n",
      "(0, 0, 1, 1, 0) 23.303974 FASTER 0.6409017282437028\n",
      "(0, 0, 1, 1, 0) 24.04882 LANE_RIGHT -0.15751081166132552\n",
      "(0, 0, 1, 1, 0) 21.701408 SLOWER 0.9693038142007432\n",
      "(0, 0, 1, 1, 0) 20.381327 LANE_LEFT -0.25229204829306406\n",
      "(0, 0, 1, 1, 0) 20.230286 SLOWER 0.6390716436155569\n",
      "(0, 0, 1, 0, 0) 19.593742 LANE_LEFT -0.5995419937699334\n",
      "(0, 0, 1, 0, 0) 19.972246 SLOWER 0.30873053694653607\n",
      "(0, 0, 1, 0, 0) 20.010876 SLOWER 0.2810002515681126\n",
      "(0, 0, 1, 0, 0) 19.552628 LANE_LEFT -0.8595449483867643\n",
      "(0, 0, 1, 0, 0) 19.851152 LANE_RIGHT 0.20312035561538708\n",
      "(0, 0, 0, 0, 0) 19.973385 SLOWER 0.24574058551406117\n",
      "(0, 0, 1, 0, 0) 19.6497 LANE_LEFT -0.867381479596962\n",
      "(0, 0, 1, 0, 0) 19.217863 LANE_LEFT -0.9802902170414503\n",
      "(0, 0, 0, 0, -1) 19.896513 IDLE -0.9979556816932554\n",
      "(0, 0, 0, 0, -1) 19.989723 IDLE 0.00019242573622481725\n",
      "(0, 0, 0, 0, -1) 19.999052 LANE_LEFT 1.7677923040642615e-05\n",
      "(0, 0, 0, 0, 0) 19.59295 LANE_RIGHT -0.9686888163981697\n",
      "(0, 0, 0, 0, 0) 22.934776 FASTER 0.05772829163943349\n",
      "(0, 0, 1, 0, 0) 27.17933 FASTER 0.06906541836554225\n",
      "(0, 0, 1, 0, 0) 28.525585 LANE_RIGHT -0.8300501607867153\n",
      "(0, 0, 0, 1, 0) 28.957653 LANE_RIGHT -0.6042044865702944\n",
      "(0, 0, 0, 1, 0) 29.156208 LANE_RIGHT -0.24568668712227792\n",
      "(0, 0, 1, 1, 0) 29.85893 IDLE -0.045620668690278365\n",
      "(0, 0, 1, 1, 0) 26.975552 SLOWER 1.013138820090409\n",
      "(0, 0, 1, 1, 0) 28.786009 FASTER 1.0278036691721355\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[239], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m kin \u001b[38;5;241m=\u001b[39m Kinematics(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[0;32m      2\u001b[0m                 state_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdanger\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      3\u001b[0m                 render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m                 reward_speed_range\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m],\n\u001b[0;32m      9\u001b[0m                 to_right_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, to_right_skewness\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, change_lane_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mkin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[229], line 280\u001b[0m, in \u001b[0;36mKinematics.test_env\u001b[1;34m(self, sleep_time)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy()\n\u001b[1;32m--> 280\u001b[0m obs, reward, done, truncate, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m speed \u001b[38;5;241m=\u001b[39m obs[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    282\u001b[0m reward \u001b[38;5;241m=\u001b[39m fix_reward(reward, obs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_right_reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_right_skewness, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchange_lane_reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_obs[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:257\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_control\u001b[39m\u001b[38;5;124m\"\u001b[39m] \\\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:324\u001b[0m, in \u001b[0;36mRoad.act\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decide the actions of each entity on the road.\"\"\"\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[1;32m--> 324\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:100\u001b[0m, in \u001b[0;36mIDMVehicle.act\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     97\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Longitudinal: IDM\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m front_vehicle, rear_vehicle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneighbour_vehicles\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlane_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macceleration(ego_vehicle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    102\u001b[0m                                            front_vehicle\u001b[38;5;241m=\u001b[39mfront_vehicle,\n\u001b[0;32m    103\u001b[0m                                            rear_vehicle\u001b[38;5;241m=\u001b[39mrear_vehicle)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# When changing lane, check both current and target lanes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:361\u001b[0m, in \u001b[0;36mRoad.neighbour_vehicles\u001b[1;34m(self, vehicle, lane_index)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vehicle \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, Landmark):  \u001b[38;5;66;03m# self.network.is_connected_road(v.lane_index,\u001b[39;00m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;66;03m# lane_index, same_lane=True):\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m         s_v, lat_v \u001b[38;5;241m=\u001b[39m \u001b[43mlane\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lane\u001b[38;5;241m.\u001b[39mon_lane(v\u001b[38;5;241m.\u001b[39mposition, s_v, lat_v, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    363\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:190\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    188\u001b[0m delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m    189\u001b[0m longitudinal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection)\n\u001b[1;32m--> 190\u001b[0m lateral \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kin = Kinematics(seed=10, \n",
    "                state_type='danger', \n",
    "                render_mode='human', \n",
    "                sim_frequency=10, \n",
    "                policy_frequency=2, \n",
    "                colision_reward=-50,\n",
    "                high_speed_reward=0,\n",
    "                reward_speed_range=[20, 30],\n",
    "                to_right_reward=5, to_right_skewness=2, change_lane_reward=-1)\n",
    "\n",
    "kin.test_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 240\n"
     ]
    }
   ],
   "source": [
    "Q = Q_learning(print_stats=True, \n",
    "        epsilon=0.9, \n",
    "        epsilon_decay=0.98,\n",
    "        min_epsilon=0.05,\n",
    "        alpha=0.1, \n",
    "        gamma=0.98, \n",
    "        state_type='danger', \n",
    "        policy_frequency=2, \n",
    "        sim_frequency=10, \n",
    "        danger_threshold_x=15,\n",
    "        danger_threshold_y=10,\n",
    "        x_speed_coef=1.2,\n",
    "        colision_reward=-75,\n",
    "        high_speed_reward=5,\n",
    "        reward_speed_range=[20, 30],\n",
    "        to_right_reward=2.5, to_right_skewness=2, change_lane_reward=-0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62953cfb5bb24b2cb499bb2e3925c252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 completed on state (1, 0, 0, 0, -1) with cumulative reward: -49.607051029735224\n",
      "Q explored: 2.0833333333333335. Epsilon: 0.9\n",
      "Episode 2 completed on state (1, 0, 1, 0, 1) with cumulative reward: 22.36718277851223\n",
      "Q explored: 10.0. Epsilon: 0.882\n",
      "Episode 3 completed on state (0, 0, 1, 0, 0) with cumulative reward: -36.05597368218276\n",
      "Q explored: 12.083333333333334. Epsilon: 0.86436\n",
      "Episode 4 completed on state (0, 0, 0, 1, -1) with cumulative reward: 51.54595782795823\n",
      "Q explored: 16.25. Epsilon: 0.8470728\n",
      "Episode 5 completed on state (0, 0, 0, 1, -1) with cumulative reward: -69.40886233098973\n",
      "Q explored: 17.083333333333332. Epsilon: 0.830131344\n",
      "Episode 6 completed on state (1, 0, 1, 0, 0) with cumulative reward: -2.5752109454186893\n",
      "Q explored: 19.166666666666668. Epsilon: 0.81352871712\n",
      "Episode 7 completed on state (0, 0, 1, 0, 0) with cumulative reward: 8.65872055216667\n",
      "Q explored: 20.833333333333332. Epsilon: 0.7972581427776\n",
      "Episode 8 completed on state (0, 0, 0, 1, 0) with cumulative reward: -16.631636364782523\n",
      "Q explored: 20.833333333333332. Epsilon: 0.781312979922048\n",
      "Episode 9 completed on state (1, 0, 1, 1, 0) with cumulative reward: -25.841667321297066\n",
      "Q explored: 22.083333333333332. Epsilon: 0.765686720323607\n",
      "Episode 10 completed on state (0, 0, 1, 0, 0) with cumulative reward: -5.385592428375844\n",
      "Q explored: 23.333333333333332. Epsilon: 0.7503729859171349\n",
      "Episode 11 completed on state (0, 0, 1, 1, 0) with cumulative reward: 89.08609549267535\n",
      "Q explored: 25.416666666666668. Epsilon: 0.7353655261987921\n",
      "Episode 12 completed on state (0, 0, 0, 1, 0) with cumulative reward: -67.67087432335335\n",
      "Q explored: 25.833333333333332. Epsilon: 0.7206582156748162\n",
      "Episode 13 completed on state (0, 0, 0, 1, 0) with cumulative reward: -66.66193216536439\n",
      "Q explored: 25.833333333333332. Epsilon: 0.7062450513613199\n",
      "Episode 14 completed on state (0, 0, 1, 0, 0) with cumulative reward: 49.06022954147865\n",
      "Q explored: 25.833333333333332. Epsilon: 0.6921201503340935\n",
      "Episode 15 completed on state (0, 0, 1, 0, 1) with cumulative reward: 325.68681533696656\n",
      "Q explored: 27.5. Epsilon: 0.6782777473274115\n",
      "Episode 16 completed on state (1, 0, 0, 1, 0) with cumulative reward: 3.865523105358818\n",
      "Q explored: 27.5. Epsilon: 0.6647121923808633\n",
      "Episode 17 completed on state (1, 0, 0, 0, -1) with cumulative reward: -23.397308071918495\n",
      "Q explored: 27.916666666666668. Epsilon: 0.6514179485332461\n",
      "Episode 18 completed on state (0, 0, 0, 1, 0) with cumulative reward: -13.582948976173675\n",
      "Q explored: 27.916666666666668. Epsilon: 0.6383895895625812\n",
      "Episode 19 completed on state (0, 0, 0, 1, -1) with cumulative reward: 146.9496052697583\n",
      "Q explored: 28.333333333333332. Epsilon: 0.6256217977713295\n",
      "Episode 20 completed on state (1, 0, 0, 0, 0) with cumulative reward: 33.20106407589904\n",
      "Q explored: 28.333333333333332. Epsilon: 0.6131093618159029\n",
      "Episode 21 completed on state (0, 0, 1, 0, 1) with cumulative reward: 14.329921409179164\n",
      "Q explored: 28.333333333333332. Epsilon: 0.6008471745795848\n",
      "Episode 22 completed on state (0, 0, 1, 1, 0) with cumulative reward: -47.85233202716131\n",
      "Q explored: 28.333333333333332. Epsilon: 0.5888302310879932\n",
      "Episode 23 completed on state (1, 0, 0, 1, 0) with cumulative reward: -55.68124983686027\n",
      "Q explored: 28.333333333333332. Epsilon: 0.5770536264662333\n",
      "Episode 24 completed on state (1, 0, 0, 1, 0) with cumulative reward: 123.31787675053758\n",
      "Q explored: 29.583333333333332. Epsilon: 0.5655125539369086\n",
      "Episode 25 completed on state (0, 0, 0, 1, 0) with cumulative reward: 12.450855795456178\n",
      "Q explored: 29.583333333333332. Epsilon: 0.5542023028581704\n",
      "Episode 26 completed on state (0, 0, 1, 0, 0) with cumulative reward: -48.24473760840277\n",
      "Q explored: 29.583333333333332. Epsilon: 0.543118256801007\n",
      "Episode 27 completed on state (1, 0, 0, 0, -1) with cumulative reward: 26.570256255693522\n",
      "Q explored: 29.583333333333332. Epsilon: 0.5322558916649869\n",
      "Episode 28 completed on state (0, 0, 1, 1, 0) with cumulative reward: -49.27289193985333\n",
      "Q explored: 29.583333333333332. Epsilon: 0.5216107738316871\n",
      "Episode 29 completed on state (0, 0, 0, 1, 0) with cumulative reward: 1.4512951268742142\n",
      "Q explored: 29.583333333333332. Epsilon: 0.5111785583550533\n",
      "Episode 30 completed on state (0, 0, 0, 1, 0) with cumulative reward: 33.19127455428395\n",
      "Q explored: 29.583333333333332. Epsilon: 0.5009549871879523\n",
      "Episode 31 completed on state (1, 0, 0, 0, 1) with cumulative reward: 55.55276631683368\n",
      "Q explored: 31.25. Epsilon: 0.49093588744419325\n",
      "Episode 32 completed on state (1, 0, 0, 0, 0) with cumulative reward: 32.52033348526629\n",
      "Q explored: 31.25. Epsilon: 0.4811171696953094\n",
      "Episode 33 completed on state (0, 0, 0, 1, -1) with cumulative reward: -34.88545748192752\n",
      "Q explored: 31.25. Epsilon: 0.47149482630140316\n",
      "Episode 34 completed on state (1, 0, 1, 0, 0) with cumulative reward: -36.668663629475695\n",
      "Q explored: 31.25. Epsilon: 0.4620649297753751\n",
      "Episode 35 completed on state (0, 0, 1, 0, 0) with cumulative reward: -61.77153914869956\n",
      "Q explored: 31.25. Epsilon: 0.4528236311798676\n",
      "Episode 36 completed on state (1, 0, 0, 0, 0) with cumulative reward: -7.082297716862172\n",
      "Q explored: 31.25. Epsilon: 0.44376715855627025\n",
      "Episode 37 completed on state (0, 0, 1, 1, 0) with cumulative reward: 26.36546753347595\n",
      "Q explored: 31.25. Epsilon: 0.4348918153851448\n",
      "Episode 38 completed on state (1, 0, 0, 0, 1) with cumulative reward: 27.07041329157846\n",
      "Q explored: 31.25. Epsilon: 0.42619397907744194\n",
      "Episode 39 completed on state (0, 0, 1, 0, 1) with cumulative reward: -47.1867704293984\n",
      "Q explored: 31.25. Epsilon: 0.4176700994958931\n",
      "Episode 40 completed on state (1, 0, 0, 0, 0) with cumulative reward: -73.62030215963019\n",
      "Q explored: 31.25. Epsilon: 0.40931669750597527\n",
      "Episode 41 completed on state (0, 0, 1, 1, 0) with cumulative reward: -23.85050128805303\n",
      "Q explored: 31.25. Epsilon: 0.40113036355585574\n",
      "Episode 42 completed on state (0, 0, 1, 1, 0) with cumulative reward: 170.3946107853919\n",
      "Q explored: 31.25. Epsilon: 0.3931077562847386\n",
      "Episode 43 completed on state (1, 0, 0, 0, -1) with cumulative reward: 117.96898952500948\n",
      "Q explored: 31.25. Epsilon: 0.3852456011590438\n",
      "Episode 44 completed on state (1, 0, 0, 0, -1) with cumulative reward: 27.26270681454436\n",
      "Q explored: 31.666666666666668. Epsilon: 0.3775406891358629\n",
      "Episode 45 completed on state (1, 0, 0, 1, 0) with cumulative reward: -14.288179294110975\n",
      "Q explored: 31.666666666666668. Epsilon: 0.36998987535314565\n",
      "Episode 46 completed on state (0, 0, 1, 0, 0) with cumulative reward: 8.858947506213482\n",
      "Q explored: 31.666666666666668. Epsilon: 0.3625900778460827\n",
      "Episode 47 completed on state (1, 0, 0, 1, -1) with cumulative reward: 52.453846912870176\n",
      "Q explored: 31.666666666666668. Epsilon: 0.35533827628916104\n",
      "Episode 48 completed on state (1, 0, 0, 1, 0) with cumulative reward: 29.007098810524667\n",
      "Q explored: 32.083333333333336. Epsilon: 0.3482315107633778\n",
      "Episode 49 completed on state (1, 0, 0, 0, 0) with cumulative reward: 41.01524077412873\n",
      "Q explored: 32.083333333333336. Epsilon: 0.3412668805481103\n",
      "Episode 50 completed on state (0, 0, 1, 0, 0) with cumulative reward: 87.44395166071627\n",
      "Q explored: 32.5. Epsilon: 0.33444154293714806\n",
      "Episode 51 completed on state (0, 0, 1, 0, 0) with cumulative reward: 3.8567920556514252\n",
      "Q explored: 32.5. Epsilon: 0.3277527120784051\n",
      "Episode 52 completed on state (1, 0, 0, 0, 1) with cumulative reward: -21.540746996267266\n",
      "Q explored: 32.5. Epsilon: 0.321197657836837\n",
      "Episode 53 completed on state (1, 0, 0, 0, 0) with cumulative reward: -15.39992066751686\n",
      "Q explored: 32.5. Epsilon: 0.31477370468010024\n",
      "Episode 54 completed on state (0, 0, 1, 0, 0) with cumulative reward: -26.890717349995803\n",
      "Q explored: 32.5. Epsilon: 0.3084782305864982\n",
      "Episode 55 completed on state (0, 0, 1, 0, 0) with cumulative reward: 0.07622513120610108\n",
      "Q explored: 32.5. Epsilon: 0.30230866597476824\n",
      "Episode 56 completed on state (1, 0, 1, 0, 1) with cumulative reward: 114.62491270514755\n",
      "Q explored: 32.916666666666664. Epsilon: 0.2962624926552729\n",
      "Episode 57 completed on state (1, 0, 1, 0, 0) with cumulative reward: 119.77852548036424\n",
      "Q explored: 32.916666666666664. Epsilon: 0.29033724280216744\n",
      "Episode 58 completed on state (0, 0, 0, 1, 0) with cumulative reward: -34.28881097475872\n",
      "Q explored: 32.916666666666664. Epsilon: 0.28453049794612406\n",
      "Episode 59 completed on state (1, 0, 1, 0, 1) with cumulative reward: 101.25108128374352\n",
      "Q explored: 32.916666666666664. Epsilon: 0.27883988798720155\n",
      "Episode 60 completed on state (1, 0, 0, 0, 1) with cumulative reward: 50.015830404259816\n",
      "Q explored: 33.75. Epsilon: 0.27326309022745754\n",
      "Episode 61 completed on state (1, 0, 1, 1, 0) with cumulative reward: 52.37534049394668\n",
      "Q explored: 34.166666666666664. Epsilon: 0.2677978284229084\n",
      "Episode 62 completed on state (1, 0, 0, 0, 0) with cumulative reward: 882.3129966160807\n",
      "Q explored: 34.166666666666664. Epsilon: 0.2624418718544502\n",
      "Episode 63 completed on state (1, 0, 1, 0, 1) with cumulative reward: 174.90068342667627\n",
      "Q explored: 34.166666666666664. Epsilon: 0.25719303441736124\n",
      "Episode 64 completed on state (0, 0, 0, 1, 0) with cumulative reward: 2.5911206579564663\n",
      "Q explored: 34.166666666666664. Epsilon: 0.25204917372901403\n",
      "Episode 65 completed on state (1, 0, 1, 1, 0) with cumulative reward: 74.33167350038893\n",
      "Q explored: 34.166666666666664. Epsilon: 0.24700819025443374\n",
      "Episode 66 completed on state (0, 0, 0, 1, 0) with cumulative reward: 8.002059208449523\n",
      "Q explored: 34.583333333333336. Epsilon: 0.24206802644934505\n",
      "Episode 67 completed on state (0, 0, 0, 1, 0) with cumulative reward: 4.3181416467782725\n",
      "Q explored: 34.583333333333336. Epsilon: 0.23722666592035815\n",
      "Episode 68 completed on state (0, 0, 1, 0, 1) with cumulative reward: 266.70662716323875\n",
      "Q explored: 34.583333333333336. Epsilon: 0.232482132601951\n",
      "Episode 69 completed on state (0, 0, 1, 1, 0) with cumulative reward: 32.821433629364776\n",
      "Q explored: 34.583333333333336. Epsilon: 0.22783248994991198\n",
      "Episode 70 completed on state (0, 0, 0, 1, 0) with cumulative reward: -44.83012748065828\n",
      "Q explored: 34.583333333333336. Epsilon: 0.22327584015091373\n",
      "Episode 71 completed on state (0, 0, 1, 1, 0) with cumulative reward: -44.56124361268429\n",
      "Q explored: 34.583333333333336. Epsilon: 0.21881032334789546\n",
      "Episode 72 completed on state (1, 0, 0, 1, -1) with cumulative reward: -39.059021471423286\n",
      "Q explored: 34.583333333333336. Epsilon: 0.21443411688093755\n",
      "Episode 73 completed on state (0, 0, 1, 1, 0) with cumulative reward: -33.55382347814589\n",
      "Q explored: 35.0. Epsilon: 0.2101454345433188\n",
      "Episode 74 completed on state (0, 0, 0, 1, 0) with cumulative reward: 339.19956265934\n",
      "Q explored: 35.0. Epsilon: 0.20594252585245243\n",
      "Episode 75 completed on state (0, 0, 1, 1, 0) with cumulative reward: -57.517424505736685\n",
      "Q explored: 35.0. Epsilon: 0.20182367533540338\n",
      "Episode 76 completed on state (1, 0, 0, 1, 0) with cumulative reward: -44.21979808707761\n",
      "Q explored: 35.0. Epsilon: 0.1977872018286953\n",
      "Episode 77 completed on state (1, 0, 1, 0, 0) with cumulative reward: -9.352075215626527\n",
      "Q explored: 35.0. Epsilon: 0.19383145779212138\n",
      "Episode 78 completed on state (1, 0, 1, 1, 0) with cumulative reward: 36.08918222011097\n",
      "Q explored: 35.0. Epsilon: 0.18995482863627894\n",
      "Episode 79 completed on state (0, 0, 1, 1, 0) with cumulative reward: 189.28920074584613\n",
      "Q explored: 35.0. Epsilon: 0.18615573206355335\n",
      "Episode 80 completed on state (1, 0, 0, 1, -1) with cumulative reward: 155.57025370887985\n",
      "Q explored: 35.0. Epsilon: 0.18243261742228228\n",
      "Episode 81 completed on state (0, 0, 0, 1, 0) with cumulative reward: -70.07569052620308\n",
      "Q explored: 35.416666666666664. Epsilon: 0.17878396507383665\n",
      "Episode 82 completed on state (1, 0, 0, 1, 0) with cumulative reward: 251.9125990933018\n",
      "Q explored: 35.416666666666664. Epsilon: 0.17520828577235992\n",
      "Episode 83 completed on state (0, 0, 1, 0, 1) with cumulative reward: -62.39294848253739\n",
      "Q explored: 35.416666666666664. Epsilon: 0.1717041200569127\n",
      "Episode 84 completed on state (0, 0, 0, 1, 0) with cumulative reward: 119.35526404692693\n",
      "Q explored: 35.416666666666664. Epsilon: 0.16827003765577445\n",
      "Episode 85 completed on state (1, 0, 0, 1, 0) with cumulative reward: 541.0996301671868\n",
      "Q explored: 35.416666666666664. Epsilon: 0.16490463690265894\n",
      "Episode 86 completed on state (0, 0, 1, 1, 0) with cumulative reward: -13.534977574946986\n",
      "Q explored: 35.416666666666664. Epsilon: 0.16160654416460576\n",
      "Episode 87 completed on state (0, 0, 0, 1, 0) with cumulative reward: -65.15370823413393\n",
      "Q explored: 35.416666666666664. Epsilon: 0.15837441328131366\n",
      "Episode 88 completed on state (1, 0, 1, 0, 0) with cumulative reward: 80.35193385546062\n",
      "Q explored: 35.416666666666664. Epsilon: 0.1552069250156874\n",
      "Episode 89 completed on state (1, 0, 1, 0, 0) with cumulative reward: 67.98490292437297\n",
      "Q explored: 35.416666666666664. Epsilon: 0.15210278651537365\n",
      "Episode 90 completed on state (1, 0, 0, 1, 0) with cumulative reward: 72.21737488755754\n",
      "Q explored: 35.416666666666664. Epsilon: 0.14906073078506618\n",
      "Episode 91 completed on state (0, 0, 0, 1, 0) with cumulative reward: 61.276702051808286\n",
      "Q explored: 35.416666666666664. Epsilon: 0.14607951616936485\n",
      "Episode 92 completed on state (0, 0, 0, 1, 0) with cumulative reward: -3.6817758746013425\n",
      "Q explored: 35.416666666666664. Epsilon: 0.14315792584597756\n",
      "Episode 93 completed on state (0, 0, 1, 1, 0) with cumulative reward: -35.79993641988603\n",
      "Q explored: 35.416666666666664. Epsilon: 0.140294767329058\n",
      "Episode 94 completed on state (0, 0, 0, 1, 0) with cumulative reward: 37.2461461456806\n",
      "Q explored: 35.416666666666664. Epsilon: 0.13748887198247683\n",
      "Episode 95 completed on state (1, 0, 1, 1, 0) with cumulative reward: 55.082712388837976\n",
      "Q explored: 35.416666666666664. Epsilon: 0.1347390945428273\n",
      "Episode 96 completed on state (0, 0, 0, 1, -1) with cumulative reward: -39.855843942277914\n",
      "Q explored: 35.416666666666664. Epsilon: 0.13204431265197075\n",
      "Episode 97 completed on state (1, 0, 0, 1, 0) with cumulative reward: 283.153822421212\n",
      "Q explored: 35.416666666666664. Epsilon: 0.12940342639893135\n",
      "Episode 98 completed on state (1, 0, 1, 0, 1) with cumulative reward: 147.07723014007414\n",
      "Q explored: 35.416666666666664. Epsilon: 0.12681535787095272\n",
      "Episode 99 completed on state (1, 0, 0, 1, 0) with cumulative reward: 257.5249280272272\n",
      "Q explored: 35.416666666666664. Epsilon: 0.12427905071353366\n",
      "Episode 100 completed on state (0, 0, 1, 0, 1) with cumulative reward: 174.39096022165285\n",
      "Q explored: 35.416666666666664. Epsilon: 0.121793469699263\n",
      "Episode 101 completed on state (0, 0, 1, 0, 0) with cumulative reward: 44.15879677277549\n",
      "Q explored: 35.416666666666664. Epsilon: 0.11935760030527773\n",
      "Episode 102 completed on state (1, 0, 0, 0, 0) with cumulative reward: -11.946264182221505\n",
      "Q explored: 35.416666666666664. Epsilon: 0.11697044829917218\n",
      "Episode 103 completed on state (1, 0, 0, 0, 0) with cumulative reward: -0.7574810135459131\n",
      "Q explored: 35.416666666666664. Epsilon: 0.11463103933318873\n",
      "Episode 104 completed on state (0, 0, 0, 1, 0) with cumulative reward: -48.66895240757782\n",
      "Q explored: 35.416666666666664. Epsilon: 0.11233841854652496\n",
      "Episode 105 completed on state (1, 0, 1, 1, 0) with cumulative reward: 227.0700351241541\n",
      "Q explored: 35.416666666666664. Epsilon: 0.11009165017559445\n",
      "Episode 106 completed on state (0, 0, 0, 1, 0) with cumulative reward: -16.84396841167097\n",
      "Q explored: 35.416666666666664. Epsilon: 0.10788981717208257\n",
      "Episode 107 completed on state (1, 0, 0, 1, 0) with cumulative reward: -43.286029672591866\n",
      "Q explored: 35.416666666666664. Epsilon: 0.10573202082864092\n",
      "Episode 108 completed on state (1, 0, 0, 0, 0) with cumulative reward: 416.354255206755\n",
      "Q explored: 35.416666666666664. Epsilon: 0.1036173804120681\n",
      "Episode 109 completed on state (1, 0, 0, 0, 0) with cumulative reward: 0.07570848160551691\n",
      "Q explored: 35.416666666666664. Epsilon: 0.10154503280382673\n",
      "Episode 110 completed on state (0, 0, 1, 0, 0) with cumulative reward: -65.05000000000001\n",
      "Q explored: 35.416666666666664. Epsilon: 0.09951413214775019\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[241], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[229], line 489\u001b[0m, in \u001b[0;36mQ_learning.train\u001b[1;34m(self, m, verbose)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m    488\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_greedy(state)\n\u001b[1;32m--> 489\u001b[0m     next_obs, reward, done, truncate, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m     reward \u001b[38;5;241m=\u001b[39m fix_reward(reward, next_obs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_right_reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_right_skewness, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchange_lane_reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_obs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    491\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:258\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mact()\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimulation_frequency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:333\u001b[0m, in \u001b[0;36mRoad.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03mStep the dynamics of each entity on the road.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03m:param dt: timestep [s]\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[1;32m--> 333\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:124\u001b[0m, in \u001b[0;36mIDMVehicle.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03mStep the simulation.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m:param dt: timestep\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dt\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:133\u001b[0m, in \u001b[0;36mVehicle.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheading \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msin(beta) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLENGTH \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m dt\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m dt\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:151\u001b[0m, in \u001b[0;36mVehicle.on_state_update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mget_lane(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mrecord_history:\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mappendleft(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_from\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:75\u001b[0m, in \u001b[0;36mIDMVehicle.create_from\u001b[1;34m(cls, vehicle)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_from\u001b[39m(\u001b[38;5;28mcls\u001b[39m, vehicle: ControlledVehicle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIDMVehicle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    Create a new vehicle from an existing one.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    :return: a new vehicle at the same dynamical state\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheading\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_lane_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_lane_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_speed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_speed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43mroute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvehicle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m v\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:58\u001b[0m, in \u001b[0;36mIDMVehicle.__init__\u001b[1;34m(self, road, position, heading, speed, target_lane_index, target_speed, route, enable_lane_change, timer)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     49\u001b[0m              road: Road,\n\u001b[0;32m     50\u001b[0m              position: Vector,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m              enable_lane_change: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     57\u001b[0m              timer: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lane_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_speed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_lane_change \u001b[38;5;241m=\u001b[39m enable_lane_change\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer \u001b[38;5;241m=\u001b[39m timer \u001b[38;5;129;01mor\u001b[39;00m (np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLANE_CHANGE_DELAY\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\controller.py:42\u001b[0m, in \u001b[0;36mControlledVehicle.__init__\u001b[1;34m(self, road, position, heading, speed, target_lane_index, target_speed, route)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     35\u001b[0m              road: Road,\n\u001b[0;32m     36\u001b[0m              position: Vector,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m              target_speed: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m              route: Route \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_lane_index \u001b[38;5;241m=\u001b[39m target_lane_index \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_speed \u001b[38;5;241m=\u001b[39m target_speed \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:40\u001b[0m, in \u001b[0;36mVehicle.__init__\u001b[1;34m(self, road, position, heading, speed, predition_type)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     35\u001b[0m              road: Road,\n\u001b[0;32m     36\u001b[0m              position: Vector,\n\u001b[0;32m     37\u001b[0m              heading: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     38\u001b[0m              speed: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     39\u001b[0m              predition_type: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant_steering\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_type \u001b[38;5;241m=\u001b[39m predition_type\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\objects.py:36\u001b[0m, in \u001b[0;36mRoadObject.__init__\u001b[1;34m(self, road, position, heading, speed)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheading \u001b[38;5;241m=\u001b[39m heading\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m=\u001b[39m speed\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_closest_lane_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheading\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mget_lane(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Enable collision with other collidables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:61\u001b[0m, in \u001b[0;36mRoadNetwork.get_closest_lane_index\u001b[1;34m(self, position, heading)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _to, lanes \u001b[38;5;129;01min\u001b[39;00m to_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _id, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lanes):\n\u001b[1;32m---> 61\u001b[0m             distances\u001b[38;5;241m.\u001b[39mappend(\u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance_with_heading\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     62\u001b[0m             indexes\u001b[38;5;241m.\u001b[39mappend((_from, _to, _id))\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexes[\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmin(distances))]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:125\u001b[0m, in \u001b[0;36mAbstractLane.distance_with_heading\u001b[1;34m(self, position, heading, heading_weight)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m heading \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance(position)\n\u001b[1;32m--> 125\u001b[0m s, r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m angle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_angle(heading, s))\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(r) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(s \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m \u001b[38;5;241m-\u001b[39m s, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m heading_weight\u001b[38;5;241m*\u001b[39mangle\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:190\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    188\u001b[0m delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m    189\u001b[0m longitudinal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection)\n\u001b[1;32m--> 190\u001b[0m lateral \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Q.train(m=150, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Q.plot_rewards_history(smoothing=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.Q = Q_save.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_save = Q.Q.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3544594939943777"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.get_state_visits((1, 0, 1, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0, 1) FASTER 6.595306069958847 (0, 0, 0, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 7.196236042775384 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 7.437736321202813 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 7.534789866738528 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 7.57379350997401 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 7.589468199417281 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 7.595767505552853 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 7.598299055408006 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 7.599316428517236 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 7.599725287952207 (0, 0, 1, 0, 1)\n",
      "(0, 0, 0, 0, 1) FASTER 7.599889599389229 (0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1) FASTER 7.599955632470595 (0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1) FASTER 7.599982169685006 (0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1) FASTER 7.599992834396302 (0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1) FASTER 7.599997120304582 (0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1) FASTER 7.599998842714998 (0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1) FASTER 7.599999534913113 (0, 0, 0, 0, 1)\n",
      "(1, 0, 0, 0, 1) LANE_LEFT 7.599999813092012 (1, 0, 0, 0, 1)\n",
      "(1, 0, 0, 0, 1) LANE_LEFT 6.890066165053625 (1, 0, 0, 0, 1)\n",
      "(0, 0, 0, 1, 0) IDLE 6.313753714002428 (0, 0, 0, 1, 0)\n",
      "(1, 0, 1, 1, 0) FASTER 6.377193819257599 (1, 0, 1, 1, 0)\n",
      "(0, 0, 1, 1, 0) FASTER 6.603377561254613 (0, 0, 1, 1, 0)\n",
      "(0, 0, 0, 1, 0) IDLE 6.593035209859009 (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 1, 0) IDLE 6.5906504263816394 (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 1, 0) IDLE 6.59020557525116 (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 1, 0) IDLE 6.590133962769103 (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 1, 0) IDLE 6.590124280876503 (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 0, 0) FASTER 6.590123456769541 (0, 0, 0, 0, 0)\n",
      "(1, 0, 0, 0, 0) LANE_RIGHT 6.590123456781856 (1, 0, 0, 0, 0)\n",
      "(1, 0, 0, 0, 0) LANE_RIGHT 6.438536031092218 (1, 0, 0, 0, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT 6.746248775609857 (0, 0, 1, 0, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT 7.232343426723524 (0, 0, 1, 0, 0)\n",
      "(0, 0, 1, 0, 1) IDLE 7.57860514822778 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 7.596056602754093 (0, 0, 1, 0, 1)\n",
      "(0, 0, 0, 0, 1) FASTER 7.599315480182524 (0, 0, 0, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 7.599893944711928 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 7.599986752550702 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 7.599998940336363 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 7.599999999999952 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 7.599999999999984 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 7.599999999999994 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 7.599999999999996 (0, 0, 1, 0, 1)\n",
      "(0, 0, 0, 0, 1) FASTER 7.599999999999996 (0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1) FASTER 7.599999999999996 (0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1) FASTER 7.599999999999996 (0, 0, 0, 0, 1)\n",
      "(1, 0, 0, 0, 1) LANE_LEFT 7.599999999999996 (1, 0, 0, 0, 1)\n",
      "(1, 0, 0, 0, 1) LANE_LEFT 6.89006624114125 (1, 0, 0, 0, 1)\n",
      "(0, 0, 1, 1, 0) FASTER 6.3137537445433916 (0, 0, 1, 1, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT 6.377193831303394 (0, 0, 1, 0, 0)\n",
      "(1, 0, 0, 1, 0) IDLE 6.487953112619951 (1, 0, 0, 1, 0)\n",
      "(0, 0, 1, 1, 0) FASTER 6.987676604978892 (0, 0, 1, 1, 0)\n",
      "(0, 0, 1, 1, 0) FASTER 7.049157855948918 (0, 0, 1, 1, 0)\n",
      "(0, 0, 0, 1, 0) IDLE 7.061312976003146 (0, 0, 0, 1, 0)\n",
      "(0, 0, 1, 1, 0) FASTER 7.063680581902139 (0, 0, 1, 1, 0)\n",
      "(1, 0, 1, 0, 0) SLOWER 7.064114576111168 (1, 0, 1, 0, 0)\n",
      "(1, 0, 0, 0, 0) LANE_RIGHT 5.568878272605213 (1, 0, 0, 0, 0)\n",
      "(1, 0, 0, 0, 0) LANE_RIGHT 4.834745163195199 (1, 0, 0, 0, 0)\n",
      "(0, 0, 1, 0, 1) IDLE 5.16233210921998 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 5.142980419691628 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 5.120969563023834 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 5.109272421284493 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 5.103929992836704 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 5.101628330073222 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 5.100666085775546 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 5.100270473399946 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 5.100109340967256 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 5.100044367529094 (0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1) IDLE 5.100017830314975 (0, 0, 1, 0, 1)\n",
      "(0, 0, 0, 0, 1) FASTER 5.100007165603696 (0, 0, 0, 0, 1)\n",
      "(1, 0, 0, 0, 1) LANE_LEFT 6.595308949654264 (1, 0, 0, 0, 1)\n",
      "(1, 0, 0, 0, 1) LANE_LEFT 6.477244686561166 (1, 0, 0, 0, 1)\n",
      "(0, 0, 0, 1, 0) IDLE 6.146776948415452 (0, 0, 0, 1, 0)\n",
      "(0, 0, 1, 1, 0) FASTER 6.311349314045427 (0, 0, 1, 1, 0)\n",
      "(1, 0, 1, 1, 0) FASTER 6.576898102716665 (1, 0, 1, 1, 0)\n",
      "(1, 0, 0, 1, 0) IDLE 6.582422762589124 (1, 0, 0, 1, 0)\n",
      "(1, 0, 1, 0, 0) SLOWER -72.47997999832621 (1, 0, 1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "Q.test(sleep_time=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c70f7934774664b7b172d33b92bc57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 completed on state (1, 0, 0, 1, 0) with cumulative reward: 55.83995013308234\n",
      "Q explored: 35.416666666666664\n",
      "Episode 2 completed on state (0, 0, 1, 1, 0) with cumulative reward: -4.100641294579048\n",
      "Q explored: 35.416666666666664\n",
      "Episode 3 completed on state (0, 0, 1, 0, 0) with cumulative reward: 154.7079343001195\n",
      "Q explored: 35.416666666666664\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[38], line 344\u001b[0m, in \u001b[0;36mSarsa.train\u001b[1;34m(self, m, verbose)\u001b[0m\n\u001b[0;32m    342\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m--> 344\u001b[0m     next_obs, reward, done, truncate, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m     reward \u001b[38;5;241m=\u001b[39m fix_reward(reward, done\u001b[38;5;241m=\u001b[39mdone, colision_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolision_reward, skew_speed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskew_speed)\n\u001b[0;32m    346\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:257\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_control\u001b[39m\u001b[38;5;124m\"\u001b[39m] \\\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:324\u001b[0m, in \u001b[0;36mRoad.act\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decide the actions of each entity on the road.\"\"\"\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[1;32m--> 324\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:100\u001b[0m, in \u001b[0;36mIDMVehicle.act\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     97\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Longitudinal: IDM\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m front_vehicle, rear_vehicle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneighbour_vehicles\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlane_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macceleration(ego_vehicle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    102\u001b[0m                                            front_vehicle\u001b[38;5;241m=\u001b[39mfront_vehicle,\n\u001b[0;32m    103\u001b[0m                                            rear_vehicle\u001b[38;5;241m=\u001b[39mrear_vehicle)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# When changing lane, check both current and target lanes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:361\u001b[0m, in \u001b[0;36mRoad.neighbour_vehicles\u001b[1;34m(self, vehicle, lane_index)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vehicle \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, Landmark):  \u001b[38;5;66;03m# self.network.is_connected_road(v.lane_index,\u001b[39;00m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;66;03m# lane_index, same_lane=True):\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m         s_v, lat_v \u001b[38;5;241m=\u001b[39m \u001b[43mlane\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lane\u001b[38;5;241m.\u001b[39mon_lane(v\u001b[38;5;241m.\u001b[39mposition, s_v, lat_v, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    363\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:190\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    188\u001b[0m delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m    189\u001b[0m longitudinal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection)\n\u001b[1;32m--> 190\u001b[0m lateral \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sar.train(m=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar.Q = Q_save.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_save = sar.Q.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FASTER': 5.133641775043947,\n",
       " 'LANE_RIGHT': 1.8789156890532015,\n",
       " 'SLOWER': 1.7437783264494309,\n",
       " 'IDLE': 0.5959434836555192,\n",
       " 'LANE_LEFT': -1.1294429580928238}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sar.search_Q((1, 0, 0, 1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 1, 0, 0)\n",
      "(0, 0, 1, 0, 0)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(1, 0, 1, 0, 1)\n",
      "(1, 0, 1, 0, 1)\n"
     ]
    }
   ],
   "source": [
    "sar.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation testing kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.03333333333333\n",
      "20.127211934156396\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[157], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m      6\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m----> 7\u001b[0m     obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     cum_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Print non-zero obs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:258\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mact()\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimulation_frequency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:333\u001b[0m, in \u001b[0;36mRoad.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03mStep the dynamics of each entity on the road.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03m:param dt: timestep [s]\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[1;32m--> 333\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:124\u001b[0m, in \u001b[0;36mIDMVehicle.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03mStep the simulation.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m:param dt: timestep\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dt\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:133\u001b[0m, in \u001b[0;36mVehicle.step\u001b[1;34m(self, dt)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheading \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msin(beta) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLENGTH \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m dt\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m dt\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\kinematics.py:148\u001b[0m, in \u001b[0;36mVehicle.on_state_update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_state_update\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad:\n\u001b[1;32m--> 148\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_closest_lane_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheading\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mget_lane(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index)\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mrecord_history:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:61\u001b[0m, in \u001b[0;36mRoadNetwork.get_closest_lane_index\u001b[1;34m(self, position, heading)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _to, lanes \u001b[38;5;129;01min\u001b[39;00m to_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _id, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lanes):\n\u001b[1;32m---> 61\u001b[0m             distances\u001b[38;5;241m.\u001b[39mappend(\u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance_with_heading\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     62\u001b[0m             indexes\u001b[38;5;241m.\u001b[39mappend((_from, _to, _id))\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexes[\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmin(distances))]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:125\u001b[0m, in \u001b[0;36mAbstractLane.distance_with_heading\u001b[1;34m(self, position, heading, heading_weight)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m heading \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance(position)\n\u001b[1;32m--> 125\u001b[0m s, r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m angle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_angle(heading, s))\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(r) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(s \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m \u001b[38;5;241m-\u001b[39m s, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m heading_weight\u001b[38;5;241m*\u001b[39mangle\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:190\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    188\u001b[0m delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m    189\u001b[0m longitudinal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection)\n\u001b[1;32m--> 190\u001b[0m lateral \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate an episode of the environment and show the rewards and cumulative rewards\n",
    "cum_reward = 0\n",
    "with gym.make(\"highway-v0\", config=kinematics, render_mode='human') as env:\n",
    "    obs = env.reset()\n",
    "    for _ in range(100):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "        # Print non-zero obs\n",
    "        obs_non = obs[~np.all(obs == 0, axis=1)]\n",
    "        print(reward)\n",
    "        if done:\n",
    "            break\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time to collision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.  0.  0.5 1. ]\n",
      " [0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.5 1.  1. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "[[0.5 1.  1.  0.5 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "[[0.  1.  1.  0.5 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 1.  1.  0.  0. ]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [1.  1.  0.5 0.  0. ]]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[0.  0.  0.  0.  0.5]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.5 1.  1.  0. ]]\n",
      "[[0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 1.  1.  0.  0. ]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [1.  1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n"
     ]
    }
   ],
   "source": [
    "class TimeToCollision:\n",
    "    def __init__(horizon=5,\n",
    "                policy_frequency=1,\n",
    "                simulation_frequency=10):\n",
    "\n",
    "        self.config = default_config.copy()\n",
    "        self.config[\"observation\"] =  {\n",
    "        \"type\": \"TimeToCollision\",\n",
    "        \"horizon\": horizon}\n",
    "        self.config['policy_frequency'] = policy_frequency\n",
    "        self.config['simulation_frequency'] = simulation_frequency\n",
    "\n",
    "    def get_state(self, env): \n",
    "        grid = env.vehicle.speed_index\n",
    "        return self.current_obs[grid]\n",
    "\n",
    "    def test_env(self):\n",
    "        with gym.make(\"highway-v0\", config=self.config, render_mode='human') as env:\n",
    "            obs = env.reset()\n",
    "            for _ in range(1000):\n",
    "                action = env.action_space.sample()\n",
    "                obs, reward, done, truncated, info = env.step(action)\n",
    "                print(self.get_state, reward)\n",
    "                if done:\n",
    "                    break\n",
    "                time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
