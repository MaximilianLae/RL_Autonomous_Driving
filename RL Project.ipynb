{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c408d461",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Final Project \n",
    "\n",
    "Welcome to your Reinforcement Learning project! Join in groups of a maximum of 5 students on a project focused on developing an RL agent capable of solving an environment for decision-making in Autonomous Driving. The project deadline has been set to the 2nd of June.\n",
    "\n",
    "Autonomous Driving has long been considered a field in which RL algorithms excel, and this project aims to leverage the power of RL to create an intelligent agent that can solve the Farama’s foundation “highway-env” project, namely the Highway environment (refer to https://highway-env.farama.org/environments/highway/).\n",
    "\n",
    "## Project Requirements:\n",
    "\n",
    "* The environments observation’s format can vary according to our preference, namely Kinematics, Grayscale Image, Occupancy grid and Time to collision (refer to https://highway-env.farama.org/observations/). In your solutions you should use 2 of these types.\n",
    "* The agents actions can also vary, as continuous actions, discrete actions and discrete meta-actions (refer to https://highway-env.farama.org/actions/). In your solutions you should use 2 of these types.\n",
    "* As for the algorithms to use, any algorithm is valid (seen or not in class), with a minimum requirement of 3 different algorithms used.\n",
    "* Apart from the environment observation types and agent action types you must use environment’s configuration provided in the annexed notebook!\n",
    "Note: Your delivery should comprise 4 solutions to the highway environment (corresponding to the combinations of the two environment observation’s types and the two agent’s action types), in which you just need to use one algorithm for each combination (knowing that you need to use at least 3 different algorithms).\n",
    "\n",
    "\n",
    "## Project Objectives:\n",
    "\n",
    "* Train an RL agent to solve the Highway environment: The primary objective of this project is to develop an RL agent that can maximize the reward given by the highway environment (refer to https://highway-env.farama.org/rewards/), which leverages to maximize speed while minimizing crash risk! \n",
    "* Optimize decision-making using RL algorithms: Explore different RL algorithms to train the agent. Compare and analyse their effectiveness in learning and decision-making capabilities in the context of the environment.\n",
    "* Explore and expand on the reward system: Although you should evaluate your agent with the reward function provided by the environment, you could/should expand it to better train your agent.\n",
    "* Enhance interpretability and analysis: Develop methods to analyse the agent's decision-making process and provide insights into its strategic thinking. Investigate techniques to visualize the agent's evaluation of chess positions and understand its reasoning behind specific moves.\n",
    "\n",
    "\n",
    "\n",
    "### Extra Objectives:\n",
    "\n",
    "* Investigate transfer learning and generalization: Explore techniques for transfer learning to leverage knowledge acquired in related domains or from pre-training on large chess datasets. Investigate the agent's ability to generalize its knowledge.\n",
    "* Explore multi agent approaches: The environment allows you to use more than one agent per episode. Explore multi agent alternatives to improve your learning times and overall benchmarks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aabeb2",
   "metadata": {},
   "source": [
    "## Imports Required\n",
    "\n",
    "You might need to restart the kernel after installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install highway-env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b431af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import time\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f15ebae",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "\n",
    "Apart from the environment observation types and agent action types you must use some of the environment’s configurations provided bellow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4117ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "\n",
    "    # Parametrization bellow cannot be changed\n",
    "    \"lanes_count\": 10, # The environment must always have 10 lanes\n",
    "    \"vehicles_count\": 50, # The environment must always have 50 other vehicles\n",
    "    \"duration\": 120,  # [s] The environment must terminate never before 120 seconds\n",
    "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.IDMVehicle\", # This is the policy of the other vehicles\n",
    "    \"initial_spacing\": 2, # Initial spacing between vehicles needs to be at most 2\n",
    "\n",
    "    # Refer to refer to https://highway-env.farama.org/observations/ to change observation space type\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\"\n",
    "    },\n",
    "\n",
    "    # Refer to refer to https://highway-env.farama.org/actions/ to change action space type\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteMetaAction\",\n",
    "    },\n",
    "\n",
    "    # Parameterization bellow can be changed (as it refers mostly to the reward system)\n",
    "    \"collision_reward\": -1,  # The reward received when colliding with a vehicle. (Can be changed)\n",
    "    \"reward_speed_range\": [20, 30],  # [m/s] The reward for high speed is mapped linearly from this range to [0, HighwayEnv.HIGH_SPEED_REWARD]. (Can be changed)\n",
    "    \"simulation_frequency\": 15,  # [Hz] (Can be changed)\n",
    "    \"policy_frequency\": 1,  # [Hz] (Can be changed)\n",
    "    \n",
    "    # Parameters defined bellow are purely for visualiztion purposes! You can alter them as you please\n",
    "    \"screen_width\": 800,  # [px]\n",
    "    \"screen_height\": 600,  # [px]\n",
    "    \"centering_position\": [0.5, 0.5],\n",
    "    \"scaling\": 5,\n",
    "    \"show_trajectories\": False,\n",
    "    \"render_agent\": True,\n",
    "    \"offscreen_rendering\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52431cb",
   "metadata": {},
   "source": [
    "## Example Solution\n",
    "### Solution 0\n",
    "Environment Observation Type: **Kinematics** \\\n",
    "Agent Action Type: **DiscreteMetaAction** \\\n",
    "Algorithm Used: **Random**\n",
    "\n",
    "Example of the environment's usage using a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196cb827",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env = gym.make('highway-v0', render_mode='human', config=configuration)\n",
    "\n",
    "obs, info = env.reset(seed=42)\n",
    "done = truncated = False\n",
    "\n",
    "Return = 0\n",
    "n_steps = 1\n",
    "Episode = 0\n",
    "while not (done or truncated):\n",
    "  # Dispatch the observations to the model to get the tuple of actions\n",
    "  action = env.action_space.sample()\n",
    "  # Execute the actions\n",
    "  next_obs, reward, done, truncated, info = env.step(action)\n",
    "  Return+=reward\n",
    "\n",
    "  print('Episode: {}, Step: {}, Return: {}'.format(Episode, n_steps, round(Return,2)))\n",
    "  n_steps+=1\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653a88d",
   "metadata": {},
   "source": [
    "### Solution 1\n",
    "Environment Observation Type: \\\n",
    "Agent Action Type: \\\n",
    "Algorithm Used: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633489a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1653ba8",
   "metadata": {},
   "source": [
    "### Solution 2\n",
    "Environment Observation Type: \\\n",
    "Agent Action Type: \\\n",
    "Algorithm Used: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071c31f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fc84f81",
   "metadata": {},
   "source": [
    "## Solution 3\n",
    "Environment Observation Type: \\\n",
    "Agent Action Type: \\\n",
    "Algorithm Used: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c0f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d66f0daf",
   "metadata": {},
   "source": [
    "## Solution 4\n",
    "Environment Observation Type: \\\n",
    "Agent Action Type: \\\n",
    "Algorithm Used: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45811a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428b3be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
