{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "\n",
    "    # Parametrization bellow cannot be changed\n",
    "    \"lanes_count\" : 10, # The environment must always have 10 lanes\n",
    "    \"vehicles_count\": 50, # The environment must always have 50 other vehicles\n",
    "    \"duration\": 120,  # [s] The environment must terminate never before 120 seconds\n",
    "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.IDMVehicle\", # This is the policy of the other vehicles\n",
    "    \"initial_spacing\": 2, # Initial spacing between vehicles needs to be at most 2\n",
    "\n",
    "    # Refer to refer to https://highway-env.farama.org/observations/ to change observation space type\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\"\n",
    "    },\n",
    "\n",
    "    # Refer to refer to https://highway-env.farama.org/actions/ to change action space type\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteMetaAction\",\n",
    "    },\n",
    "\n",
    "    # Parameterization bellow can be changed (as it refers mostly to the reward system)\n",
    "    # \"collision_reward\": -10,  # The reward received when colliding with a vehicle. (Can be changed)\n",
    "    # \"reward_speed_range\": [20, 30],  # [m/s] The reward for high speed is mapped linearly from this range to [0, HighwayEnv.HIGH_SPEED_REWARD]. (Can be changed)\n",
    "    \"simulation_frequency\": 15, #15,  # [Hz] (Can be changed)\n",
    "    \"policy_frequency\": 5, #5,  # [Hz] (Can be changed)\n",
    "\n",
    "    \"collision_reward\": -100,  # The reward received when colliding with a vehicle.\n",
    "    \"right_lane_reward\": 0.1,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
    "    # zero for other lanes.\n",
    "    \"high_speed_reward\": 100,  # The reward received when driving at full speed, linearly mapped to zero for\n",
    "    # lower speeds according to config[\"reward_speed_range\"].\n",
    "    \"lane_change_reward\": 0,  # The reward received at each lane change action.\n",
    "    \"reward_speed_range\": [20, 30],\n",
    "    \n",
    "    # Parameters defined bellow are purely for visualiztion purposes! You can alter them as you please\n",
    "    \"screen_width\": 800,  # [px]\n",
    "    \"screen_height\": 600,  # [px]\n",
    "    \"centering_position\": [0.5, 0.5],\n",
    "    \"scaling\": 5,\n",
    "    \"show_trajectories\": True,\n",
    "    \"render_agent\": True,\n",
    "    \"offscreen_rendering\": False\n",
    "}\n",
    "\n",
    "default_config = configuration.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occupancyGrid = configuration.copy()\n",
    "occupancyGrid[\"observation\"] =  {\n",
    "    \"type\": \"OccupancyGrid\",\n",
    "    # \"vehicles_count\": 50,\n",
    "    \"features\": [\n",
    "                \"presence\",\n",
    "                #\"x\", \"y\", \n",
    "                #\"vx\", \"vy\"\n",
    "                ],\n",
    "    # \"features_range\": {\n",
    "    #      \"x\": [-500, 500],\n",
    "    #      \"y\": [-500, 500],\n",
    "    #     \"vx\": [-20, 20],\n",
    "    #     \"vy\": [-20, 20]\n",
    "    # },\n",
    "    \"grid_size\": [[-100, 100], [-100, 100]],    # X controls how many lanes, Y controls how far ahead\n",
    "    \"grid_step\": [1, 1],\n",
    "    #\"absolute\": False,                     # Not implemented in the library\n",
    "    #\"as_image\": True,\n",
    "    # \"align_to_vehicle_axes\" : True\n",
    "}\n",
    "\n",
    "# The higher the number, the more frequent the policy and the simulation frequencies, the slower the simulation\n",
    "occupancyGrid[\"simulation_frequency\"] = 15\n",
    "occupancyGrid[\"policy_frequency\"] = 1\n",
    "\n",
    "env = gym.make('highway-v0', render_mode='human', config=occupancyGrid)\n",
    "\n",
    "obs, info = env.reset(seed = 30)\n",
    "\n",
    "env.close()\n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.750013881947915 0.750013881947915\n",
      "0.0003886945416181051 0.750402576489533\n"
     ]
    }
   ],
   "source": [
    "# Generate an episode of the environment and show the rewards and cumulative rewards\n",
    "cum_reward = 0\n",
    "with gym.make(\"highway-v0\", config=occupancyGrid, render_mode='human') as env:\n",
    "    obs = env.reset()\n",
    "    for _ in range(1000):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "        print(reward, cum_reward)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccupancyGrid():\n",
    "    def __init__(\n",
    "            self,\n",
    "            grid_size=[[-50, 50], [-50, 50]],  # X controls the lane-width, Y controls how far ahead\n",
    "            grid_step=[1, 1],\n",
    "            colision_reward=-100,\n",
    "            skew_speed=1,\n",
    "            policy=None,\n",
    "            sim_frequency=10,\n",
    "            policy_frequency=1,\n",
    "            render_mode = 'human',\n",
    "            seed = 50,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Occupancy view class constructor\n",
    "        Arguments:\n",
    "            grid_size: list of lists, the size of the grid in the x and y direction, where x controls the lane-width and y controls how far ahead. Lanes are 5m wide, and the car position is (0,0)\n",
    "            grid_step: list, the step size of the grid in the x and y direction, in meters\n",
    "            colision_reward: float, the reward to give when a colision occurs\n",
    "            skew_speed: float, the skew speed to apply to the reward\n",
    "            policy: function, the policy to use in the simulation\n",
    "            sim_frequency: int, the frequency of the simulation\n",
    "            policy_frequency: int, the frequency of the policy\n",
    "            render_mode: str, the mode to render the simulation\n",
    "            seed: int, the seed to use in the simulation\n",
    "        \"\"\"\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.grid_step = grid_step\n",
    "        self.config = default_config.copy()\n",
    "        self.config[\"observation\"] =  {\n",
    "            \"type\": \"OccupancyGrid\",\n",
    "            \"features\": [\"presence\"],\n",
    "            \"grid_size\": grid_size,    # X controls how many lanes, Y controls how far ahead\n",
    "            \"grid_step\": grid_step,\n",
    "        }\n",
    "        self.config[\"simulation_frequency\"] = sim_frequency\n",
    "        self.config[\"policy_frequency\"] = policy_frequency\n",
    "        self.render_mode = render_mode\n",
    "        self.seed = seed\n",
    "        self.policy = policy\n",
    "        self.colision_reward = colision_reward\n",
    "        self.skew_speed = skew_speed\n",
    "        self.initialize_states()\n",
    "\n",
    "\n",
    "    def initialize_states(self):\n",
    "        \"\"\"\n",
    "        Initialize the states of the occupancy grid\n",
    "        \"\"\"\n",
    "        # Start the environment\n",
    "        with gym.make('highway-v0', render_mode=self.render_mode, config=self.config) as env:\n",
    "            obs, info = env.reset(seed = self.seed)\n",
    "            self.current_obs = obs\n",
    "            self.env = env\n",
    "        \n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Get the state of the environment\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def test_env(self):\n",
    "        \"\"\"\n",
    "        Function to test the environment with a random policy, or with a policy\n",
    "        \"\"\"\n",
    "        obs, info = self.env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        while not done:\n",
    "            # start = time.time()\n",
    "            if self.policy is None:\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                action = self.policy()\n",
    "            obs, reward, done, truncate, info = self.env.step(action)\n",
    "            self.current_obs = obs\n",
    "            print(self.get_state(type='lane-wise', decode=True), decode_meta_action(action), fix_reward(reward, skew_speed=2))\n",
    "            time.sleep(1)\n",
    "            # end = time.time()\n",
    "            # print(f\"Time taken: {end-start}\")\n",
    "        self.env.close()\n",
    "        return info[\"score\"]\n",
    "\n",
    "# ------------------- SARSA -------------------    \n",
    "class Sarsa(OccupancyGrid):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha=0.75,\n",
    "        gamma=0.95,\n",
    "        epsilon=0.6,\n",
    "        print_stats=False,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        SARSA class constructor\n",
    "        Arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            m: int, the number of episodes to train the agent for\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.initialize_Q(print_stats)\n",
    "        self.alpha, self.gamma, self.epsilon = alpha, gamma, epsilon\n",
    "\n",
    "    def policy_Q(self, state):\n",
    "        values = [self.Q[(state, action)] for action in range(5)]\n",
    "        return np.argmax(values)   \n",
    "\n",
    "    def initialize_Q(self, print_stats = False):\n",
    "        # Combine the possible states with the possible actions\n",
    "        keys = list(itertools.product(self.states, range(5)))       # 5 possible actions, 0-4: left, idle, right, accelerate, decelerate\n",
    "        if len(keys) > 150000:\n",
    "            print(\"Warning: The number of states is too large, consider reducing the number of states\")\n",
    "        if print_stats:\n",
    "            print(f\"Number of states: {len(keys)}\")   \n",
    "        self.Q = {key: 0 for key in keys}\n",
    "\n",
    "    def epsilon_greedy(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(5)\n",
    "        else:\n",
    "            values = [self.Q[(state, action)] for action in range(5)]\n",
    "            return np.argmax(values)\n",
    "        \n",
    "    def train(self, m=100, verbose=0): \n",
    "        env = gym.make('highway-v0', render_mode=None, config=self.config)\n",
    "        obs, info = env.reset(seed = np.random.randint(1000))\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        state = self.get_state()\n",
    "        action = self.epsilon_greedy(state)\n",
    "        q_explored = np.count_nonzero(list(self.Q.values()))\n",
    "        for i in tqdm(range(m)):\n",
    "            env.reset()\n",
    "            cum_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                reward = fix_reward(reward, done=done, colision_reward=self.colision_reward, skew_speed=self.skew_speed)\n",
    "                next_state = self.get_state()\n",
    "                next_action = self.epsilon_greedy(next_state)\n",
    "\n",
    "                print(next_state, decode_meta_action(next_action), reward) if verbose > 2 else None\n",
    "                if self.Q[(state, action)] == 0:\n",
    "                    q_explored += 1\n",
    "                cum_reward += reward\n",
    "\n",
    "                self.Q[(state, action)] += self.alpha*(reward + self.gamma*self.Q[(next_state, next_action)] - self.Q[(state, action)])\n",
    "                state, action = next_state, next_action\n",
    "                self.current_obs = next_obs\n",
    "            print(f\"Episode {i+1} completed, cumulative reward: {cum_reward}\") if verbose > 0 else None\n",
    "            print(f\"Q explored: {100*q_explored/len(self.Q)}\") if verbose > 1 else None\n",
    "        env.close()\n",
    "\n",
    "    def test(self):\n",
    "        env = gym.make('highway-v0', render_mode=self.render_mode, config=self.config)\n",
    "        obs, info = env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        state = self.get_state(self.state_type)\n",
    "        action = self.policy_Q(state)\n",
    "        while not done:\n",
    "            next_obs, reward, done, truncate, info = env.step(action)\n",
    "            next_state = self.get_state(self.state_type)\n",
    "            next_action = self.policy_Q(next_state)\n",
    "            state, action = next_state, next_action\n",
    "            self.current_obs = next_obs\n",
    "            print(next_state)\n",
    "        env.close()\n",
    "        return info[\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- grid_size=[[-50, 50], [-50, 50]],  # X controls the lane-width, Y controls how far ahead\n",
    "- grid_step=[1, 1],\n",
    "- n_closest=3,\n",
    "- ss_bins=[5,6],\n",
    "- crop_dist=[[-10,10], [-10,25]],\n",
    "- policy=None,\n",
    "- sim_frequency=15,\n",
    "- policy_frequency=5,\n",
    "- render_mode = 'human',\n",
    "- seed = 50,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "copyQ = a.Q.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 540\n"
     ]
    }
   ],
   "source": [
    "a = Sarsa(print_stats=True, epsilon=0.6, alpha=0.3, gamma=0.8)\n",
    "a.Q = copyQ.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! TODO !!!\n",
    "\n",
    "- Use kinematics to substitute the current occupancy grid class\n",
    "\n",
    "- He cannot know when he cant turn left or right because the lane is the final one\n",
    "\n",
    "- Change the occupancy class to use occupancy grid with 5m grid size, 3 lanes and -30, 30 ahead\n",
    "\n",
    "- Make a plot history \n",
    "\n",
    "- Check how many times each state is visited\n",
    "\n",
    "- Check the action distribution, so as to see if slowing down is the most chosen action and the one with the best Q value\n",
    "\n",
    "- Change the reward for colision, so that the agent goes faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! IDEA WITH KINEMATICS !!!\n",
    "- State space in this maner: (danger ahead, danger left, danger right, danger behind, lane position, (maybe) speed)\n",
    "- Lane position can be 0 if in the middle, 1 if in the right, -1 if in the left\n",
    "\n",
    "- We need speed, because if the speed is too fast, it might not be able to turn in time\n",
    "- Actually, we could just increase the safety distance, and then we wouldn't need speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c280dc5269724502bed494f225b52c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[106], line 307\u001b[0m, in \u001b[0;36mSarsa.train\u001b[1;34m(self, m, verbose)\u001b[0m\n\u001b[0;32m    305\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m--> 307\u001b[0m     next_obs, reward, done, truncate, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     reward \u001b[38;5;241m=\u001b[39m fix_reward(reward, done\u001b[38;5;241m=\u001b[39mdone, colision_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolision_reward, skew_speed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskew_speed)\n\u001b[0;32m    309\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:257\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_control\u001b[39m\u001b[38;5;124m\"\u001b[39m] \\\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:324\u001b[0m, in \u001b[0;36mRoad.act\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decide the actions of each entity on the road.\"\"\"\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[1;32m--> 324\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:95\u001b[0m, in \u001b[0;36mIDMVehicle.act\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_road()\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_lane_change:\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchange_lane_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteering_control(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_lane_index)\n\u001b[0;32m     97\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:214\u001b[0m, in \u001b[0;36mIDMVehicle.change_lane_policy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# Does the MOBIL model recommend a lane change?\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmobil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlane_index\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_lane_index \u001b[38;5;241m=\u001b[39m lane_index\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:236\u001b[0m, in \u001b[0;36mIDMVehicle.mobil\u001b[1;34m(self, lane_index)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Do I have a planned route for a specific lane which is safe for me to access?\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m old_preceding, old_following \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneighbour_vehicles\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m self_pred_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macceleration(ego_vehicle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, front_vehicle\u001b[38;5;241m=\u001b[39mnew_preceding)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroute \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroute[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# Wrong direction\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:361\u001b[0m, in \u001b[0;36mRoad.neighbour_vehicles\u001b[1;34m(self, vehicle, lane_index)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vehicle \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, Landmark):  \u001b[38;5;66;03m# self.network.is_connected_road(v.lane_index,\u001b[39;00m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;66;03m# lane_index, same_lane=True):\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m         s_v, lat_v \u001b[38;5;241m=\u001b[39m \u001b[43mlane\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lane\u001b[38;5;241m.\u001b[39mon_lane(v\u001b[38;5;241m.\u001b[39mposition, s_v, lat_v, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    363\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:190\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    188\u001b[0m delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m    189\u001b[0m longitudinal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection)\n\u001b[1;32m--> 190\u001b[0m lateral \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a.train(m=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.37037037037037\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for keys, values in a.Q.items():\n",
    "    if values != 0:\n",
    "        count += 1\n",
    "\n",
    "print(100*count/len(a.Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: LANE_LEFT, Q-value: -46.81659543807035\n",
      "Action: IDLE, Q-value: -13.458657418241788\n",
      "Action: LANE_RIGHT, Q-value: -12.65590344059557\n",
      "Action: FASTER, Q-value: -7.4392318919133285\n",
      "Action: SLOWER, Q-value: -8.453300283131032\n"
     ]
    }
   ],
   "source": [
    "# Check the values for Q for this state (15, 30, 18, 18)\n",
    "state = (30, 30, 18, 18)\n",
    "for action in range(5):\n",
    "    print(f\"Action: {decode_meta_action(action)}, Q-value: {a.Q[(state, action)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 30, 18, 18)\n",
      "(30, 30, 18, 6)\n",
      "(30, 30, 18, 18)\n",
      "(15, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(30, 30, 18, 18)\n",
      "(30, 30, 18, 6)\n",
      "(30, 30, 18, 12)\n",
      "(15, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 12)\n",
      "(5, 30, 18, 12)\n",
      "(5, 30, 18, 12)\n",
      "(10, 30, 18, 12)\n",
      "(10, 30, 18, 12)\n",
      "(30, 30, 12, 6)\n",
      "(15, 30, 18, 12)\n",
      "(10, 30, 18, 12)\n",
      "(10, 30, 18, 6)\n",
      "(30, 10, 6, 6)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[106], line 339\u001b[0m, in \u001b[0;36mSarsa.test\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mprint\u001b[39m(next_state)\n\u001b[0;32m    338\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'score'"
     ]
    }
   ],
   "source": [
    "a.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_occupancy = OcupancyGrid(render_mode=None)\n",
    "# print(new_occupancy.x_bins, new_occupancy.y_bins, len(new_occupancy.states))\n",
    "# new_occupancy.test_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________\n",
    "### Using kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 83.56591  ,  28.       ,  25.       ,   0.       ],\n",
       "       [  9.4535885,   8.       ,  -1.5146654,   0.       ],\n",
       "       [ 20.187185 , -24.       ,  -1.9328905,   0.       ],\n",
       "       [ 31.133034 , -24.       ,  -1.129222 ,   0.       ],\n",
       "       [ 41.896053 ,   0.       ,  -1.7400944,   0.       ],\n",
       "       [ 51.945835 ,  -8.       ,  -3.565016 ,   0.       ],\n",
       "       [ 62.276524 , -16.       ,  -3.3216567,   0.       ],\n",
       "       [ 72.60146  ,   8.       ,  -1.0905089,   0.       ],\n",
       "       [ 82.24522  ,  -8.       ,  -2.182405 ,   0.       ],\n",
       "       [ 91.352806 , -20.       ,  -3.5367248,   0.       ],\n",
       "       [100.35392  , -16.       ,  -3.2140508,   0.       ],\n",
       "       [109.795944 ,  -4.       ,  -3.2532372,   0.       ],\n",
       "       [119.59274  ,   0.       ,  -1.6719042,   0.       ],\n",
       "       [129.31784  ,   4.       ,  -2.6289418,   0.       ],\n",
       "       [138.54662  ,  -4.       ,  -2.0985248,   0.       ],\n",
       "       [149.64609  , -16.       ,  -1.698277 ,   0.       ],\n",
       "       [159.69713  ,   0.       ,  -1.612952 ,   0.       ],\n",
       "       [169.55093  ,   4.       ,  -2.264635 ,   0.       ],\n",
       "       [179.66725  ,   0.       ,  -3.794033 ,   0.       ],\n",
       "       [190.1403   ,   4.       ,  -2.7294507,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kinematics = configuration.copy()\n",
    "kinematics[\"observation\"] =  {\n",
    "    \"type\": \"Kinematics\",\n",
    "    \"vehicles_count\": 50,\n",
    "    \"features\": [\"x\", \"y\", \"vx\", \"vy\"],\n",
    "    # \"features_range\": {\n",
    "    #     \"x\": [-40, 40],\n",
    "    #     \"y\": [-40, 40],\n",
    "    #     \"vx\": [-200, 200],\n",
    "    #     \"vy\": [-200, 200]\n",
    "    # }, \n",
    "    \"absolute\": False,\n",
    "    \"normalize\": False,\n",
    "}\n",
    "\n",
    "# The higher the number, the more frequent the policy and the simulation frequencies, the slower the simulation\n",
    "kinematics[\"simulation_frequency\"] = 10\n",
    "kinematics[\"policy_frequency\"] = 2\n",
    "\n",
    "env = gym.make('highway-v0', render_mode='human', config=kinematics)\n",
    "\n",
    "obs, info = env.reset(seed = 10)\n",
    "\n",
    "env.close()\n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def fix_reward(position, colision_reward=-1000, speed_rewards=[0,30], right_lane=10, done=True): \n",
    "    \"\"\"\n",
    "    This function is used to correct the reward function, which is not correctly outputted by the environment\n",
    "    Params: \n",
    "        position: float, the position to fix\n",
    "        colision_reward: float, the colision reward to fix\n",
    "        speed_rewards: list, the speed rewards to fix\n",
    "        right_lane: float, the right lane reward to fix\n",
    "        done: bool, double check if the reward was indeed a colision\n",
    "    \"\"\"\n",
    "    if done:\n",
    "        return colision_reward\n",
    "    \n",
    "    lane = position[1]\n",
    "    speed = position[3]\n",
    "    \n",
    "    raise NotImplementedError(\"This function is not yet implemented\")\n",
    "    \n",
    "    \n",
    "\n",
    "def decode_meta_action(action):\n",
    "    \"\"\"\n",
    "    Function to output the corresponding action in text-form\n",
    "    \"\"\"\n",
    "    assert action in range(5), \"The action must be between 0 and 4\"\n",
    "    if action == 0:\n",
    "        return \"LANE_LEFT\"\n",
    "    elif action == 1:\n",
    "        return \"IDLE\"\n",
    "    elif action == 2:\n",
    "        return \"LANE_RIGHT\"\n",
    "    elif action == 3:\n",
    "        return \"FASTER\"\n",
    "    elif action == 4:\n",
    "        return \"SLOWER\"\n",
    "    \n",
    "def decode_danger(state):\n",
    "    \"\"\"\n",
    "    Function to decode the danger state\n",
    "    \"\"\"\n",
    "    state_meaning = ['front', 'back', 'left', 'right']\n",
    "    to_return = ''\n",
    "    for i in range(3): \n",
    "        if state[i] == 1:\n",
    "            if to_return == '':\n",
    "                to_return = 'Danger in '\n",
    "            to_return += state_meaning[i] + ', '\n",
    "    if to_return == '': \n",
    "        to_return = 'No danger'\n",
    "    if state[4] == -1:\n",
    "        to_return += '. Cant turn left'\n",
    "    elif state[4] == 1:\n",
    "        to_return += '. Cant turn right'\n",
    "    return to_return\n",
    "    \n",
    "\n",
    "\n",
    "def decode_Q(Q): \n",
    "    \"\"\"\n",
    "    Function to decode the Q-values\n",
    "    \"\"\"\n",
    "    return {(decode_danger(key[0]), decode_meta_action(key[1])) : value for key, value in Q.items()}\n",
    "\n",
    "\n",
    "class Kinematics:\n",
    "    def __init__(self, \n",
    "                 sim_frequency=10,\n",
    "                 policy_frequency=2,\n",
    "                 render_mode='human',\n",
    "                 seed=None, \n",
    "                 state_type='danger',\n",
    "                 policy=None,\n",
    "                 colision_reward=-20, skew_speed=1, \n",
    "                 crop=100, lane_tolerance=2, danger_threshold_x=10, danger_threshold_y=10, x_speed_coef=1, y_speed_coef=1):\n",
    "            \"\"\"\n",
    "            Kinematics class constructor\n",
    "            \"\"\"\n",
    "\n",
    "            self.config = default_config.copy()\n",
    "            self.config[\"observation\"] =  {\n",
    "                \"type\": \"Kinematics\",\n",
    "                \"vehicles_count\": 50,\n",
    "                \"features\": [\"x\", \"y\", \"vx\", \"vy\"],\n",
    "                \"absolute\": False,\n",
    "                \"normalize\": False,\n",
    "            }\n",
    "            self.config[\"simulation_frequency\"] = sim_frequency\n",
    "            self.config[\"policy_frequency\"] = policy_frequency\n",
    "            self.seed = seed\n",
    "            self.policy = policy\n",
    "            self.colision_reward, self.skew_speed = colision_reward, skew_speed\n",
    "\n",
    "            with gym.make('highway-v0', render_mode=render_mode, config=self.config) as env:\n",
    "                self.env = env\n",
    "                obs, info = env.reset(seed = self.seed)\n",
    "                self.current_obs = obs\n",
    "\n",
    "            self.state_type = state_type\n",
    "            self.crop, self.lane_tolerance, self.danger_threshold_x, self.danger_threshold_y, self.x_speed_coef, self.y_speed_coef = crop, lane_tolerance, danger_threshold_x, danger_threshold_y, x_speed_coef, y_speed_coef\n",
    "            self.initialize_states()\n",
    "\n",
    "    def initialize_states(self):\n",
    "        \"\"\"\n",
    "        Initialize the states of the occupancy grid\n",
    "        \"\"\"\n",
    "        # If the state type is danger, we need to initialize the states\n",
    "        if self.state_type == 'danger':\n",
    "            # The states will be the possible combinations of 0s and 1s for the 4 features + {-1,0,1} for the lane \n",
    "            a = list(itertools.product([0, 1], repeat=4))\n",
    "            # Now we need to add the product of {-1,0,1}\n",
    "            a = list(itertools.product(a, [-1, 0, 1]))\n",
    "            flattened = [(*x, y) for x, y in a]\n",
    "            self.states = flattened\n",
    "\n",
    "        elif self.state_type == 'binned': \n",
    "            pass\n",
    "\n",
    "    def get_state(self, decode=False):\n",
    "        \"\"\"\n",
    "        Get the state of the environment\n",
    "        Arguments:\n",
    "            type: str, the type of state to get. Options are 'n_neighbours' or 'danger' :\n",
    "                n_neighbours: the state is the n closest neighbours in the form (x1,x2,...,xn), (y1,y2,...,yn)\n",
    "                danger: the state is an array with 4 binary variables representing whether there is danger ahead, behind, on the left or right lanes\n",
    "            decode: bool, whether to decode the state\n",
    "        \"\"\"\n",
    "        assert self.state_type in ['n_neighbours', 'danger'], \"The type of state must be either 'n_neighbours' or 'lane-wise'\"\n",
    "        if self.state_type == 'n_neighbours':\n",
    "            state = self.state_n_neighbours()\n",
    "        elif self.state_type == 'danger':\n",
    "            state = self.state_danger()\n",
    "        return state\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------------------------------\n",
    "    def get_n_closest(self):\n",
    "        \"\"\"\n",
    "        Get the n closest cars to the agent\n",
    "        Returns:\n",
    "            closest_car_positions: np.array, the positions of the n closest cars to the agent. If there are less than n_closest cars, the array is padded with the crop_dist values\n",
    "        \"\"\"\n",
    "\n",
    "        car_positions = self.get_car_positions()\n",
    "        distances = np.linalg.norm(car_positions, axis=1)\n",
    "\n",
    "        # Remove the agent position\n",
    "        closest = np.argsort(distances)[1:self.n_closest+1]\n",
    "        closest_car_positions = car_positions[closest]\n",
    "\n",
    "        # If there are less than n_closest cars, pad the array with the crop_dist values\n",
    "        if len(closest_car_positions) < self.n_closest:\n",
    "            n_missing = self.n_closest - len(closest_car_positions)\n",
    "            closest_car_positions = np.pad(closest_car_positions, ((0, n_missing), (0,0)), 'constant', constant_values=(self.crop_dist[0][0], self.crop_dist[1][0]))\n",
    "\n",
    "        # Values that are \n",
    "        return closest_car_positions\n",
    "    \n",
    "\n",
    "    def state_n_neighbours(self):\n",
    "        \"\"\"\n",
    "        Get the state of the environment in a neighbour-wise manner\n",
    "        Returns:\n",
    "            state: tuple, the state of the environment in the form (x1,x2,...,xn), (y1,y2,...,yn)\n",
    "        \"\"\"\n",
    "        n_closest = self.get_n_closest()\n",
    "        # For the closest cars, get the state\n",
    "        state_x, state_y = [], []\n",
    "        # Get the bin values for each of the x,y positions, and return a tuple with the values\n",
    "        for car in n_closest:\n",
    "            x = np.digitize(car[0], self.x_bins) - 1\n",
    "            y = np.digitize(car[1], self.y_bins) - 1\n",
    "            x_val, y_val = self.x_bins[x], self.y_bins[y]\n",
    "            state_x.append(x_val)\n",
    "            state_y.append(y_val)\n",
    "        state = (tuple(state_x), tuple(state_y))\n",
    "        return tuple(state)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def state_danger(self):\n",
    "\n",
    "        global values_x\n",
    "        global values_y\n",
    "        global turn_possibility\n",
    "        \"\"\"\n",
    "        Get the state of the environment\n",
    "        \"\"\"\n",
    "        def get_sign(num): \n",
    "            sign = num/np.abs(num)\n",
    "            return sign\n",
    "\n",
    "        lane = self.current_obs[0,1]\n",
    "        observation = self.current_obs[1:][:,0:4]\n",
    "        observation = observation[~np.all(observation == 0, axis=1)]\n",
    "\n",
    "        # The bin 0 indicates (0, 8], which is the safety distance\n",
    "        # bins =  [[0,8,self.crop], [0,8,self.crop], [0,8,self.crop], [0,8,self.crop]] if bins is None else bins\n",
    "        # bins =  [[0,10], [0,10], [0,10], [0,10]] if bins is None else bins\n",
    "\n",
    "        # Lane observations\n",
    "        same_lane = observation[np.abs(observation[:,1]) <= self.lane_tolerance]\n",
    "        lane_front = same_lane[(same_lane[:,0] > 0)]\n",
    "        lane_back = same_lane[(same_lane[:,0] < 0)]\n",
    "\n",
    "        # For the left and right lanes we consider 2 lanes, instead of just one \n",
    "        left_lanes = observation[(observation[:,1] >= -8 - self.lane_tolerance) & (observation[:,1] <= -4 + self.lane_tolerance)]\n",
    "        right_lanes = observation[(observation[:,1] <= 8 + self.lane_tolerance) & (observation[:,1] >= 4 - self.lane_tolerance)]\n",
    "\n",
    "        # Calculating the adjusted distances\n",
    "        front_dist = lane_front[0,0] if len(lane_front) > 0 else self.crop \n",
    "        front_speed_diff = lane_front[0,2] if len(lane_front) > 0 else 0\n",
    "        front_adj_dist = front_dist + self.x_speed_coef*front_speed_diff   # The more the front speed diff, the harder it is to get to the car in the front, so adjusted distance is higher\n",
    "\n",
    "        back_dist = -lane_back[0,0] if len(lane_back) > 0 else self.crop\n",
    "        back_speed_diff = lane_back[0,2] - 5 if len(lane_back) > 0 else 0\n",
    "        back_adj_dist = back_dist - self.x_speed_coef*back_speed_diff    # The faster the car in the back is driving, the more dangerous it is, so the adjusted distance is lower\n",
    "\n",
    "        left_signs = [get_sign(left_lanes[i,0]) for i in range(len(left_lanes))]\n",
    "        left_adj_dists = (np.abs(left_lanes[:,1])-4)*1.5 + left_lanes[:,0] + self.x_speed_coef*left_lanes[:,2]*left_signs - self.y_speed_coef*left_lanes[:,3]\n",
    "        left_adj_dist = np.min(left_adj_dists) if len(left_adj_dists) > 0 else self.crop\n",
    "\n",
    "        right_signs = [get_sign(right_lanes[i,0]) for i in range(len(right_lanes))]\n",
    "        right_adj_dists = (np.abs(right_lanes[:,1])-4)*1.5 + right_lanes[:,0] + self.x_speed_coef*right_lanes[:,2]*right_signs + self.y_speed_coef*right_lanes[:,3]\n",
    "        right_adj_dist = np.min(right_adj_dists) if len(right_adj_dists) > 0 else self.crop\n",
    "\n",
    "        turn_possibility = -1 if lane < 2 else 1 if lane > 34 else 0\n",
    "        values = np.array([front_adj_dist, back_adj_dist, left_adj_dist, right_adj_dist])\n",
    "\n",
    "        # print('------------------/-----------------')\n",
    "        # print(right_lanes)\n",
    "        # print(right_adj_dists)\n",
    "        # print(values)\n",
    "\n",
    "        # Use the danger threshold to make 0 or 1 \n",
    "        values_x = np.where(values[:2] < self.danger_threshold_x, 1, 0)\n",
    "        values_y = np.where(values[2:] < self.danger_threshold_y, 1, 0)\n",
    "\n",
    "        values = np.append(values_x, values_y)\n",
    "        values = np.append(values, turn_possibility)\n",
    "        return tuple(values)\n",
    "    \n",
    "    @staticmethod\n",
    "    def bin_values(values, bins, digitize=False):\n",
    "        \"\"\"\n",
    "        Function to bin the values\n",
    "        Arguments:\n",
    "            values: np.array, the values to bin\n",
    "            bins: list, the bins to use\n",
    "                Example: bins = [[5,10,15,30], [5,10,30], [8,14,20], [8,14,20]], for x,y,vx,vy\n",
    "            digitize: bool, whether to digitize the values. If set to False, the values will be returned as the bin index\n",
    "        Returns:\n",
    "            binned_values: np.array, the binned values\n",
    "        \"\"\"\n",
    "        # Check if there are as much bins as values\n",
    "        if len(values.shape) == 1:\n",
    "            assert len(bins) == len(values), \"The number of bins must be equal to the number of values\"\n",
    "            binned_values = []\n",
    "            if digitize:\n",
    "                binned_values = [np.digitize(values[i], bins[i]) for i in range(len(bins))]\n",
    "                return np.array(binned_values)\n",
    "\n",
    "            binned_values = [bins[i][np.digitize(values[i], bins[i])-1] for i in range(len(bins))]\n",
    "            return np.array(binned_values)\n",
    "\n",
    "        # For a NxM matrix, with N>1\n",
    "        assert len(bins) == values.shape[1], \"The number of bins must be equal to the number of values\"\n",
    "        binned_values = []\n",
    "        for i in range(len(values)):\n",
    "            if digitize:\n",
    "                binned_values.append([np.digitize(values[i,j], bins[j]) for j in range(len(bins))])\n",
    "            else:\n",
    "                binned_values.append([bins[j][np.digitize(values[i,j], bins[j])-1] for j in range(len(bins))])\n",
    "        return np.array(binned_values)\n",
    "    \n",
    "    def test_env(self, sleep_time=1):\n",
    "        \"\"\"\n",
    "        Function to test the environment with a random policy, or with a policy\n",
    "        \"\"\" \n",
    "        obs, info = self.env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        while not done:\n",
    "            # start = time.time()\n",
    "            if self.policy is None:\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                action = self.policy()\n",
    "            obs, reward, done, truncate, info = self.env.step(action)\n",
    "            self.current_obs = obs\n",
    "            print(self.get_state(), decode_meta_action(action), reward, fix_reward(reward, skew_speed=self.skew_speed, colision_reward=self.colision_reward))\n",
    "            time.sleep(sleep_time)\n",
    "            # end = time.time()\n",
    "            # print(f\"Time taken: {end-start}\")\n",
    "        self.env.close()\n",
    "        \n",
    "class Algortihm(Kinematics):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha=0.75,\n",
    "        gamma=0.95,\n",
    "        epsilon=0.6,\n",
    "        epsilon_decay=1, min_epsilon=0.05,\n",
    "        print_stats=False,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Algorithm class constructor\n",
    "        Arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            epsilon_decay: float, the decay value for epsilon. If set to 1, the epsilon will not decay\n",
    "            min_epsilon: float, the minimum value for epsilon. If the epsilon is lower than this value, it will not decay\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.initialize_Q(print_stats)\n",
    "        self.alpha, self.gamma, self.epsilon = alpha, gamma, epsilon\n",
    "        self.epsilon_decay, self.min_epsilon = epsilon_decay, min_epsilon\n",
    "        self.Q_stats = self.Q.copy()\n",
    "        self.rewards_hist = []\n",
    "\n",
    "    def policy_Q(self, state):\n",
    "        values = [self.Q[(state, action)] for action in range(5)]\n",
    "        return np.argmax(values)   \n",
    "    \n",
    "    def initialize_Q(self, print_stats = False):\n",
    "        # Combine the possible states with the possible actions\n",
    "        keys = list(itertools.product(self.states, range(5)))       # 5 possible actions, 0-4: left, idle, right, accelerate, decelerate\n",
    "        if len(keys) > 150000:\n",
    "            print(\"Warning: The number of states is too large, consider reducing the number of states\")\n",
    "        if print_stats:\n",
    "            print(f\"Number of states: {len(keys)}\")   \n",
    "        self.Q = {key: 0 for key in keys}\n",
    "\n",
    "    def epsilon_greedy(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(5)\n",
    "        else:\n",
    "            values = [self.Q[(state, action)] for action in range(5)]\n",
    "            return np.argmax(values)\n",
    "\n",
    "    def decay_epsilon(self, episode):\n",
    "        if self.epsilon > self.min_epsilon:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def test(self, sleep_time=1):\n",
    "        with gym.make('highway-v0', render_mode='human', config=self.config) as env:\n",
    "            obs, info = env.reset(seed = self.seed)\n",
    "            self.current_obs = obs\n",
    "            done = False\n",
    "            state = self.get_state()\n",
    "            action = self.policy_Q(state)\n",
    "            while not done:\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                next_state = self.get_state()\n",
    "                next_action = self.policy_Q(next_state)\n",
    "                state, action = next_state, next_action\n",
    "                self.current_obs = next_obs\n",
    "                print(state, decode_meta_action(action), next_state)\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "    def get_state_visits(self):\n",
    "        state_visits = {state: np.sum([self.Q_stats[(state, action)] for action in range(5)]) for state in self.states}\n",
    "        state_visits = {k:100*v/np.sum(list(state_visits.values())) for k, v in sorted(state_visits.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return state_visits\n",
    "\n",
    "    def plot_rewards_history(self):\n",
    "        plt.plot(self.rewards_hist)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Cumulative reward')\n",
    "        plt.title('Cumulative reward per episode')\n",
    "        plt.show()\n",
    "\n",
    "    def search_Q(self, state):\n",
    "        assert len(state) == 5, \"The state must be a tuple of 5 values\"\n",
    "        Q_vals = {decode_meta_action(a) : self.Q[state, a] for a in range(5)}\n",
    "        # Order a \n",
    "        Q_vals = {k: v for k, v in sorted(Q_vals.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return Q_vals\n",
    "    \n",
    "\n",
    "\n",
    "class Sarsa(Algortihm):\n",
    "    def __init__(\n",
    "        self,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        SARSA class constructor\n",
    "        Arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            m: int, the number of episodes to train the agent for\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def train(self, m=100, verbose=0): \n",
    "        env = gym.make('highway-v0', render_mode=None, config=self.config)\n",
    "        obs, info = env.reset(seed = np.random.randint(1000))\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        state = self.get_state()\n",
    "        action = self.epsilon_greedy(state)\n",
    "        q_explored = np.count_nonzero(list(self.Q.values()))\n",
    "        for i in tqdm(range(m)):\n",
    "            last_state = None\n",
    "            env.reset(seed = np.random.randint(1000))\n",
    "            cum_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                reward = fix_reward(reward, done=done, colision_reward=self.colision_reward, skew_speed=self.skew_speed)\n",
    "                next_state = self.get_state()\n",
    "                next_action = self.epsilon_greedy(next_state)\n",
    "                if done:\n",
    "                    last_state = state\n",
    "\n",
    "                print(next_state, decode_meta_action(next_action), reward) if verbose > 2 else None\n",
    "                if self.Q[(state, action)] == 0:\n",
    "                    q_explored += 1\n",
    "                cum_reward += reward\n",
    "\n",
    "                self.Q[(state, action)] += self.alpha*(reward + self.gamma*self.Q[(next_state, next_action)] - self.Q[(state, action)])\n",
    "                self.Q_stats[(state, action)] += 1\n",
    "                state, action = next_state, next_action\n",
    "                self.current_obs = next_obs\n",
    "            self.rewards_hist.append(cum_reward)\n",
    "            print(f\"Episode {i+1} completed on state {last_state} with cumulative reward: {cum_reward}\") if verbose > 0 else None\n",
    "            print(f\"Q explored: {100*q_explored/len(self.Q)}\") if verbose > 1 else None\n",
    "            self.decay_epsilon(i)\n",
    "        env.close()\n",
    "    \n",
    "\n",
    "class Q_learning(Algortihm):\n",
    "    def __init__(\n",
    "        self,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Q-learning class constructor\n",
    "        Arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            m: int, the number of episodes to train the agent for\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def train(self, m=100, verbose=0):\n",
    "        env = gym.make('highway-v0', render_mode=None, config=self.config)\n",
    "        obs, info = env.reset(seed = np.random.randint(1000))\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        state = self.get_state()\n",
    "        q_explored = np.count_nonzero(list(self.Q.values()))\n",
    "        for i in tqdm(range(m)):\n",
    "            last_state = None\n",
    "            env.reset(seed = np.random.randint(1000))\n",
    "            cum_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.epsilon_greedy(state)\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                reward = fix_reward(reward, done=done, colision_reward=self.colision_reward, skew_speed=self.skew_speed)\n",
    "                next_state = self.get_state()\n",
    "                if done:\n",
    "                    last_state = state\n",
    "\n",
    "                print(next_state, decode_meta_action(action), reward) if verbose > 2 else None\n",
    "                if self.Q[(state, action)] == 0:\n",
    "                    q_explored += 1\n",
    "                cum_reward += reward\n",
    "\n",
    "                self.Q[(state, action)] += self.alpha*(reward + self.gamma*np.max([self.Q[(next_state, a)] for a in range(5)]) - self.Q[(state, action)])\n",
    "                self.Q_stats[(state, action)] += 1\n",
    "                state = next_state\n",
    "                self.current_obs = next_obs\n",
    "            self.rewards_hist.append(cum_reward)\n",
    "            print(f\"Episode {i+1} completed on state {last_state} with cumulative reward: {cum_reward}\") if verbose > 0 else None\n",
    "            print(f\"Q explored: {100*q_explored/len(self.Q)}\") if verbose > 1 else None\n",
    "            self.decay_epsilon(i)\n",
    "        env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 240\n"
     ]
    }
   ],
   "source": [
    "sar = Sarsa(print_stats=True, \n",
    "        epsilon=0.2, \n",
    "        alpha=0.1, \n",
    "        gamma=0.98, \n",
    "        state_type='danger', \n",
    "        policy_frequency=2, \n",
    "        sim_frequency=10, \n",
    "        colision_reward=-5, \n",
    "        skew_speed=1, \n",
    "        danger_threshold_x=10,\n",
    "        danger_threshold_y=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 240\n"
     ]
    }
   ],
   "source": [
    "Q = Q_learning(print_stats=True, \n",
    "        epsilon=0.9, \n",
    "        epsilon_decay=0.98,\n",
    "        min_epsilon=0.05,\n",
    "        alpha=0.1, \n",
    "        gamma=0.98, \n",
    "        state_type='danger', \n",
    "        policy_frequency=2, \n",
    "        sim_frequency=10, \n",
    "        colision_reward=-10, \n",
    "        skew_speed=1, \n",
    "        danger_threshold_x=15,\n",
    "        danger_threshold_y=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331c84cec9474f9ab086f1bb3e12ca54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 completed on state (1, 0, 0, 1, 0) with cumulative reward: -4.819779680638334\n",
      "Q explored: 2.9166666666666665\n",
      "Episode 2 completed on state (1, 0, 0, 0, 1) with cumulative reward: 21.213501023977948\n",
      "Q explored: 10.0\n",
      "Episode 3 completed on state (1, 0, 0, 1, 0) with cumulative reward: 18.331208453046436\n",
      "Q explored: 13.333333333333334\n",
      "Episode 4 completed on state (0, 0, 1, 0, 0) with cumulative reward: 4.182063792029343\n",
      "Q explored: 13.333333333333334\n",
      "Episode 5 completed on state (1, 0, 0, 0, 1) with cumulative reward: 11.918207368125156\n",
      "Q explored: 15.0\n",
      "Episode 6 completed on state (0, 0, 0, 1, 0) with cumulative reward: -8.343256215286061\n",
      "Q explored: 15.0\n",
      "Episode 7 completed on state (0, 0, 1, 0, 0) with cumulative reward: -2.3558321984965813\n",
      "Q explored: 15.416666666666666\n",
      "Episode 8 completed on state (1, 0, 1, 1, 0) with cumulative reward: 21.968544931131948\n",
      "Q explored: 16.25\n",
      "Episode 9 completed on state (0, 0, 0, 1, 0) with cumulative reward: 4.947870009894521\n",
      "Q explored: 17.083333333333332\n",
      "Episode 10 completed on state (1, 0, 0, 0, 1) with cumulative reward: 24.874549191512422\n",
      "Q explored: 17.5\n",
      "Episode 11 completed on state (0, 0, 0, 1, 0) with cumulative reward: 3.7595763236751267\n",
      "Q explored: 18.75\n",
      "Episode 12 completed on state (1, 0, 0, 0, 1) with cumulative reward: -5.793825118463429\n",
      "Q explored: 19.583333333333332\n",
      "Episode 13 completed on state (0, 0, 1, 1, 0) with cumulative reward: -4.9607440137386485\n",
      "Q explored: 20.416666666666668\n",
      "Episode 14 completed on state (0, 0, 1, 1, 0) with cumulative reward: -4.692585214916851\n",
      "Q explored: 20.833333333333332\n",
      "Episode 15 completed on state (0, 0, 1, 0, 0) with cumulative reward: 51.08439065557594\n",
      "Q explored: 23.333333333333332\n",
      "Episode 16 completed on state (0, 0, 0, 1, 0) with cumulative reward: 4.022096846523935\n",
      "Q explored: 25.0\n",
      "Episode 17 completed on state (0, 0, 1, 0, 0) with cumulative reward: -7.784866058562413\n",
      "Q explored: 25.0\n",
      "Episode 18 completed on state (1, 0, 0, 1, 0) with cumulative reward: 7.229864759686929\n",
      "Q explored: 25.0\n",
      "Episode 19 completed on state (0, 0, 1, 1, 0) with cumulative reward: -1.4782568728139562\n",
      "Q explored: 25.0\n",
      "Episode 20 completed on state (0, 0, 1, 1, 0) with cumulative reward: 9.198538656731248\n",
      "Q explored: 25.0\n",
      "Episode 21 completed on state (1, 0, 0, 0, 0) with cumulative reward: -8.359418946244745\n",
      "Q explored: 25.0\n",
      "Episode 22 completed on state (1, 0, 1, 0, 0) with cumulative reward: 16.977496298882436\n",
      "Q explored: 25.416666666666668\n",
      "Episode 23 completed on state (0, 0, 1, 0, 0) with cumulative reward: -6.331388422872754\n",
      "Q explored: 25.416666666666668\n",
      "Episode 24 completed on state (0, 0, 1, 1, 0) with cumulative reward: 2.2750883388004404\n",
      "Q explored: 25.416666666666668\n",
      "Episode 25 completed on state (1, 0, 1, 1, 0) with cumulative reward: 41.328148185522224\n",
      "Q explored: 25.833333333333332\n",
      "Episode 26 completed on state (1, 0, 0, 0, 0) with cumulative reward: 7.267150999001242\n",
      "Q explored: 25.833333333333332\n",
      "Episode 27 completed on state (1, 0, 0, 0, 0) with cumulative reward: -3.0963309365661615\n",
      "Q explored: 25.833333333333332\n",
      "Episode 28 completed on state (1, 0, 0, 1, 0) with cumulative reward: 37.439625811352954\n",
      "Q explored: 26.25\n",
      "Episode 29 completed on state (0, 0, 1, 1, 0) with cumulative reward: 6.609191142777419\n",
      "Q explored: 27.083333333333332\n",
      "Episode 30 completed on state (1, 0, 0, 1, 0) with cumulative reward: 49.84397084040772\n",
      "Q explored: 28.333333333333332\n",
      "Episode 31 completed on state (1, 0, 0, 0, 1) with cumulative reward: 19.908739198005968\n",
      "Q explored: 28.333333333333332\n",
      "Episode 32 completed on state (0, 0, 1, 0, 0) with cumulative reward: 18.45637876945654\n",
      "Q explored: 29.166666666666668\n",
      "Episode 33 completed on state (0, 0, 1, 0, 0) with cumulative reward: 8.01590947685165\n",
      "Q explored: 29.166666666666668\n",
      "Episode 34 completed on state (1, 0, 0, 0, 0) with cumulative reward: -2.443048585798773\n",
      "Q explored: 29.166666666666668\n",
      "Episode 35 completed on state (1, 0, 1, 0, 0) with cumulative reward: 3.2802921521427315\n",
      "Q explored: 29.166666666666668\n",
      "Episode 36 completed on state (1, 0, 0, 0, 0) with cumulative reward: 21.938425361636664\n",
      "Q explored: 29.583333333333332\n",
      "Episode 37 completed on state (0, 0, 1, 0, 0) with cumulative reward: 11.028520714322983\n",
      "Q explored: 29.583333333333332\n",
      "Episode 38 completed on state (0, 0, 1, 1, 0) with cumulative reward: 12.891102844428797\n",
      "Q explored: 29.583333333333332\n",
      "Episode 39 completed on state (0, 0, 0, 1, 0) with cumulative reward: 7.238059534610738\n",
      "Q explored: 29.583333333333332\n",
      "Episode 40 completed on state (0, 0, 1, 0, 0) with cumulative reward: 36.33411654843546\n",
      "Q explored: 30.0\n",
      "Episode 41 completed on state (1, 0, 0, 0, 1) with cumulative reward: 6.50788489723967\n",
      "Q explored: 30.416666666666668\n",
      "Episode 42 completed on state (0, 0, 1, 0, 1) with cumulative reward: 7.486930715189811\n",
      "Q explored: 30.416666666666668\n",
      "Episode 43 completed on state (1, 0, 0, 0, 0) with cumulative reward: 4.699345887107535\n",
      "Q explored: 30.416666666666668\n",
      "Episode 44 completed on state (0, 0, 0, 1, 0) with cumulative reward: 11.581027602756247\n",
      "Q explored: 30.416666666666668\n",
      "Episode 45 completed on state (0, 0, 0, 0, 1) with cumulative reward: -9.249930590260425\n",
      "Q explored: 30.416666666666668\n",
      "Episode 46 completed on state (1, 0, 0, 0, 0) with cumulative reward: -8.649428115160527\n",
      "Q explored: 30.416666666666668\n",
      "Episode 47 completed on state (0, 0, 1, 0, 0) with cumulative reward: 4.753411172088546\n",
      "Q explored: 30.416666666666668\n",
      "Episode 48 completed on state (1, 0, 1, 1, 0) with cumulative reward: 47.595795609521055\n",
      "Q explored: 30.416666666666668\n",
      "Episode 49 completed on state (0, 0, 1, 0, 0) with cumulative reward: -6.679122209710378\n",
      "Q explored: 30.416666666666668\n",
      "Episode 50 completed on state (1, 0, 0, 0, 1) with cumulative reward: 64.7882940682507\n",
      "Q explored: 30.833333333333332\n",
      "Episode 51 completed on state (1, 0, 1, 1, 0) with cumulative reward: 4.774552011505813\n",
      "Q explored: 31.25\n",
      "Episode 52 completed on state (1, 0, 0, 0, 0) with cumulative reward: -6.697792579402413\n",
      "Q explored: 31.25\n",
      "Episode 53 completed on state (1, 0, 0, 0, 0) with cumulative reward: 8.680378900825708\n",
      "Q explored: 31.25\n",
      "Episode 54 completed on state (0, 0, 0, 1, 0) with cumulative reward: -6.101391990741387\n",
      "Q explored: 31.25\n",
      "Episode 55 completed on state (0, 0, 1, 0, 0) with cumulative reward: -4.693676227836868\n",
      "Q explored: 31.25\n",
      "Episode 56 completed on state (1, 0, 0, 1, 0) with cumulative reward: -6.667335717572707\n",
      "Q explored: 31.25\n",
      "Episode 57 completed on state (0, 0, 1, 0, 0) with cumulative reward: -5.045093596037954\n",
      "Q explored: 31.25\n",
      "Episode 58 completed on state (0, 0, 0, 1, 0) with cumulative reward: -8.55218992706197\n",
      "Q explored: 31.25\n",
      "Episode 59 completed on state (1, 0, 1, 1, 0) with cumulative reward: 16.80666849756099\n",
      "Q explored: 31.25\n",
      "Episode 60 completed on state (1, 0, 0, 1, 0) with cumulative reward: 26.128973505956687\n",
      "Q explored: 31.25\n",
      "Episode 61 completed on state (1, 0, 1, 0, 0) with cumulative reward: 14.752707068011624\n",
      "Q explored: 31.25\n",
      "Episode 62 completed on state (1, 0, 1, 1, 0) with cumulative reward: -2.934114880691359\n",
      "Q explored: 31.25\n",
      "Episode 63 completed on state (1, 0, 0, 0, 0) with cumulative reward: 8.440281268946503\n",
      "Q explored: 31.25\n",
      "Episode 64 completed on state (1, 0, 0, 0, 0) with cumulative reward: -6.40731294457845\n",
      "Q explored: 31.25\n",
      "Episode 65 completed on state (1, 0, 0, 0, 0) with cumulative reward: -9.268183993074462\n",
      "Q explored: 31.25\n",
      "Episode 66 completed on state (0, 0, 1, 0, 0) with cumulative reward: -9.100585766787388\n",
      "Q explored: 31.25\n",
      "Episode 67 completed on state (0, 0, 1, 0, 0) with cumulative reward: 5.900278583909747\n",
      "Q explored: 31.25\n",
      "Episode 68 completed on state (0, 0, 1, 1, 0) with cumulative reward: -1.8142618672400666\n",
      "Q explored: 31.666666666666668\n",
      "Episode 69 completed on state (0, 0, 1, 1, 0) with cumulative reward: 31.34280939117121\n",
      "Q explored: 31.666666666666668\n",
      "Episode 70 completed on state (1, 0, 0, 1, 0) with cumulative reward: 25.68682361633939\n",
      "Q explored: 31.666666666666668\n",
      "Episode 71 completed on state (1, 0, 0, 0, 0) with cumulative reward: 53.79162307502807\n",
      "Q explored: 31.666666666666668\n",
      "Episode 72 completed on state (1, 0, 0, 0, -1) with cumulative reward: -5.07452401240682\n",
      "Q explored: 31.666666666666668\n",
      "Episode 73 completed on state (1, 0, 0, 0, 0) with cumulative reward: -7.771347948274542\n",
      "Q explored: 31.666666666666668\n",
      "Episode 74 completed on state (0, 0, 1, 0, 0) with cumulative reward: 2.0794733135735886\n",
      "Q explored: 31.666666666666668\n",
      "Episode 75 completed on state (0, 0, 1, 1, 0) with cumulative reward: 5.932196055937261\n",
      "Q explored: 32.5\n",
      "Episode 76 completed on state (1, 0, 0, 1, 0) with cumulative reward: 14.110364830466775\n",
      "Q explored: 32.5\n",
      "Episode 77 completed on state (0, 0, 1, 1, 0) with cumulative reward: 1.7166229714463555\n",
      "Q explored: 32.5\n",
      "Episode 78 completed on state (1, 0, 0, 1, 0) with cumulative reward: 3.4321850520452415\n",
      "Q explored: 32.5\n",
      "Episode 79 completed on state (0, 0, 0, 1, -1) with cumulative reward: 39.786903998028386\n",
      "Q explored: 32.5\n",
      "Episode 80 completed on state (1, 0, 0, 1, -1) with cumulative reward: 16.671939850144284\n",
      "Q explored: 32.916666666666664\n",
      "Episode 81 completed on state (0, 0, 1, 1, 0) with cumulative reward: 10.743366134620441\n",
      "Q explored: 32.916666666666664\n",
      "Episode 82 completed on state (0, 0, 0, 1, 0) with cumulative reward: 15.270004164907931\n",
      "Q explored: 32.916666666666664\n",
      "Episode 83 completed on state (1, 0, 1, 0, 1) with cumulative reward: 7.5948794610104855\n",
      "Q explored: 33.333333333333336\n",
      "Episode 84 completed on state (1, 0, 0, 1, -1) with cumulative reward: 18.29666762066839\n",
      "Q explored: 33.333333333333336\n",
      "Episode 85 completed on state (0, 0, 1, 1, 0) with cumulative reward: 9.366195626027846\n",
      "Q explored: 33.333333333333336\n",
      "Episode 86 completed on state (0, 0, 1, 0, 0) with cumulative reward: -0.7903240200644444\n",
      "Q explored: 33.333333333333336\n",
      "Episode 87 completed on state (1, 0, 0, 0, 0) with cumulative reward: 1.200170316740932\n",
      "Q explored: 33.333333333333336\n",
      "Episode 88 completed on state (1, 0, 0, 1, 0) with cumulative reward: -8.157552102347903\n",
      "Q explored: 33.333333333333336\n",
      "Episode 89 completed on state (1, 0, 1, 0, 0) with cumulative reward: 35.93385958560232\n",
      "Q explored: 33.333333333333336\n",
      "Episode 90 completed on state (1, 0, 0, 0, 1) with cumulative reward: 1.51395717552845\n",
      "Q explored: 33.333333333333336\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[108], line 463\u001b[0m, in \u001b[0;36mQ_learning.train\u001b[1;34m(self, m, verbose)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m    462\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_greedy(state)\n\u001b[1;32m--> 463\u001b[0m     next_obs, reward, done, truncate, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m     reward \u001b[38;5;241m=\u001b[39m fix_reward(reward, done\u001b[38;5;241m=\u001b[39mdone, colision_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolision_reward, skew_speed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskew_speed)\n\u001b[0;32m    465\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:257\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_control\u001b[39m\u001b[38;5;124m\"\u001b[39m] \\\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:324\u001b[0m, in \u001b[0;36mRoad.act\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decide the actions of each entity on the road.\"\"\"\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[1;32m--> 324\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:100\u001b[0m, in \u001b[0;36mIDMVehicle.act\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     97\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Longitudinal: IDM\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m front_vehicle, rear_vehicle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneighbour_vehicles\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlane_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macceleration(ego_vehicle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    102\u001b[0m                                            front_vehicle\u001b[38;5;241m=\u001b[39mfront_vehicle,\n\u001b[0;32m    103\u001b[0m                                            rear_vehicle\u001b[38;5;241m=\u001b[39mrear_vehicle)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# When changing lane, check both current and target lanes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:361\u001b[0m, in \u001b[0;36mRoad.neighbour_vehicles\u001b[1;34m(self, vehicle, lane_index)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vehicle \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, Landmark):  \u001b[38;5;66;03m# self.network.is_connected_road(v.lane_index,\u001b[39;00m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;66;03m# lane_index, same_lane=True):\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m         s_v, lat_v \u001b[38;5;241m=\u001b[39m \u001b[43mlane\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lane\u001b[38;5;241m.\u001b[39mon_lane(v\u001b[38;5;241m.\u001b[39mposition, s_v, lat_v, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    363\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:189\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlocal_coordinates\u001b[39m(\u001b[38;5;28mself\u001b[39m, position: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    188\u001b[0m     delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[1;32m--> 189\u001b[0m     longitudinal \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m     lateral \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection_lateral)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Q.train(m=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Q.plot_rewards_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LANE_RIGHT': 2.4421872756646277,\n",
       " 'IDLE': 0.4002800548128033,\n",
       " 'LANE_LEFT': 0.37453475972036876,\n",
       " 'FASTER': 0.3195998234442916,\n",
       " 'SLOWER': -0.8487285128203023}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.search_Q((1, 0, 1, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, -1) LANE_RIGHT (0, 0, 0, 0, -1)\n",
      "(0, 0, 0, 0, -1) LANE_RIGHT (0, 0, 0, 0, -1)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(1, 0, 0, 0, 0) LANE_LEFT (1, 0, 0, 0, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 1, -1) IDLE (0, 0, 0, 1, -1)\n",
      "(0, 0, 0, 0, -1) LANE_RIGHT (0, 0, 0, 0, -1)\n",
      "(0, 0, 0, 1, -1) IDLE (0, 0, 0, 1, -1)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(1, 0, 0, 0, 0) LANE_LEFT (1, 0, 0, 0, 0)\n",
      "(1, 0, 0, 0, 0) LANE_LEFT (1, 0, 0, 0, 0)\n",
      "(0, 0, 0, 1, -1) IDLE (0, 0, 0, 1, -1)\n",
      "(0, 0, 0, 1, -1) IDLE (0, 0, 0, 1, -1)\n",
      "(0, 0, 0, 1, -1) IDLE (0, 0, 0, 1, -1)\n",
      "(0, 0, 0, 1, -1) IDLE (0, 0, 0, 1, -1)\n",
      "(0, 0, 0, 1, -1) IDLE (0, 0, 0, 1, -1)\n",
      "(0, 0, 0, 1, -1) IDLE (0, 0, 0, 1, -1)\n",
      "(0, 0, 0, 1, -1) IDLE (0, 0, 0, 1, -1)\n",
      "(0, 0, 0, 0, -1) LANE_RIGHT (0, 0, 0, 0, -1)\n",
      "(0, 0, 0, 0, -1) LANE_RIGHT (0, 0, 0, 0, -1)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT (0, 0, 1, 0, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT (0, 0, 1, 0, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT (0, 0, 1, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT (0, 0, 1, 0, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT (0, 0, 1, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(1, 0, 1, 1, 0) LANE_RIGHT (1, 0, 1, 1, 0)\n",
      "(1, 0, 1, 0, 0) IDLE (1, 0, 1, 0, 0)\n",
      "(1, 0, 1, 1, 0) LANE_RIGHT (1, 0, 1, 1, 0)\n",
      "(1, 0, 1, 0, 0) IDLE (1, 0, 1, 0, 0)\n",
      "(0, 0, 1, 1, 0) IDLE (0, 0, 1, 1, 0)\n",
      "(0, 0, 1, 1, 0) IDLE (0, 0, 1, 1, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT (0, 0, 1, 0, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT (0, 0, 1, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT (0, 0, 1, 0, 0)\n",
      "(1, 0, 0, 0, 0) LANE_LEFT (1, 0, 0, 0, 0)\n",
      "(1, 0, 0, 0, 0) LANE_LEFT (1, 0, 0, 0, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(1, 0, 1, 0, 0) IDLE (1, 0, 1, 0, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT (0, 0, 1, 0, 0)\n",
      "(0, 0, 1, 1, 0) IDLE (0, 0, 1, 1, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT (0, 0, 1, 0, 0)\n",
      "(0, 0, 1, 0, 0) LANE_RIGHT (0, 0, 1, 0, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 1, 0) LANE_LEFT (0, 0, 0, 1, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, -1) LANE_RIGHT (0, 0, 0, 0, -1)\n",
      "(0, 0, 0, 0, -1) LANE_RIGHT (0, 0, 0, 0, -1)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0) FASTER (0, 0, 0, 0, 0)\n",
      "(0, 0, 1, 1, 0) IDLE (0, 0, 1, 1, 0)\n",
      "(1, 0, 1, 1, 0) LANE_RIGHT (1, 0, 1, 1, 0)\n",
      "(1, 0, 1, 1, 0) LANE_RIGHT (1, 0, 1, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "Q.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c70f7934774664b7b172d33b92bc57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 completed on state (1, 0, 0, 1, 0) with cumulative reward: 55.83995013308234\n",
      "Q explored: 35.416666666666664\n",
      "Episode 2 completed on state (0, 0, 1, 1, 0) with cumulative reward: -4.100641294579048\n",
      "Q explored: 35.416666666666664\n",
      "Episode 3 completed on state (0, 0, 1, 0, 0) with cumulative reward: 154.7079343001195\n",
      "Q explored: 35.416666666666664\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[38], line 344\u001b[0m, in \u001b[0;36mSarsa.train\u001b[1;34m(self, m, verbose)\u001b[0m\n\u001b[0;32m    342\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m--> 344\u001b[0m     next_obs, reward, done, truncate, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m     reward \u001b[38;5;241m=\u001b[39m fix_reward(reward, done\u001b[38;5;241m=\u001b[39mdone, colision_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolision_reward, skew_speed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskew_speed)\n\u001b[0;32m    346\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:257\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_control\u001b[39m\u001b[38;5;124m\"\u001b[39m] \\\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:324\u001b[0m, in \u001b[0;36mRoad.act\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decide the actions of each entity on the road.\"\"\"\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[1;32m--> 324\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:100\u001b[0m, in \u001b[0;36mIDMVehicle.act\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     97\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Longitudinal: IDM\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m front_vehicle, rear_vehicle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneighbour_vehicles\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlane_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macceleration(ego_vehicle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    102\u001b[0m                                            front_vehicle\u001b[38;5;241m=\u001b[39mfront_vehicle,\n\u001b[0;32m    103\u001b[0m                                            rear_vehicle\u001b[38;5;241m=\u001b[39mrear_vehicle)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# When changing lane, check both current and target lanes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:361\u001b[0m, in \u001b[0;36mRoad.neighbour_vehicles\u001b[1;34m(self, vehicle, lane_index)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vehicle \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, Landmark):  \u001b[38;5;66;03m# self.network.is_connected_road(v.lane_index,\u001b[39;00m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;66;03m# lane_index, same_lane=True):\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m         s_v, lat_v \u001b[38;5;241m=\u001b[39m \u001b[43mlane\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lane\u001b[38;5;241m.\u001b[39mon_lane(v\u001b[38;5;241m.\u001b[39mposition, s_v, lat_v, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    363\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:190\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    188\u001b[0m delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m    189\u001b[0m longitudinal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection)\n\u001b[1;32m--> 190\u001b[0m lateral \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sar.train(m=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar.Q = Q_save.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_save = sar.Q.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FASTER': 5.133641775043947,\n",
       " 'LANE_RIGHT': 1.8789156890532015,\n",
       " 'SLOWER': 1.7437783264494309,\n",
       " 'IDLE': 0.5959434836555192,\n",
       " 'LANE_LEFT': -1.1294429580928238}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sar.search_Q((1, 0, 0, 1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 0, 0, 0)\n",
      "(0, 0, 1, 0, 0)\n",
      "(0, 0, 1, 0, 0)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 0, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(0, 0, 1, 0, 1)\n",
      "(1, 0, 1, 0, 1)\n",
      "(1, 0, 1, 0, 1)\n"
     ]
    }
   ],
   "source": [
    "sar.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation testing kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.array([[111.21628   ,  28.534393  ,  24.951689  ,  -1.5534488 ],\n",
    "         [  7.215894  , -28.534393  ,  -3.1784813 ,   1.5534488 ],\n",
    "         [ 13.563559  ,  -9.321229  ,  -8.850195  ,   3.4367235 ],\n",
    "         [ 26.42035   , -14.356143  ,  -3.1136448 ,  -3.102817  ],\n",
    "         [ 32.715305  ,  -8.534393  ,  -8.347136  ,   1.5534488 ],\n",
    "         [ 47.327545  ,  -4.534394  ,  -1.7440301 ,   1.5534488 ],\n",
    "         [ 53.866665  ,  -8.534393  ,  -3.6438825 ,   1.5534488 ],\n",
    "         [ 63.812023  , -21.71256   ,  -6.453715  ,   4.5213614 ],\n",
    "         [ 74.69441   ,   6.18834   ,  -7.2160125 ,   2.12416   ],\n",
    "         [ 83.25759   , -26.404202  ,  -6.699316  ,  -2.9349995 ],\n",
    "         [ 91.63243   ,   6.007195  ,  -8.328622  ,  -3.1048355 ],\n",
    "         [102.30845   , -14.389156  ,  -5.285321  ,  -3.018829  ],\n",
    "         [113.73761   , -24.534393  ,  -3.4926977 ,   1.5534488 ],\n",
    "         [120.51923   ,   7.066979  ,  -9.3168545 ,   2.4114895 ],\n",
    "         [132.30734   ,   4.4144945 ,  -5.9441433 ,  -0.90478116],\n",
    "         [145.12323   , -12.534393  ,  -2.4250896 ,   1.5534488 ],\n",
    "         [155.93086   ,   7.465606  ,  -2.768306  ,   1.5534488 ],\n",
    "         [164.46223   ,  -4.3472805 ,  -7.0121    ,   1.1194158 ],\n",
    "         [178.45152   ,   3.4656062 ,  -2.5009947 ,   1.5534488 ],\n",
    "         [183.40137   ,  -1.2960489 ,  -6.600557  ,   3.5758533 ],\n",
    "         [195.75583   , -20.534393  ,  -3.785254  ,   1.5534488 ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "lane_tolerance = 2\n",
    "x_speed_coef = 1\n",
    "y_speed_coef = 1\n",
    "crop = 100\n",
    "\n",
    "\"\"\"\n",
    "Function to get the state of the environment\n",
    "\"\"\"\n",
    "def get_sign(num): \n",
    "    sign = num/np.abs(num)\n",
    "    return sign\n",
    "\n",
    "lane = obs[0,1]\n",
    "observation = obs[1:][:,0:4]\n",
    "observation = observation[~np.all(observation == 0, axis=1)]\n",
    "\n",
    "# The bin 0 indicates (0, 8], which is the safety distance\n",
    "# bins =  [[0,8,crop], [0,8,crop], [0,8,crop], [0,8,crop]] if bins is None else bins\n",
    "# bins =  [[0,10], [0,10], [0,10], [0,10]] if bins is None else bins\n",
    "\n",
    "# Lane observations\n",
    "same_lane = observation[np.abs(observation[:,1]) <= lane_tolerance]\n",
    "lane_front = same_lane[(same_lane[:,0] > 0)]\n",
    "lane_back = same_lane[(same_lane[:,0] < 0)]\n",
    "\n",
    "# For the left and right lanes we consider 2 lanes, instead of just one \n",
    "left_lanes = observation[(observation[:,1] >= -8 - lane_tolerance) & (observation[:,1] <= -4 + lane_tolerance)]\n",
    "right_lanes = observation[(observation[:,1] <= 8 + lane_tolerance) & (observation[:,1] >= 4 - lane_tolerance)]\n",
    "\n",
    "# Calculating the adjusted distances\n",
    "front_dist = lane_front[0,0] if len(lane_front) > 0 else crop \n",
    "front_speed_diff = lane_front[0,2] if len(lane_front) > 0 else 0\n",
    "front_adj_dist = front_dist + x_speed_coef*front_speed_diff   # The more the front speed diff, the harder it is to get to the car in the front, so adjusted distance is higher\n",
    "\n",
    "back_dist = -lane_back[0,0] if len(lane_back) > 0 else crop\n",
    "back_speed_diff = lane_back[0,2] - 5 if len(lane_back) > 0 else 0\n",
    "back_adj_dist = back_dist - x_speed_coef*back_speed_diff    # The faster the car in the back is driving, the more dangerous it is, so the adjusted distance is lower\n",
    "\n",
    "left_signs = [get_sign(left_lanes[i,0]) for i in range(len(left_lanes))]\n",
    "left_adj_dists = left_lanes[:,0] + x_speed_coef*left_lanes[:,2]*left_signs - y_speed_coef*left_lanes[:,3]\n",
    "left_adj_dists = np.min(left_adj_dists) if len(left_adj_dists) > 0 else crop\n",
    "\n",
    "right_signs = [get_sign(right_lanes[i,0]) for i in range(len(right_lanes))]\n",
    "right_adj_dists = right_lanes[:,0] + x_speed_coef*right_lanes[:,2]*right_signs + y_speed_coef*right_lanes[:,3]\n",
    "right_adj_dists = np.min(right_adj_dists) if len(right_adj_dists) > 0 else crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 13.563559 ,  -9.321229 ,  -8.850195 ,   3.4367235],\n",
       "       [ 32.715305 ,  -8.534393 ,  -8.347136 ,   1.5534488],\n",
       "       [ 47.327545 ,  -4.534394 ,  -1.7440301,   1.5534488],\n",
       "       [ 53.866665 ,  -8.534393 ,  -3.6438825,   1.5534488],\n",
       "       [164.46223  ,  -4.3472805,  -7.0121   ,   1.1194158]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_signs = [get_sign(left_lanes[i,0]) for i in range(len(left_lanes))]\n",
    "left_adj_dists = left_lanes[:,0] + x_speed_coef*left_lanes[:,2]*left_signs - y_speed_coef*left_lanes[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.2766405,  22.8147202,  44.0300661,  48.6693337, 156.3307142])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_adj_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 4), dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs, crop=100, lane_tolerance=2, danger_threshold=5, x_speed_coef=1, y_speed_coef=1, bins=None):\n",
    "    \"\"\"\n",
    "    Function to get the state of the environment\n",
    "    \"\"\"\n",
    "    def get_sign(num): \n",
    "        sign = num/np.abs(num)\n",
    "        return sign\n",
    "\n",
    "    lane = obs[0,1]\n",
    "    observation = obs[1:][:,0:4]\n",
    "    observation = observation[~np.all(observation == 0, axis=1)]\n",
    "\n",
    "    # The bin 0 indicates (0, 8], which is the safety distance\n",
    "    # bins =  [[0,8,crop], [0,8,crop], [0,8,crop], [0,8,crop]] if bins is None else bins\n",
    "    # bins =  [[0,10], [0,10], [0,10], [0,10]] if bins is None else bins\n",
    "\n",
    "    # Lane observations\n",
    "    same_lane = observation[np.abs(observation[:,1]) <= lane_tolerance]\n",
    "    lane_front = same_lane[(same_lane[:,0] > 0)]\n",
    "    lane_back = same_lane[(same_lane[:,0] < 0)]\n",
    "\n",
    "    # For the left and right lanes we consider 2 lanes, instead of just one \n",
    "    left_lanes = observation[(observation[:,1] >= -8 - lane_tolerance) & (observation[:,1] <= -4 + lane_tolerance)]\n",
    "    right_lanes = observation[(observation[:,1] <= 8 + lane_tolerance) & (observation[:,1] >= 4 - lane_tolerance)]\n",
    "\n",
    "    # Calculating the adjusted distances\n",
    "    front_dist = lane_front[0,0] if len(lane_front) > 0 else crop \n",
    "    front_speed_diff = lane_front[0,2] if len(lane_front) > 0 else 0\n",
    "    front_adj_dist = front_dist + x_speed_coef*front_speed_diff   # The more the front speed diff, the harder it is to get to the car in the front, so adjusted distance is higher\n",
    "\n",
    "    back_dist = -lane_back[0,0] if len(lane_back) > 0 else crop\n",
    "    back_speed_diff = lane_back[0,2] - 5 if len(lane_back) > 0 else 0\n",
    "    back_adj_dist = back_dist - x_speed_coef*back_speed_diff    # The faster the car in the back is driving, the more dangerous it is, so the adjusted distance is lower\n",
    "\n",
    "    left_signs = [get_sign(left_lanes[i,0]) for i in range(len(left_lanes))]\n",
    "    left_adj_dists = left_lanes[:,0] + x_speed_coef*left_lanes[:,2]*left_signs - y_speed_coef*left_lanes[:,3]\n",
    "    left_adj_dist = np.min(left_adj_dists) if len(left_adj_dists) > 0 else crop\n",
    "\n",
    "    right_signs = [get_sign(right_lanes[i,0]) for i in range(len(right_lanes))]\n",
    "    right_adj_dists = right_lanes[:,0] + x_speed_coef*right_lanes[:,2]*right_signs + y_speed_coef*right_lanes[:,3]\n",
    "    right_adj_dist = np.min(right_adj_dists) if len(right_adj_dists) > 0 else crop\n",
    "\n",
    "    turn_possibility = -1 if lane < 2 else 1 if lane > 34 else 0\n",
    "\n",
    "    values = np.array([front_adj_dist, back_adj_dist, left_adj_dist, right_adj_dist])\n",
    "    if bins is not None:\n",
    "        values = bin_values(values, bins)\n",
    "    else: \n",
    "         # Use the danger threshold to make 0 or 1 \n",
    "         values = np.where(values < danger_threshold, 1, 0)\n",
    "    values = np.append(values, [turn_possibility])\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18745937714209923, nan, -3.4536432678049263, -5.516046166419983]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.mean(t) for t in danger_thresholds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bfa4fca4ad4e55a06bf3a85b75f3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 22.95870781 100.          -3.51674032  82.20952606] [[], [], [-3.516740322113037], []]\n",
      "[  0.20784283 100.          55.73577499  17.40564346] [[0.20784282684326172], [], [-3.516740322113037], []]\n",
      "[ 46.52709818 100.          11.65915489  -1.81392288] [[0.20784282684326172], [], [-3.516740322113037], [-1.8139228820800781]]\n",
      "[ 63.29769802 100.           8.33742332  -5.15916538] [[0.20784282684326172], [], [-3.516740322113037], [-1.8139228820800781, -5.159165382385254]]\n",
      "[ 17.2305603  100.          -1.51900303  70.45836639] [[0.20784282684326172], [], [-3.516740322113037, -1.5190030336380005], [-1.8139228820800781, -5.159165382385254]]\n",
      "[9.30061340e-02 1.00000000e+02 1.00000000e+02 1.96587811e+01] [[0.20784282684326172, 0.09300613403320312], [], [-3.516740322113037, -1.5190030336380005], [-1.8139228820800781, -5.159165382385254]]\n",
      "[109.59207559 100.          -9.0344162    0.40899324] [[0.20784282684326172, 0.09300613403320312], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469], [-1.8139228820800781, -5.159165382385254]]\n",
      "[ 36.87415218 100.          -3.45689869  21.8559494 ] [[0.20784282684326172, 0.09300613403320312], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195], [-1.8139228820800781, -5.159165382385254]]\n",
      "[  2.04116106 100.          86.29600525  41.36038208] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195], [-1.8139228820800781, -5.159165382385254]]\n",
      "[  1.44543934 100.          17.87964058  76.54245758] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195], [-1.8139228820800781, -5.159165382385254]]\n",
      "[ -0.40295124 100.          -0.86690092 107.02994537] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195, -0.8669009208679199], [-1.8139228820800781, -5.159165382385254]]\n",
      "[  0.60060835 100.          12.76965427  79.65642548] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082, 0.6006083488464355], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195, -0.8669009208679199], [-1.8139228820800781, -5.159165382385254]]\n",
      "[-5.77993393e-02  1.00000000e+02  1.94936218e+01  5.84228745e+01] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082, 0.6006083488464355, -0.057799339294433594], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195, -0.8669009208679199], [-1.8139228820800781, -5.159165382385254]]\n",
      "[  0.67923021 100.          50.90086746  18.85128784] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082, 0.6006083488464355, -0.057799339294433594, 0.6792302131652832], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195, -0.8669009208679199], [-1.8139228820800781, -5.159165382385254]]\n",
      "[ -0.21889496 100.          10.4319725    9.15818596] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082, 0.6006083488464355, -0.057799339294433594, 0.6792302131652832, -0.21889495849609375], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195, -0.8669009208679199], [-1.8139228820800781, -5.159165382385254]]\n",
      "[ -1.16030407 100.          31.57858467 100.        ] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082, 0.6006083488464355, -0.057799339294433594, 0.6792302131652832, -0.21889495849609375, -1.160304069519043], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195, -0.8669009208679199], [-1.8139228820800781, -5.159165382385254]]\n",
      "[ 19.09445286 100.          -9.52751827  18.2283802 ] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082, 0.6006083488464355, -0.057799339294433594, 0.6792302131652832, -0.21889495849609375, -1.160304069519043], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195, -0.8669009208679199, -9.527518272399902], [-1.8139228820800781, -5.159165382385254]]\n",
      "[ 40.86229205 100.          -6.94803715   9.04139423] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082, 0.6006083488464355, -0.057799339294433594, 0.6792302131652832, -0.21889495849609375, -1.160304069519043], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195, -0.8669009208679199, -9.527518272399902, -6.948037147521973], [-1.8139228820800781, -5.159165382385254]]\n",
      "[  0.96869087 100.          22.83184433  51.00129318] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082, 0.6006083488464355, -0.057799339294433594, 0.6792302131652832, -0.21889495849609375, -1.160304069519043, 0.9686908721923828], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195, -0.8669009208679199, -9.527518272399902, -6.948037147521973], [-1.8139228820800781, -5.159165382385254]]\n",
      "[ -1.59549665 100.          -3.07967186  64.27008057] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082, 0.6006083488464355, -0.057799339294433594, 0.6792302131652832, -0.21889495849609375, -1.160304069519043, 0.9686908721923828], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195, -0.8669009208679199, -9.527518272399902, -6.948037147521973, -3.079671859741211], [-1.8139228820800781, -5.159165382385254]]\n",
      "[ 14.69919542 100.           2.21466398 100.        ] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082, 0.6006083488464355, -0.057799339294433594, 0.6792302131652832, -0.21889495849609375, -1.160304069519043, 0.9686908721923828], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195, -0.8669009208679199, -9.527518272399902, -6.948037147521973, -3.079671859741211, 2.2146639823913574], [-1.8139228820800781, -5.159165382385254]]\n",
      "[ -1.5381484  100.          40.39410782 100.        ] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082, 0.6006083488464355, -0.057799339294433594, 0.6792302131652832, -0.21889495849609375, -1.160304069519043, 0.9686908721923828, -1.5381484031677246], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195, -0.8669009208679199, -9.527518272399902, -6.948037147521973, -3.079671859741211, 2.2146639823913574], [-1.8139228820800781, -5.159165382385254]]\n",
      "[  2.93503213 100.          42.46667099 100.        ] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082, 0.6006083488464355, -0.057799339294433594, 0.6792302131652832, -0.21889495849609375, -1.160304069519043, 0.9686908721923828, -1.5381484031677246, 2.9350321292877197], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195, -0.8669009208679199, -9.527518272399902, -6.948037147521973, -3.079671859741211, 2.2146639823913574], [-1.8139228820800781, -5.159165382385254]]\n",
      "[100.         100.          18.311409    -9.65667152] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082, 0.6006083488464355, -0.057799339294433594, 0.6792302131652832, -0.21889495849609375, -1.160304069519043, 0.9686908721923828, -1.5381484031677246, 2.9350321292877197], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195, -0.8669009208679199, -9.527518272399902, -6.948037147521973, -3.079671859741211, 2.2146639823913574], [-1.8139228820800781, -5.159165382385254, -9.656671524047852]]\n",
      "[ 55.9618832  100.           2.30101848  46.00012207] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082, 0.6006083488464355, -0.057799339294433594, 0.6792302131652832, -0.21889495849609375, -1.160304069519043, 0.9686908721923828, -1.5381484031677246, 2.9350321292877197], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195, -0.8669009208679199, -9.527518272399902, -6.948037147521973, -3.079671859741211, 2.2146639823913574, 2.301018476486206], [-1.8139228820800781, -5.159165382385254, -9.656671524047852]]\n",
      "[  2.89120603 100.          -4.55657196  67.73519135] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082, 0.6006083488464355, -0.057799339294433594, 0.6792302131652832, -0.21889495849609375, -1.160304069519043, 0.9686908721923828, -1.5381484031677246, 2.9350321292877197], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195, -0.8669009208679199, -9.527518272399902, -6.948037147521973, -3.079671859741211, 2.2146639823913574, 2.301018476486206, -4.556571960449219], [-1.8139228820800781, -5.159165382385254, -9.656671524047852]]\n",
      "[ 65.65766239 100.          -3.69396162  -5.43442488] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082, 0.6006083488464355, -0.057799339294433594, 0.6792302131652832, -0.21889495849609375, -1.160304069519043, 0.9686908721923828, -1.5381484031677246, 2.9350321292877197], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195, -0.8669009208679199, -9.527518272399902, -6.948037147521973, -3.079671859741211, 2.2146639823913574, 2.301018476486206, -4.556571960449219], [-1.8139228820800781, -5.159165382385254, -9.656671524047852, -5.434424877166748]]\n",
      "[ -3.55889225 100.          29.77308464  13.3438406 ] [[0.20784282684326172, 0.09300613403320312, 2.041161060333252, 1.445439338684082, 0.6006083488464355, -0.057799339294433594, 0.6792302131652832, -0.21889495849609375, -1.160304069519043, 0.9686908721923828, -1.5381484031677246, 2.9350321292877197, -3.558892250061035], [], [-3.516740322113037, -1.5190030336380005, -9.034416198730469, -3.4568986892700195, -0.8669009208679199, -9.527518272399902, -6.948037147521973, -3.079671859741211, 2.2146639823913574, 2.301018476486206, -4.556571960449219], [-1.8139228820800781, -5.159165382385254, -9.656671524047852, -5.434424877166748]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m     12\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m---> 13\u001b[0m     obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     temp_hist[_] \u001b[38;5;241m=\u001b[39m obs[\u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39mall(obs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)],get_state(obs)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;66;03m# Check the minimum distance to the cars in obs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:257\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_control\u001b[39m\u001b[38;5;124m\"\u001b[39m] \\\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:324\u001b[0m, in \u001b[0;36mRoad.act\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decide the actions of each entity on the road.\"\"\"\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[1;32m--> 324\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:100\u001b[0m, in \u001b[0;36mIDMVehicle.act\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     97\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Longitudinal: IDM\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m front_vehicle, rear_vehicle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneighbour_vehicles\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlane_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macceleration(ego_vehicle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    102\u001b[0m                                            front_vehicle\u001b[38;5;241m=\u001b[39mfront_vehicle,\n\u001b[0;32m    103\u001b[0m                                            rear_vehicle\u001b[38;5;241m=\u001b[39mrear_vehicle)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# When changing lane, check both current and target lanes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:361\u001b[0m, in \u001b[0;36mRoad.neighbour_vehicles\u001b[1;34m(self, vehicle, lane_index)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vehicle \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, Landmark):  \u001b[38;5;66;03m# self.network.is_connected_road(v.lane_index,\u001b[39;00m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;66;03m# lane_index, same_lane=True):\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m         s_v, lat_v \u001b[38;5;241m=\u001b[39m \u001b[43mlane\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lane\u001b[38;5;241m.\u001b[39mon_lane(v\u001b[38;5;241m.\u001b[39mposition, s_v, lat_v, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    363\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:190\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    188\u001b[0m delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m    189\u001b[0m longitudinal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection)\n\u001b[1;32m--> 190\u001b[0m lateral \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Lets try to find the best danger threshold \n",
    "danger_thresholds = [[],[],[],[]]\n",
    "\n",
    "history = {}\n",
    "# Run 1000 episodes\n",
    "with gym.make(\"highway-v0\", config=kinematics) as env:\n",
    "    for i in tqdm(range(1000)):\n",
    "        temp_hist = {}\n",
    "        obs = env.reset()\n",
    "        last_state = None\n",
    "        for _ in range(1000):\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            temp_hist[_] = obs[~np.all(obs == 0, axis=1)],get_state(obs)[0:4]\n",
    "            if done:\n",
    "                # Check the minimum distance to the cars in obs\n",
    "                loc_min = np.argmin(last_state)\n",
    "                if last_state is not None:\n",
    "                    danger_thresholds[loc_min].append(last_state[loc_min])\n",
    "                    if last_state[loc_min] > 20:\n",
    "                        print('Warning !!')\n",
    "                        history = temp_hist.copy()\n",
    "                    print(last_state, danger_thresholds)\n",
    "                break\n",
    "            last_state = get_state(obs)[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[98.20936   10.211578  24.641417   4.2190776]\n",
      " [10.238828   5.788422  -1.4295353 -4.2190776]\n",
      " [20.053162  -6.211578  -2.7973175 -4.2190776]\n",
      " [31.385824  21.788422  -1.1722001 -4.2190776]\n",
      " [39.95975   25.788422  -2.625083  -4.2190776]\n",
      " [50.331      1.788422  -2.5729313 -4.2190776]\n",
      " [59.972057   9.788422  -3.5926745 -4.2190776]\n",
      " [70.92825   -2.2115781 -3.997028  -4.2190776]\n",
      " [81.38444   -2.2115781 -3.0901124 -4.2190776]\n",
      " [89.76614   -6.211578  -3.4806619 -4.2190776]]\n",
      "[0 0 0 1 0]\n",
      "[[111.345955   11.498738   27.945198    1.593811 ]\n",
      " [  8.702689    4.501261   -4.759802   -1.593811 ]\n",
      " [ 17.744877   -7.498739   -6.555018   -1.593811 ]\n",
      " [ 29.885103   20.50126    -5.058059   -1.593811 ]\n",
      " [ 37.808144   24.50126    -6.03821    -1.593811 ]\n",
      " [ 48.145264    0.5012613  -6.2648883  -1.593811 ]\n",
      " [ 56.967197   10.353649   -8.260862    2.9742053]\n",
      " [ 67.513855   -3.4987388 -10.300811   -1.593811 ]\n",
      " [ 78.90431    -3.4987388  -6.9523883  -1.593811 ]\n",
      " [ 86.89785    -9.747918   -7.529662   -5.6805706]]\n",
      "[0 0 0 1 0]\n",
      "[[125.38512    14.089047   28.807158    4.7273808]\n",
      " [  6.2278214   1.9109522  -5.870586   -4.7273808]\n",
      " [ 14.33689   -10.089047   -7.599626   -4.7273808]\n",
      " [ 27.181868   17.910952   -6.4243026  -4.7273808]\n",
      " [ 34.705284   21.910952   -6.9813437  -4.7273808]\n",
      " [ 44.888428   -2.089048   -7.396616   -4.7273808]\n",
      " [ 52.87837     9.187159   -8.729511   -2.7561874]\n",
      " [ 61.696884   -6.089048  -14.162769   -4.7273808]\n",
      " [ 75.23479    -6.4997253  -8.493327   -8.094699 ]\n",
      " [ 83.12001   -13.52072    -8.418608   -6.2902718]]\n",
      "[1 0 0 0 0]\n",
      "[[ 1.3918654e+02  1.3672475e+01  2.1431963e+01 -1.1196517e+00]\n",
      " [ 3.1999049e+00  2.3277986e+00 -4.8066373e+00  1.1196517e+00]\n",
      " [ 1.1079916e+01 -1.0075820e+01 -7.7894288e-01 -2.2416840e+00]\n",
      " [ 2.4240036e+01  1.6094791e+01  5.1510446e-02 -2.9952922e+00]\n",
      " [ 3.1761318e+01  2.2327526e+01 -6.7482218e-02  1.1196517e+00]\n",
      " [ 4.1752125e+01 -1.6724745e+00 -2.0839486e-01  1.1196517e+00]\n",
      " [ 4.9064110e+01  8.6898394e+00 -1.5878916e+00 -3.1571846e+00]\n",
      " [ 5.4826710e+01 -5.6724744e+00 -8.3051090e+00  1.1196517e+00]\n",
      " [ 7.1419350e+01 -8.2634840e+00 -1.4078853e+00 -2.3727725e+00]\n",
      " [ 7.9471909e+01 -1.3497953e+01 -1.2863734e+00  6.2240976e-01]]\n",
      "[0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Generate an episode of the environment and show the rewards and cumulative rewards\n",
    "cum_reward = 0\n",
    "with gym.make(\"highway-v0\", config=kinematics, render_mode='human') as env:\n",
    "    obs = env.reset()\n",
    "    for _ in range(1000):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "        # Print non-zero obs\n",
    "        obs_non = obs[~np.all(obs == 0, axis=1)]\n",
    "        print(obs_non[:10])\n",
    "        print(get_state(obs))\n",
    "        if done:\n",
    "            break\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time to collision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.  0.  0.5 1. ]\n",
      " [0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.5 1.  1. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "[[0.5 1.  1.  0.5 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "[[0.  1.  1.  0.5 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 1.  1.  0.  0. ]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [1.  1.  0.5 0.  0. ]]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[0.  0.  0.  0.  0.5]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.5 1.  1.  0. ]]\n",
      "[[0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 1.  1.  0.  0. ]]\n",
      "[[0.  0.  0.  0.  0. ]\n",
      " [1.  1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n"
     ]
    }
   ],
   "source": [
    "class TimeToCollision:\n",
    "    def __init__(horizon=5,\n",
    "                policy_frequency=1,\n",
    "                simulation_frequency=10):\n",
    "\n",
    "        self.config = default_config.copy()\n",
    "        self.config[\"observation\"] =  {\n",
    "        \"type\": \"TimeToCollision\",\n",
    "        \"horizon\": horizon}\n",
    "        self.config['policy_frequency'] = policy_frequency\n",
    "        self.config['simulation_frequency'] = simulation_frequency\n",
    "\n",
    "    def get_state(self, env): \n",
    "        grid = env.vehicle.speed_index\n",
    "        return self.current_obs[grid]\n",
    "\n",
    "    def test_env(self):\n",
    "        with gym.make(\"highway-v0\", config=self.config, render_mode='human') as env:\n",
    "            obs = env.reset()\n",
    "            for _ in range(1000):\n",
    "                action = env.action_space.sample()\n",
    "                obs, reward, done, truncated, info = env.step(action)\n",
    "                print(self.get_state, reward)\n",
    "                if done:\n",
    "                    break\n",
    "                time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
