{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "\n",
    "    # Parametrization bellow cannot be changed\n",
    "    \"lanes_count\" : 10, # The environment must always have 10 lanes\n",
    "    \"vehicles_count\": 50, # The environment must always have 50 other vehicles\n",
    "    \"duration\": 120,  # [s] The environment must terminate never before 120 seconds\n",
    "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.IDMVehicle\", # This is the policy of the other vehicles\n",
    "    \"initial_spacing\": 2, # Initial spacing between vehicles needs to be at most 2\n",
    "\n",
    "    # Refer to refer to https://highway-env.farama.org/observations/ to change observation space type\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\"\n",
    "    },\n",
    "\n",
    "    # Refer to refer to https://highway-env.farama.org/actions/ to change action space type\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteMetaAction\",\n",
    "    },\n",
    "\n",
    "    # Parameterization bellow can be changed (as it refers mostly to the reward system)\n",
    "    # \"collision_reward\": -10,  # The reward received when colliding with a vehicle. (Can be changed)\n",
    "    # \"reward_speed_range\": [20, 30],  # [m/s] The reward for high speed is mapped linearly from this range to [0, HighwayEnv.HIGH_SPEED_REWARD]. (Can be changed)\n",
    "    \"simulation_frequency\": 15, #15,  # [Hz] (Can be changed)\n",
    "    \"policy_frequency\": 5, #5,  # [Hz] (Can be changed)\n",
    "\n",
    "    \"collision_reward\": -100,  # The reward received when colliding with a vehicle.\n",
    "    \"right_lane_reward\": 0.1,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
    "    # zero for other lanes.\n",
    "    \"high_speed_reward\": 100,  # The reward received when driving at full speed, linearly mapped to zero for\n",
    "    # lower speeds according to config[\"reward_speed_range\"].\n",
    "    \"lane_change_reward\": 0,  # The reward received at each lane change action.\n",
    "    \"reward_speed_range\": [20, 30],\n",
    "    \n",
    "    # Parameters defined bellow are purely for visualiztion purposes! You can alter them as you please\n",
    "    \"screen_width\": 800,  # [px]\n",
    "    \"screen_height\": 600,  # [px]\n",
    "    \"centering_position\": [0.5, 0.5],\n",
    "    \"scaling\": 5,\n",
    "    \"show_trajectories\": True,\n",
    "    \"render_agent\": True,\n",
    "    \"offscreen_rendering\": False\n",
    "}\n",
    "\n",
    "default_config = configuration.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occupancyGrid = configuration.copy()\n",
    "occupancyGrid[\"observation\"] =  {\n",
    "    \"type\": \"OccupancyGrid\",\n",
    "    # \"vehicles_count\": 50,\n",
    "    \"features\": [\n",
    "                \"presence\",\n",
    "                #\"x\", \"y\", \n",
    "                #\"vx\", \"vy\"\n",
    "                ],\n",
    "    # \"features_range\": {\n",
    "    #      \"x\": [-500, 500],\n",
    "    #      \"y\": [-500, 500],\n",
    "    #     \"vx\": [-20, 20],\n",
    "    #     \"vy\": [-20, 20]\n",
    "    # },\n",
    "    \"grid_size\": [[-100, 100], [-100, 100]],    # X controls how many lanes, Y controls how far ahead\n",
    "    \"grid_step\": [1, 1],\n",
    "    #\"absolute\": False,                     # Not implemented in the library\n",
    "    #\"as_image\": True,\n",
    "    # \"align_to_vehicle_axes\" : True\n",
    "}\n",
    "\n",
    "# The higher the number, the more frequent the policy and the simulation frequencies, the slower the simulation\n",
    "occupancyGrid[\"simulation_frequency\"] = 15\n",
    "occupancyGrid[\"policy_frequency\"] = 1\n",
    "\n",
    "env = gym.make('highway-v0', render_mode='human', config=occupancyGrid)\n",
    "\n",
    "obs, info = env.reset(seed = 30)\n",
    "\n",
    "env.close()\n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7470041889935329 0.7470041889935329\n",
      "0.7497703427235373 1.4967745317170702\n",
      "0.7497916630997844 2.2465661948168547\n",
      "0.747062037436638 2.9936282322534926\n",
      "0.7498258836702131 3.7434541159237056\n",
      "0.9570215828047481 4.700475698728454\n",
      "0.992425312301707 5.692901011030161\n",
      "0.9984753812783485 6.691376392308509\n",
      "0.9971407464067447 7.6885171387152536\n",
      "0.9996186311465904 8.688135769861844\n",
      "0.9996605856977958 9.687796355559641\n",
      "0.7924914156140906 10.480287771173732\n",
      "0.7542418714829565 11.234529642656689\n",
      "0.747622191656039 11.982151834312727\n",
      "0.7498691284710792 12.732020962783807\n",
      "0.7497170063753728 13.48173796915918\n",
      "0.7468413438148681 14.228579312974048\n",
      "0.7496048082651992 14.978184121239247\n",
      "0.7469279147275342 15.72511203596678\n",
      "5.552779165969887e-05 15.72516756375844\n"
     ]
    }
   ],
   "source": [
    "# Generate an episode of the environment and show the rewards and cumulative rewards\n",
    "cum_reward = 0\n",
    "with gym.make(\"highway-v0\", config=occupancyGrid, render_mode='human') as env:\n",
    "    obs = env.reset()\n",
    "    for _ in range(1000):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "        print(reward, cum_reward)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_reward(reward, colision_reward=-100, skew_speed=1, done=True): \n",
    "    \"\"\"\n",
    "    This function is used to correct the reward function, which is not correctly outputted by the environment\n",
    "    Params: \n",
    "        reward: float, the reward to fix\n",
    "        colision_reward: float, the colision reward to fix\n",
    "        skew_speed: float, the skew speed to fix. It is an exponent applied to the reward\n",
    "        done: bool, double check if the reward was indeed a colision\n",
    "    \"\"\"\n",
    "    if reward < 0.5 and done:\n",
    "        return colision_reward\n",
    "    else:\n",
    "        return reward**skew_speed\n",
    "\n",
    "def decode_meta_action(action):\n",
    "    \"\"\"\n",
    "    Function to output the corresponding action in text-form\n",
    "    \"\"\"\n",
    "    assert action in range(5), \"The action must be between 0 and 4\"\n",
    "    if action == 0:\n",
    "        return \"LANE_LEFT\"\n",
    "    elif action == 1:\n",
    "        return \"IDLE\"\n",
    "    elif action == 2:\n",
    "        return \"LANE_RIGHT\"\n",
    "    elif action == 3:\n",
    "        return \"FASTER\"\n",
    "    elif action == 4:\n",
    "        return \"SLOWER\"\n",
    "\n",
    "\n",
    "class OccupancyGrid():\n",
    "    def __init__(\n",
    "            self,\n",
    "            grid_size=[[-50, 50], [-50, 50]],  # X controls the lane-width, Y controls how far ahead\n",
    "            grid_step=[1, 1],\n",
    "            state_type='lane-wise',\n",
    "            colision_reward=-100,\n",
    "            skew_speed=1,\n",
    "            n_closest=3,\n",
    "            ss_bins=[5,6],\n",
    "            crop_dist=[[-10,10], [-10,25]],\n",
    "            policy=None,\n",
    "            sim_frequency=10,\n",
    "            policy_frequency=1,\n",
    "            render_mode = 'human',\n",
    "            seed = 50,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Occupancy view class constructor\n",
    "        Arguments:\n",
    "            grid_size: list of lists, the size of the grid in the x and y direction, where x controls the lane-width and y controls how far ahead. Lanes are 5m wide, and the car position is (0,0)\n",
    "            grid_step: list, the step size of the grid in the x and y direction, in meters\n",
    "            state_type: str, the type of state to use. Options are 'lane-wise' or 'n_neighbours'\n",
    "            colision_reward: float, the reward to give when a colision occurs\n",
    "            skew_speed: float, the skew speed to apply to the reward\n",
    "            n_closest: int, the number of closest cars to consider in the state space\n",
    "            ss_bins: list, the number of bins to divide the x and y directions\n",
    "            crop_dist: list of lists, the distance to crop the x and y directions, above which the values will be clipped\n",
    "            policy: function, the policy to use in the simulation\n",
    "            sim_frequency: int, the frequency of the simulation\n",
    "            policy_frequency: int, the frequency of the policy\n",
    "            render_mode: str, the mode to render the simulation\n",
    "            seed: int, the seed to use in the simulation\n",
    "        \"\"\"\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.grid_step = grid_step\n",
    "        self.config = default_config.copy()\n",
    "        self.config[\"observation\"] =  {\n",
    "            \"type\": \"OccupancyGrid\",\n",
    "            \"features\": [\"presence\"],\n",
    "            \"grid_size\": grid_size,    # X controls how many lanes, Y controls how far ahead\n",
    "            \"grid_step\": grid_step,\n",
    "        }\n",
    "        self.config[\"simulation_frequency\"] = sim_frequency\n",
    "        self.config[\"policy_frequency\"] = policy_frequency\n",
    "        self.render_mode = render_mode\n",
    "        self.seed = seed\n",
    "        self.state_type = state_type\n",
    "        self.policy = policy\n",
    "        self.n_closest, self.ss_bins, self.crop_dist = n_closest, ss_bins, crop_dist\n",
    "        self.colision_reward = colision_reward\n",
    "        self.skew_speed = skew_speed\n",
    "        self.initialize_states()\n",
    "\n",
    "\n",
    "    def initialize_states(self):\n",
    "        \"\"\"\n",
    "        Initialize the states of the occupancy grid\n",
    "        \"\"\"\n",
    "        # Start the environment\n",
    "        with gym.make('highway-v0', render_mode=self.render_mode, config=self.config) as env:\n",
    "            obs, info = env.reset(seed = self.seed)\n",
    "            self.current_obs = obs\n",
    "            self.env = env\n",
    "        \n",
    "        if self.state_type == 'lane-wise':\n",
    "            self.initialize_lane_wise()\n",
    "\n",
    "        elif self.state_type == 'n_neighbours':\n",
    "            self.initialize_n_neighbours()\n",
    "        \n",
    "    def initialize_n_neighbours(self):\n",
    "        # States will be stored in a dictionary, with the key being ((x1,x2,...,xn), (y1,y2,...,yn)), and n is the number of neighbors\n",
    "        # Make ss_bins[0] from the crop_dist[0] and ss_bins[1] from crop_dist[1]\n",
    "        self.bins_left_right = np.linspace(self.crop_dist[0][0], self.crop_dist[0][1], self.ss_bins[0])\n",
    "        self.bins_front = np.linspace(self.crop_dist[1][0], self.crop_dist[1][1], self.ss_bins[1])\n",
    "        self.bins_back = self.bins_front\n",
    "\n",
    "        # Each of the nearest neighbors will have a state of the form (x,y). Create the first key of the dictionary in the form (x1,x2,...xn)\n",
    "        x_keys = list(itertools.product(self.bins_left_right, repeat=self.n_closest))\n",
    "        y_keys = list(itertools.product(self.bins_front, repeat=self.n_closest))\n",
    "        self.states = list(itertools.product(x_keys, y_keys))\n",
    "\n",
    "    def initialize_lane_wise(self):\n",
    "        self.bins_front = [5,10,15,30]   #  [5,10,15,30]\n",
    "        self.bins_back = [5,10,30]\n",
    "        self.bins_left_right = [8,14,20]   # [-20,-10,-5,5,10,20]\n",
    "        self.states = list(itertools.product(self.bins_front, self.bins_back, self.bins_left_right, self.bins_left_right))\n",
    "\n",
    "    def get_car_positions(self):\n",
    "        \"\"\"\n",
    "        Get the car positions in the occupancy grid\n",
    "        Returns:\n",
    "            car_positions: np.array, the car positions in the occupancy grid, in the form ([x1,y1], [x2,y2], ...)\n",
    "        \"\"\" \n",
    "        positions = np.nonzero(self.current_obs[0])\n",
    "        car_positions = np.array([positions[0]*self.grid_step[0] + self.grid_size[0][0], positions[1]*self.grid_step[1] + self.grid_size[1][0]]).T\n",
    "        return car_positions\n",
    "\n",
    "    def get_n_closest(self):\n",
    "        \"\"\"\n",
    "        Get the n closest cars to the agent\n",
    "        Returns:\n",
    "            closest_car_positions: np.array, the positions of the n closest cars to the agent. If there are less than n_closest cars, the array is padded with the crop_dist values\n",
    "        \"\"\"\n",
    "\n",
    "        car_positions = self.get_car_positions()\n",
    "        distances = np.linalg.norm(car_positions, axis=1)\n",
    "\n",
    "        # Remove the agent position\n",
    "        closest = np.argsort(distances)[1:self.n_closest+1]\n",
    "        closest_car_positions = car_positions[closest]\n",
    "\n",
    "        # If there are less than n_closest cars, pad the array with the crop_dist values\n",
    "        if len(closest_car_positions) < self.n_closest:\n",
    "            n_missing = self.n_closest - len(closest_car_positions)\n",
    "            closest_car_positions = np.pad(closest_car_positions, ((0, n_missing), (0,0)), 'constant', constant_values=(self.crop_dist[0][0], self.crop_dist[1][0]))\n",
    "\n",
    "        # Values that are \n",
    "        return closest_car_positions\n",
    "    \n",
    "    def get_state(self, type='lane-wise', decode=False):\n",
    "        \"\"\"\n",
    "        Get the state of the environment\n",
    "        Arguments:\n",
    "            type: str, the type of state to get. Options are 'n_neighbours' or 'lane-wise' :\n",
    "                n_neighbours: the state is the n closest neighbours in the form (x1,x2,...,xn), (y1,y2,...,yn)\n",
    "                lane-wise: the state is a matrix with the binned distances of the agent to the car in front, back, left-lane and right-lane\n",
    "            decode: bool, whether to decode the state\n",
    "        \"\"\"\n",
    "        assert type in ['n_neighbours', 'lane-wise'], \"The type of state must be either 'n_neighbours' or 'lane-wise'\"\n",
    "        if type == 'n_neighbours':\n",
    "            state = self.state_n_neighbours()\n",
    "        elif type == 'lane-wise':\n",
    "            state = self.state_lane_wise(decode=decode)\n",
    "        return state\n",
    "\n",
    "    def state_n_neighbours(self):\n",
    "        \"\"\"\n",
    "        Get the state of the environment in a neighbour-wise manner\n",
    "        Returns:\n",
    "            state: tuple, the state of the environment in the form (x1,x2,...,xn), (y1,y2,...,yn)\n",
    "        \"\"\"\n",
    "        n_closest = self.get_n_closest()\n",
    "        # For the closest cars, get the state\n",
    "        state_x, state_y = [], []\n",
    "        # Get the bin values for each of the x,y positions, and return a tuple with the values\n",
    "        for car in n_closest:\n",
    "            x = np.digitize(car[0], self.x_bins) - 1\n",
    "            y = np.digitize(car[1], self.y_bins) - 1\n",
    "            x_val, y_val = self.x_bins[x], self.y_bins[y]\n",
    "            state_x.append(x_val)\n",
    "            state_y.append(y_val)\n",
    "        state = (tuple(state_x), tuple(state_y))\n",
    "        return tuple(state)\n",
    "    \n",
    "    def state_lane_wise(self, decode=False):\n",
    "        \"\"\"\n",
    "        Get the state of the environment in a lane-wise manner\n",
    "        Arguments:\n",
    "            state: np.array, the state of the environment with the binned distances of the agent to the car in front, back, left-lane and right-lane \n",
    "            decode: bool, whether to return the state in a decoded manner\n",
    "        \"\"\"\n",
    "        # Get the car positions in relation to the agent. The agent is at position (0,0)\n",
    "        car_positions = self.get_car_positions()\n",
    "\n",
    "        # The same lane cars are the ones with the same x+-2, while front and back are the ones with y > 0 and y < 0, respectively\n",
    "        same_lane = car_positions[np.abs(car_positions[:,0]) <= 2]\n",
    "        front = same_lane[(same_lane[:,1] > 0) & (same_lane[:,1] < 30)]\n",
    "        back = same_lane[(same_lane[:,1] < 0) & (same_lane[:,1] > -30)]\n",
    "\n",
    "        # The other lane cars have to be in the range of (2,7] and [-2,-7), for the left and right lanes, respectively\n",
    "        left_lane = car_positions[(car_positions[:,0] < -2) & (car_positions[:,0] >= -7) & (np.abs(car_positions[:,1]) < 20)]\n",
    "        right_lane = car_positions[(car_positions[:,0] > 2) & (car_positions[:,0] <= 7) & (np.abs(car_positions[:,1]) < 20)]\n",
    "        \n",
    "        # Now we need to get the cars that are the closest from the arrays above\n",
    "        front_dist = np.min(front[:,1]) if len(front) > 0 else 30            # The one with the smallest y value, e.g.: (0, 4) is closer than (0,12)\n",
    "        back_dist = -np.max(back[:,1]) if len(back) > 0 else 30               # The one with the largest y value, e.g.: (0, -4) is closer than (0,-12)     \n",
    "        \n",
    "        # For the left and right lanes, we need to get the closest euclidean distance\n",
    "        left_closest = left_lane[np.argmin(np.linalg.norm(left_lane, axis=1))] if len(left_lane) > 0 else np.array([30,30])\n",
    "        right_closest = right_lane[np.argmin(np.linalg.norm(right_lane, axis=1))] if len(right_lane) > 0 else np.array([30,30])\n",
    "        \n",
    "        # If the left or right cars are in front of the agent, then the euclidean distance will be positive, otherwise negative\n",
    "        left_dist = np.linalg.norm(left_closest)            # if left_closest[1] > 0 else -np.linalg.norm(left_closest)\n",
    "        right_dist = np.linalg.norm(right_closest)          # if right_closest[1] > 0 else -np.linalg.norm(right_closest)\n",
    "\n",
    "        front_dist = self.bins_front[np.digitize(front_dist, self.bins_front)-1]\n",
    "        back_dist = self.bins_back[np.digitize(back_dist, self.bins_back)-1]\n",
    "        left_dist = self.bins_left_right[np.digitize(left_dist, self.bins_left_right)-1]\n",
    "        right_dist = self.bins_left_right[np.digitize(right_dist, self.bins_left_right)-1]\n",
    "\n",
    "        state = tuple((front_dist, back_dist, left_dist, right_dist)) if not decode else \"Front: {}, Back: {}, Left: {}, Right: {}\".format(front_dist, back_dist, left_dist, right_dist)\n",
    "        return state\n",
    "\n",
    "    def test_env(self):\n",
    "        \"\"\"\n",
    "        Function to test the environment with a random policy, or with a policy\n",
    "        \"\"\"\n",
    "        obs, info = self.env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        while not done:\n",
    "            # start = time.time()\n",
    "            if self.policy is None:\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                action = self.policy()\n",
    "            obs, reward, done, truncate, info = self.env.step(action)\n",
    "            self.current_obs = obs\n",
    "            print(self.get_state(type='lane-wise', decode=True), decode_meta_action(action), fix_reward(reward, skew_speed=2))\n",
    "            time.sleep(1)\n",
    "            # end = time.time()\n",
    "            # print(f\"Time taken: {end-start}\")\n",
    "        self.env.close()\n",
    "        return info[\"score\"]\n",
    "\n",
    "# ------------------- SARSA -------------------    \n",
    "class Sarsa(OccupancyGrid):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha=0.75,\n",
    "        gamma=0.95,\n",
    "        epsilon=0.6,\n",
    "        print_stats=False,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        SARSA class constructor\n",
    "        Arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            m: int, the number of episodes to train the agent for\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.initialize_Q(print_stats)\n",
    "        self.alpha, self.gamma, self.epsilon = alpha, gamma, epsilon\n",
    "\n",
    "    def policy_Q(self, state):\n",
    "        values = [self.Q[(state, action)] for action in range(5)]\n",
    "        return np.argmax(values)   \n",
    "\n",
    "    def initialize_Q(self, print_stats = False):\n",
    "        # Combine the possible states with the possible actions\n",
    "        keys = list(itertools.product(self.states, range(5)))       # 5 possible actions, 0-4: left, idle, right, accelerate, decelerate\n",
    "        if len(keys) > 150000:\n",
    "            print(\"Warning: The number of states is too large, consider reducing the number of states\")\n",
    "        if print_stats:\n",
    "            print(f\"Number of states: {len(keys)}\")   \n",
    "        self.Q = {key: 0 for key in keys}\n",
    "\n",
    "    def epsilon_greedy(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(5)\n",
    "        else:\n",
    "            values = [self.Q[(state, action)] for action in range(5)]\n",
    "            return np.argmax(values)\n",
    "        \n",
    "    def train(self, m=100, verbose=0): \n",
    "        env = gym.make('highway-v0', render_mode=None, config=self.config)\n",
    "        obs, info = env.reset(seed = np.random.randint(1000))\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        state = self.get_state()\n",
    "        action = self.epsilon_greedy(state)\n",
    "        q_explored = np.count_nonzero(list(self.Q.values()))\n",
    "        for i in tqdm(range(m)):\n",
    "            env.reset()\n",
    "            cum_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                reward = fix_reward(reward, done=done, colision_reward=self.colision_reward, skew_speed=self.skew_speed)\n",
    "                next_state = self.get_state()\n",
    "                next_action = self.epsilon_greedy(next_state)\n",
    "\n",
    "                print(next_state, decode_meta_action(next_action), reward) if verbose > 2 else None\n",
    "                if self.Q[(state, action)] == 0:\n",
    "                    q_explored += 1\n",
    "                cum_reward += reward\n",
    "\n",
    "                self.Q[(state, action)] += self.alpha*(reward + self.gamma*self.Q[(next_state, next_action)] - self.Q[(state, action)])\n",
    "                state, action = next_state, next_action\n",
    "                self.current_obs = next_obs\n",
    "            print(f\"Episode {i+1} completed, cumulative reward: {cum_reward}\") if verbose > 0 else None\n",
    "            print(f\"Q explored: {100*q_explored/len(self.Q)}\") if verbose > 1 else None\n",
    "        env.close()\n",
    "\n",
    "    def test(self):\n",
    "        env = gym.make('highway-v0', render_mode=self.render_mode, config=self.config)\n",
    "        obs, info = env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        state = self.get_state(self.state_type)\n",
    "        action = self.policy_Q(state)\n",
    "        while not done:\n",
    "            next_obs, reward, done, truncate, info = env.step(action)\n",
    "            next_state = self.get_state(self.state_type)\n",
    "            next_action = self.policy_Q(next_state)\n",
    "            state, action = next_state, next_action\n",
    "            self.current_obs = next_obs\n",
    "            print(next_state)\n",
    "        env.close()\n",
    "        return info[\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- grid_size=[[-50, 50], [-50, 50]],  # X controls the lane-width, Y controls how far ahead\n",
    "- grid_step=[1, 1],\n",
    "- n_closest=3,\n",
    "- ss_bins=[5,6],\n",
    "- crop_dist=[[-10,10], [-10,25]],\n",
    "- policy=None,\n",
    "- sim_frequency=15,\n",
    "- policy_frequency=5,\n",
    "- render_mode = 'human',\n",
    "- seed = 50,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "copyQ = a.Q.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 540\n"
     ]
    }
   ],
   "source": [
    "a = Sarsa(print_stats=True, epsilon=0.6, alpha=0.3, gamma=0.8)\n",
    "a.Q = copyQ.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! TODO !!!\n",
    "\n",
    "- Use kinematics to substitute the current occupancy grid class\n",
    "\n",
    "- He cannot know when he cant turn left or right because the lane is the final one\n",
    "\n",
    "- Change the occupancy class to use occupancy grid with 5m grid size, 3 lanes and -30, 30 ahead\n",
    "\n",
    "- Make a plot history \n",
    "\n",
    "- Check how many times each state is visited\n",
    "\n",
    "- Check the action distribution, so as to see if slowing down is the most chosen action and the one with the best Q value\n",
    "\n",
    "- Change the reward for colision, so that the agent goes faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! IDEA WITH KINEMATICS !!!\n",
    "- State space in this maner: (danger ahead, danger left, danger right, danger behind, lane position, (maybe) speed)\n",
    "- Lane position can be 0 if in the middle, 1 if in the right, -1 if in the left\n",
    "\n",
    "- We need speed, because if the speed is too fast, it might not be able to turn in time\n",
    "- Actually, we could just increase the safety distance, and then we wouldn't need speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c280dc5269724502bed494f225b52c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[106], line 307\u001b[0m, in \u001b[0;36mSarsa.train\u001b[1;34m(self, m, verbose)\u001b[0m\n\u001b[0;32m    305\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m--> 307\u001b[0m     next_obs, reward, done, truncate, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     reward \u001b[38;5;241m=\u001b[39m fix_reward(reward, done\u001b[38;5;241m=\u001b[39mdone, colision_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolision_reward, skew_speed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskew_speed)\n\u001b[0;32m    309\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:257\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_control\u001b[39m\u001b[38;5;124m\"\u001b[39m] \\\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:324\u001b[0m, in \u001b[0;36mRoad.act\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decide the actions of each entity on the road.\"\"\"\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[1;32m--> 324\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:95\u001b[0m, in \u001b[0;36mIDMVehicle.act\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_road()\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_lane_change:\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchange_lane_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteering_control(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_lane_index)\n\u001b[0;32m     97\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:214\u001b[0m, in \u001b[0;36mIDMVehicle.change_lane_policy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# Does the MOBIL model recommend a lane change?\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmobil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlane_index\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_lane_index \u001b[38;5;241m=\u001b[39m lane_index\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:236\u001b[0m, in \u001b[0;36mIDMVehicle.mobil\u001b[1;34m(self, lane_index)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Do I have a planned route for a specific lane which is safe for me to access?\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m old_preceding, old_following \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneighbour_vehicles\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m self_pred_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macceleration(ego_vehicle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, front_vehicle\u001b[38;5;241m=\u001b[39mnew_preceding)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroute \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroute[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# Wrong direction\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:361\u001b[0m, in \u001b[0;36mRoad.neighbour_vehicles\u001b[1;34m(self, vehicle, lane_index)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vehicle \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, Landmark):  \u001b[38;5;66;03m# self.network.is_connected_road(v.lane_index,\u001b[39;00m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;66;03m# lane_index, same_lane=True):\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m         s_v, lat_v \u001b[38;5;241m=\u001b[39m \u001b[43mlane\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lane\u001b[38;5;241m.\u001b[39mon_lane(v\u001b[38;5;241m.\u001b[39mposition, s_v, lat_v, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    363\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:190\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    188\u001b[0m delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m    189\u001b[0m longitudinal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection)\n\u001b[1;32m--> 190\u001b[0m lateral \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a.train(m=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.37037037037037\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for keys, values in a.Q.items():\n",
    "    if values != 0:\n",
    "        count += 1\n",
    "\n",
    "print(100*count/len(a.Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: LANE_LEFT, Q-value: -46.81659543807035\n",
      "Action: IDLE, Q-value: -13.458657418241788\n",
      "Action: LANE_RIGHT, Q-value: -12.65590344059557\n",
      "Action: FASTER, Q-value: -7.4392318919133285\n",
      "Action: SLOWER, Q-value: -8.453300283131032\n"
     ]
    }
   ],
   "source": [
    "# Check the values for Q for this state (15, 30, 18, 18)\n",
    "state = (30, 30, 18, 18)\n",
    "for action in range(5):\n",
    "    print(f\"Action: {decode_meta_action(action)}, Q-value: {a.Q[(state, action)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 30, 18, 18)\n",
      "(30, 30, 18, 6)\n",
      "(30, 30, 18, 18)\n",
      "(15, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(30, 30, 18, 18)\n",
      "(30, 30, 18, 6)\n",
      "(30, 30, 18, 12)\n",
      "(15, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 18)\n",
      "(5, 30, 18, 12)\n",
      "(5, 30, 18, 12)\n",
      "(5, 30, 18, 12)\n",
      "(10, 30, 18, 12)\n",
      "(10, 30, 18, 12)\n",
      "(30, 30, 12, 6)\n",
      "(15, 30, 18, 12)\n",
      "(10, 30, 18, 12)\n",
      "(10, 30, 18, 6)\n",
      "(30, 10, 6, 6)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[106], line 339\u001b[0m, in \u001b[0;36mSarsa.test\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mprint\u001b[39m(next_state)\n\u001b[0;32m    338\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'score'"
     ]
    }
   ],
   "source": [
    "a.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_occupancy = OcupancyGrid(render_mode=None)\n",
    "# print(new_occupancy.x_bins, new_occupancy.y_bins, len(new_occupancy.states))\n",
    "# new_occupancy.test_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________\n",
    "### Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_car_positions(obs, grid_step=1, grid_size=50):\n",
    "    # car_positions = []\n",
    "    # for i in range(int(grid_size*2 / grid_step)):\n",
    "    #     for j in range(int(grid_size*2 / grid_step)):\n",
    "    #         if obs[0,i,j] == 1:\n",
    "    #             car_positions.append([i*grid_step - grid_size, j*grid_step - grid_size])\n",
    "    positions = np.nonzero(obs[0])\n",
    "    car_positions = list(zip(positions[0]*grid_step - grid_size, positions[1]*grid_step - grid_size))\n",
    "    return car_positions\n",
    "\n",
    "def get_n_closest(obs, grid_step=1, grid_size=50, n=3):\n",
    "    car_positions = return_car_positions(obs, grid_step, grid_size)\n",
    "    distances = [np.linalg.norm(car) for car in car_positions]\n",
    "    closest = np.argsort(distances)[1:n+1]\n",
    "    return [car_positions[i] for i in closest]\n",
    "\n",
    "def get_distances(car_positions):\n",
    "    return [np.linalg.norm(car) for car in car_positions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space is as follows\n",
    "</br></br>\n",
    "ACTIONS_ALL = {\n",
    "        0: 'LANE_LEFT',\n",
    "        1: 'IDLE',\n",
    "        2: 'LANE_RIGHT',\n",
    "        3: 'FASTER',\n",
    "        4: 'SLOWER'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.848857801796104, 22.47220505424423]\n",
      "[10.295630140987, 22.47220505424423]\n",
      "[10.63014581273465, 21.095023109728988]\n",
      "[11.313708498984761, 21.095023109728988]\n",
      "[12.041594578792296, 20.591260281974]\n",
      "[13.601470508735444, 19.697715603592208]\n",
      "[15.264337522473747, 18.384776310850235]\n",
      "[16.1245154965971, 18.027756377319946]\n",
      "[16.492422502470642, 17.88854381999832]\n",
      "[16.278820596099706, 18.788294228055936]\n",
      "[15.132745950421556, 18.788294228055936]\n",
      "[14.560219778561036, 17.88854381999832]\n",
      "[14.560219778561036, 17.88854381999832]\n",
      "[13.601470508735444, 17.88854381999832]\n",
      "[12.649110640673518, 17.88854381999832]\n",
      "[11.40175425099138, 18.788294228055936]\n",
      "[9.486832980505138, 18.788294228055936]\n",
      "[7.615773105863909, 18.384776310850235]\n",
      "[5.385164807134504, 18.973665961010276]\n",
      "[3.605551275463989, 18.439088914585774]\n"
     ]
    }
   ],
   "source": [
    "# Render the environment slow motion and print the observations\n",
    "env = gym.make('highway-v0', render_mode='human', config=occupancyGrid)\n",
    "env.reset(seed = 500)\n",
    "env.render()\n",
    "for _ in range(100):\n",
    "    print(env.action_space.sample())\n",
    "    obs, reward, done, truncate, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "    #print(obs)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
