{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "\n",
    "    # Parametrization bellow cannot be changed\n",
    "    \"lanes_count\" : 10, # The environment must always have 10 lanes\n",
    "    \"vehicles_count\": 50, # The environment must always have 50 other vehicles\n",
    "    \"duration\": 120,  # [s] The environment must terminate never before 120 seconds\n",
    "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.IDMVehicle\", # This is the policy of the other vehicles\n",
    "    \"initial_spacing\": 2, # Initial spacing between vehicles needs to be at most 2\n",
    "\n",
    "    # Refer to refer to https://highway-env.farama.org/observations/ to change observation space type\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\"\n",
    "    },\n",
    "\n",
    "    # Refer to refer to https://highway-env.farama.org/actions/ to change action space type\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteMetaAction\",\n",
    "    },\n",
    "\n",
    "    # Parameterization bellow can be changed (as it refers mostly to the reward system)\n",
    "    # \"collision_reward\": -10,  # The reward received when colliding with a vehicle. (Can be changed)\n",
    "    # \"reward_speed_range\": [20, 30],  # [m/s] The reward for high speed is mapped linearly from this range to [0, HighwayEnv.HIGH_SPEED_REWARD]. (Can be changed)\n",
    "    \"simulation_frequency\": 15, #15,  # [Hz] (Can be changed)\n",
    "    \"policy_frequency\": 5, #5,  # [Hz] (Can be changed)\n",
    "\n",
    "    \"collision_reward\": -100,  # The reward received when colliding with a vehicle.\n",
    "    \"right_lane_reward\": 0.1,  # The reward received when driving on the right-most lanes, linearly mapped to\n",
    "    # zero for other lanes.\n",
    "    \"high_speed_reward\": 100,  # The reward received when driving at full speed, linearly mapped to zero for\n",
    "    # lower speeds according to config[\"reward_speed_range\"].\n",
    "    \"lane_change_reward\": 0,  # The reward received at each lane change action.\n",
    "    \"reward_speed_range\": [20, 30],\n",
    "    \n",
    "    # Parameters defined bellow are purely for visualiztion purposes! You can alter them as you please\n",
    "    \"screen_width\": 800,  # [px]\n",
    "    \"screen_height\": 600,  # [px]\n",
    "    \"centering_position\": [0.5, 0.5],\n",
    "    \"scaling\": 5,\n",
    "    \"show_trajectories\": True,\n",
    "    \"render_agent\": True,\n",
    "    \"offscreen_rendering\": False\n",
    "}\n",
    "\n",
    "default_config = configuration.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_reward(reward, colision_reward=-100, skew_speed=1): \n",
    "    \"\"\"\n",
    "    This function is used to correct the reward function, which is not correctly outputted by the environment\n",
    "    Params: \n",
    "        reward: float, the reward to fix\n",
    "        colision_reward: float, the colision reward to fix\n",
    "        skew_speed: float, the skew speed to fix. It is an exponent applied to the reward\n",
    "    \"\"\"\n",
    "    if reward < 0.01:\n",
    "        return colision_reward\n",
    "    else:\n",
    "        return reward**skew_speed\n",
    "\n",
    "def decode_meta_action(action):\n",
    "    \"\"\"\n",
    "    Function to output the corresponding action in text-form\n",
    "    \"\"\"\n",
    "    assert action in range(5), \"The action must be between 0 and 4\"\n",
    "    if action == 0:\n",
    "        return \"LANE_LEFT\"\n",
    "    elif action == 1:\n",
    "        return \"IDLE\"\n",
    "    elif action == 2:\n",
    "        return \"LANE_RIGHT\"\n",
    "    elif action == 3:\n",
    "        return \"FASTER\"\n",
    "    elif action == 4:\n",
    "        return \"SLOWER\"\n",
    "\n",
    "\n",
    "class OccupancyGrid():\n",
    "    def __init__(\n",
    "            self,\n",
    "            grid_size=[[-50, 50], [-50, 50]],  # X controls the lane-width, Y controls how far ahead\n",
    "            grid_step=[1, 1],\n",
    "            n_closest=3,\n",
    "            ss_bins=[5,6],\n",
    "            crop_dist=[[-10,10], [-10,25]],\n",
    "            policy=None,\n",
    "            sim_frequency=15,\n",
    "            policy_frequency=3,\n",
    "            render_mode = 'human',\n",
    "            seed = 50,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Occupancy view class constructor\n",
    "        Arguments:\n",
    "            grid_size: list of lists, the size of the grid in the x and y direction, where x controls the lane-width and y controls how far ahead. Lanes are 5m wide, and the car position is (0,0)\n",
    "            grid_step: list, the step size of the grid in the x and y direction, in meters\n",
    "            n_closest: int, the number of closest cars to consider in the state space\n",
    "            ss_bins: list, the number of bins to divide the x and y directions\n",
    "            crop_dist: list of lists, the distance to crop the x and y directions, above which the values will be clipped\n",
    "            policy: function, the policy to use in the simulation\n",
    "            sim_frequency: int, the frequency of the simulation\n",
    "            policy_frequency: int, the frequency of the policy\n",
    "            render_mode: str, the mode to render the simulation\n",
    "            seed: int, the seed to use in the simulation\n",
    "        \"\"\"\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.grid_step = grid_step\n",
    "        self.config = default_config.copy()\n",
    "        self.config[\"observation\"] =  {\n",
    "            \"type\": \"OccupancyGrid\",\n",
    "            \"features\": [\"presence\"],\n",
    "            \"grid_size\": grid_size,    # X controls how many lanes, Y controls how far ahead\n",
    "            \"grid_step\": grid_step,\n",
    "        }\n",
    "        self.config[\"simulation_frequency\"] = sim_frequency\n",
    "        self.config[\"policy_frequency\"] = policy_frequency\n",
    "        self.render_mode = render_mode\n",
    "        self.seed = seed\n",
    "        self.policy = policy\n",
    "        self.n_closest, self.ss_bins, self.crop_dist = n_closest, ss_bins, crop_dist\n",
    "        self.initialize_states()\n",
    "\n",
    "\n",
    "    def initialize_states(self):\n",
    "        \"\"\"\n",
    "        Initialize the states of the occupancy grid\n",
    "        \"\"\"\n",
    "        # Start the environment\n",
    "        with gym.make('highway-v0', render_mode=self.render_mode, config=self.config) as env:\n",
    "            obs, info = env.reset(seed = self.seed)\n",
    "            self.current_obs = obs\n",
    "            self.env = env\n",
    "        \n",
    "        # States will be stored in a dictionary, with the key being ((x1,x2,...,xn), (y1,y2,...,yn)), and n is the number of neighbors\n",
    "        # Make ss_bins[0] from the crop_dist[0] and ss_bins[1] from crop_dist[1]\n",
    "        self.x_bins = np.linspace(self.crop_dist[0][0], self.crop_dist[0][1], self.ss_bins[0])\n",
    "        self.y_bins = np.linspace(self.crop_dist[1][0], self.crop_dist[1][1], self.ss_bins[1])\n",
    "\n",
    "        # Each of the nearest neighbors will have a state of the form (x,y). Create the first key of the dictionary in the form (x1,x2,...xn)\n",
    "        x_keys = list(itertools.product(self.x_bins, repeat=self.n_closest))\n",
    "        y_keys = list(itertools.product(self.y_bins, repeat=self.n_closest))\n",
    "        self.states = list(itertools.product(x_keys, y_keys))\n",
    "\n",
    "    def get_car_positions(self):\n",
    "        \"\"\"\n",
    "        Get the car positions in the occupancy grid\n",
    "        Returns:\n",
    "            car_positions: np.array, the car positions in the occupancy grid, in the form ([x1,y1], [x2,y2], ...)\n",
    "        \"\"\" \n",
    "        positions = np.nonzero(self.current_obs[0])\n",
    "        car_positions = np.array([positions[0]*self.grid_step[0] + self.grid_size[0][0], positions[1]*self.grid_step[1] + self.grid_size[1][0]]).T\n",
    "        return car_positions\n",
    "\n",
    "    def get_n_closest(self):\n",
    "        \"\"\"\n",
    "        Get the n closest cars to the agent\n",
    "        Returns:\n",
    "            closest_car_positions: np.array, the positions of the n closest cars to the agent. If there are less than n_closest cars, the array is padded with the crop_dist values\n",
    "        \"\"\"\n",
    "\n",
    "        car_positions = self.get_car_positions()\n",
    "        distances = np.linalg.norm(car_positions, axis=1)\n",
    "\n",
    "        # Remove the agent position\n",
    "        closest = np.argsort(distances)[1:self.n_closest+1]\n",
    "        closest_car_positions = car_positions[closest]\n",
    "\n",
    "        # If there are less than n_closest cars, pad the array with the crop_dist values\n",
    "        if len(closest_car_positions) < self.n_closest:\n",
    "            n_missing = self.n_closest - len(closest_car_positions)\n",
    "            closest_car_positions = np.pad(closest_car_positions, ((0, n_missing), (0,0)), 'constant', constant_values=(self.crop_dist[0][0], self.crop_dist[1][0]))\n",
    "\n",
    "        # Values that are \n",
    "        return closest_car_positions\n",
    "    \n",
    "    def get_state(self, type='n_neighbours', decode=False):\n",
    "        \"\"\"\n",
    "        Get the state of the environment\n",
    "        Arguments:\n",
    "            type: str, the type of state to get. Options are 'n_neighbours' or 'lane-wise' :\n",
    "                n_neighbours: the state is the n closest neighbours in the form (x1,x2,...,xn), (y1,y2,...,yn)\n",
    "                lane-wise: the state is a matrix with the binned distances of the agent to the car in front, back, left-lane and right-lane\n",
    "            decode: bool, whether to decode the state\n",
    "        \"\"\"\n",
    "        assert type in ['n_neighbours', 'lane-wise'], \"The type of state must be either 'n_neighbours' or 'lane-wise'\"\n",
    "        if type == 'n_neighbours':\n",
    "            state = self.state_n_neighbours()\n",
    "        elif type == 'lane-wise':\n",
    "            state = self.state_lane_wise(decode=decode)\n",
    "        return state\n",
    "\n",
    "    def state_n_neighbours(self):\n",
    "        \"\"\"\n",
    "        Get the state of the environment in a neighbour-wise manner\n",
    "        Returns:\n",
    "            state: tuple, the state of the environment in the form (x1,x2,...,xn), (y1,y2,...,yn)\n",
    "        \"\"\"\n",
    "        n_closest = self.get_n_closest()\n",
    "        # For the closest cars, get the state\n",
    "        state_x, state_y = [], []\n",
    "        # Get the bin values for each of the x,y positions, and return a tuple with the values\n",
    "        for car in n_closest:\n",
    "            x = np.digitize(car[0], self.x_bins) - 1\n",
    "            y = np.digitize(car[1], self.y_bins) - 1\n",
    "            x_val, y_val = self.x_bins[x], self.y_bins[y]\n",
    "            state_x.append(x_val)\n",
    "            state_y.append(y_val)\n",
    "        state = (tuple(state_x), tuple(state_y))\n",
    "        return tuple(state)\n",
    "    \n",
    "    def state_lane_wise(self, decode=False):\n",
    "        \"\"\"\n",
    "        Get the state of the environment in a lane-wise manner\n",
    "        Arguments:\n",
    "            state: np.array, the state of the environment with the binned distances of the agent to the car in front, back, left-lane and right-lane \n",
    "            decode: bool, whether to return the state in a decoded manner\n",
    "        \"\"\"\n",
    "        # Get the car positions in relation to the agent. The agent is at position (0,0)\n",
    "        car_positions = self.get_car_positions()\n",
    "\n",
    "        # The same lane cars are the ones with the same x+-2, while front and back are the ones with y > 0 and y < 0, respectively\n",
    "        same_lane = car_positions[np.abs(car_positions[:,0]) <= 2]\n",
    "        front = same_lane[(same_lane[:,1] > 0) & (same_lane[:,1] < 30)]\n",
    "        back = same_lane[(same_lane[:,1] < 0) & (same_lane[:,1] > -30)]\n",
    "\n",
    "        # The other lane cars have to be in the range of (2,7] and [-2,-7), for the left and right lanes, respectively\n",
    "        left_lane = car_positions[(car_positions[:,0] < -2) & (car_positions[:,0] >= -7) & (np.abs(car_positions[:,1]) < 20)]\n",
    "        right_lane = car_positions[(car_positions[:,0] > 2) & (car_positions[:,0] <= 7) & (np.abs(car_positions[:,1]) < 20)]\n",
    "        \n",
    "        # Now we need to get the cars that are the closest from the arrays above\n",
    "        front_dist = np.min(front[:,1]) if len(front) > 0 else 30            # The one with the smallest y value, e.g.: (0, 4) is closer than (0,12)\n",
    "        back_dist = -np.max(back[:,1]) if len(back) > 0 else 30               # The one with the largest y value, e.g.: (0, -4) is closer than (0,-12)     \n",
    "        \n",
    "        # For the left and right lanes, we need to get the closest euclidean distance\n",
    "        left_closest = left_lane[np.argmin(np.linalg.norm(left_lane, axis=1))] if len(left_lane) > 0 else np.array([30,30])\n",
    "        right_closest = right_lane[np.argmin(np.linalg.norm(right_lane, axis=1))] if len(right_lane) > 0 else np.array([30,30])\n",
    "        \n",
    "        # If the left or right cars are in front of the agent, then the euclidean distance will be positive, otherwise negative\n",
    "        left_dist = np.linalg.norm(left_closest) if left_closest[1] > 0 else -np.linalg.norm(left_closest)\n",
    "        right_dist = np.linalg.norm(right_closest) if right_closest[1] > 0 else -np.linalg.norm(right_closest)\n",
    "\n",
    "        # Now lets put the space in bins. 5 means that the distance is below 5m, 10 means that the distance is between 5 and 10m, and so on.\n",
    "        bins_front = [5,10,15,30]\n",
    "\n",
    "        # The back car is less important\n",
    "        bins_back = [5,10,30]\n",
    "        bins_left_right = [-20,-10,-5,5,10,20]\n",
    "\n",
    "        front_dist = np.digitize(front_dist, bins_front)\n",
    "        back_dist = np.digitize(back_dist, bins_back)\n",
    "        left_dist = np.digitize(left_dist, bins_left_right)\n",
    "        right_dist = np.digitize(right_dist, bins_left_right)\n",
    "\n",
    "        state = np.array([front_dist, back_dist, left_dist, right_dist]) if not decode else \"Front: {}, Back: {}, Left: {}, Right: {}\".format(front_dist, back_dist, left_dist, right_dist)\n",
    "        return state\n",
    "\n",
    "\n",
    "    def test_env(self):\n",
    "        \"\"\"\n",
    "        Function to test the environment with a random policy, or with a policy\n",
    "        \"\"\"\n",
    "        obs, info = self.env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        while not done:\n",
    "            # start = time.time()\n",
    "            if self.policy is None:\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                action = self.policy()\n",
    "            obs, reward, done, truncate, info = self.env.step(action)\n",
    "            self.current_obs = obs\n",
    "            print(self.get_state(type='lane-wise', decode=True), decode_meta_action(action), reward)\n",
    "            time.sleep(1)\n",
    "            # end = time.time()\n",
    "            # print(f\"Time taken: {end-start}\")\n",
    "        self.env.close()\n",
    "        return info[\"score\"]\n",
    "\n",
    "\n",
    "# ------------------- SARSA -------------------    \n",
    "class Sarsa(OccupancyGrid):\n",
    "    def __init__(\n",
    "        self,s\n",
    "        alpha=0.75,\n",
    "        gamma=0.8,\n",
    "        m=50,\n",
    "        epsilon=0.6,\n",
    "        print_stats=False,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        SARSA class constructor\n",
    "        Arguments:\n",
    "            alpha: float, the learning rate\n",
    "            gamma: float, the discount factor\n",
    "            m: int, the number of episodes to train the agent for\n",
    "            epsilon: float, the epsilon value for the epsilon-greedy policy\n",
    "            print_stats: bool, whether to print the statistics during initialization\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.initialize_Q()\n",
    "        self.alpha, self.gamma, self.m, self.epsilon = alpha, gamma, m, epsilon\n",
    "\n",
    "    def policy_Q(self, state):\n",
    "        values = [self.Q[(state, action)] for action in range(5)]\n",
    "        return np.argmax(values)   \n",
    "\n",
    "    def initialize_Q(self):\n",
    "        # Combine the possible states with the possible actions\n",
    "        keys = list(itertools.product(self.states, range(5)))       # 5 possible actions, 0-4: left, idle, right, accelerate, decelerate\n",
    "        if len(keys) > 150000:\n",
    "            print(\"Warning: The number of states is too large, consider reducing the number of states\")\n",
    "        self.Q = {key: 0 for key in keys}\n",
    "\n",
    "    def epsilon_greedy(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(5)\n",
    "        else:\n",
    "            values = [self.Q[(state, action)] for action in range(5)]\n",
    "            return np.argmax(values)\n",
    "        \n",
    "    def train(self): \n",
    "        env = gym.make('highway-v0', render_mode=None, config=self.config)\n",
    "        obs, info = env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        state = self.get_state()\n",
    "        action = self.epsilon_greedy(state)\n",
    "        for i in tqdm(range(self.m)):\n",
    "            env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                next_obs, reward, done, truncate, info = env.step(action)\n",
    "                reward = fix_reward(reward)\n",
    "                next_state = self.get_state()\n",
    "                next_action = self.epsilon_greedy(next_state)\n",
    "                print(next_state, decode_meta_action(next_action), reward)\n",
    "                self.Q[(state, action)] += self.alpha*(reward + self.gamma*self.Q[(next_state, next_action)] - self.Q[(state, action)])\n",
    "                state, action = next_state, next_action\n",
    "                self.current_obs = next_obs\n",
    "        env.close()\n",
    "\n",
    "    def test(self):\n",
    "        env = gym.make('highway-v0', render_mode=self.render_mode, config=self.config)\n",
    "        obs, info = env.reset(seed = self.seed)\n",
    "        self.current_obs = obs\n",
    "        done = False\n",
    "        state = self.get_state()\n",
    "        action = self.policy_Q(state)\n",
    "        while not done:\n",
    "            next_obs, reward, done, truncate, info = env.step(action)\n",
    "            next_state = self.get_state()\n",
    "            next_action = self.policy_Q(next_state)\n",
    "            state, action = next_state, next_action\n",
    "            self.current_obs = next_obs\n",
    "            print(next_state)\n",
    "        env.close()\n",
    "        return info[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = OccupancyGrid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Front: 1, Back: 4, Left: 6, Right: 6 IDLE 0.750013881947915\n",
      "Front: 1, Back: 4, Left: 6, Right: 6 IDLE 0.750013881947915\n",
      "Front: 5, Back: 4, Left: 6, Right: 4 IDLE 0.750013881947915\n",
      "Front: 5, Back: 4, Left: 6, Right: 4 FASTER 0.8612260365439561\n",
      "Front: 5, Back: 4, Left: 6, Right: 4 SLOWER 0.7005165723416802\n",
      "Front: 5, Back: 4, Left: 6, Right: 3 SLOWER 0.6113342369426019\n",
      "Front: 5, Back: 4, Left: 6, Right: 4 LANE_LEFT 0.5372146837978964\n",
      "Front: 4, Back: 4, Left: 6, Right: 6 IDLE 0.5224288037788087\n",
      "Front: 4, Back: 4, Left: 6, Right: 6 SLOWER 0.5155887104961816\n",
      "Front: 3, Back: 4, Left: 6, Right: 6 LANE_RIGHT 0.5001388194791494\n",
      "Front: 5, Back: 4, Left: 5, Right: 4 FASTER 0.6079475694315856\n",
      "Front: 5, Back: 4, Left: 5, Right: 3 FASTER 0.6739213378304756\n",
      "Front: 5, Back: 4, Left: 5, Right: 3 IDLE 0.7086255829442321\n",
      "Front: 5, Back: 4, Left: 5, Right: 3 FASTER 0.8384392437884479\n",
      "Front: 5, Back: 1, Left: 5, Right: 6 LANE_RIGHT 0.8884931108422481\n",
      "Front: 5, Back: 1, Left: 4, Right: 6 IDLE 0.9407143482229892\n",
      "Front: 5, Back: 2, Left: 6, Right: 6 FASTER 0.970038709542097\n",
      "Front: 5, Back: 3, Left: 6, Right: 6 SLOWER 0.872990568660947\n",
      "Front: 5, Back: 4, Left: 2, Right: 1 LANE_LEFT 0.8003273560630241\n",
      "Front: 5, Back: 4, Left: 2, Right: 6 IDLE 0.7787141792265321\n",
      "Front: 5, Back: 4, Left: 2, Right: 6 FASTER 0.8796759988048347\n",
      "Front: 5, Back: 4, Left: 1, Right: 6 IDLE 0.9341253594740248\n",
      "Front: 5, Back: 4, Left: 1, Right: 6 LANE_RIGHT 0.9464669169729646\n",
      "Front: 5, Back: 4, Left: 1, Right: 6 FASTER 0.9714059596423464\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 LANE_RIGHT 0.9541271111365782\n",
      "Front: 4, Back: 4, Left: 6, Right: 6 FASTER 0.9803862170975637\n",
      "Front: 3, Back: 4, Left: 6, Right: 6 LANE_LEFT 0.9898604847677726\n",
      "Front: 3, Back: 4, Left: 6, Right: 6 FASTER 0.9931246295480317\n",
      "Front: 5, Back: 4, Left: 5, Right: 5 SLOWER 0.8862243507706032\n",
      "Front: 5, Back: 4, Left: 5, Right: 5 IDLE 0.8260807001080668\n",
      "Front: 5, Back: 4, Left: 5, Right: 5 LANE_LEFT 0.7685000942842333\n",
      "Front: 2, Back: 4, Left: 6, Right: 5 IDLE 0.7625560517322316\n",
      "Front: 1, Back: 4, Left: 6, Right: 5 SLOWER 0.6488950653625748\n",
      "Front: 1, Back: 4, Left: 6, Right: 5 SLOWER 0.5835796080641839\n",
      "Front: 1, Back: 4, Left: 6, Right: 5 LANE_RIGHT 0.5260301959083573\n",
      "Front: 5, Back: 4, Left: 4, Right: 5 SLOWER 0.5155153502509082\n",
      "Front: 5, Back: 4, Left: 4, Right: 4 SLOWER 0.5114361453674722\n",
      "Front: 5, Back: 4, Left: 4, Right: 5 IDLE 0.5073960870437204\n",
      "Front: 5, Back: 4, Left: 4, Right: 5 FASTER 0.6156549991695303\n",
      "Front: 5, Back: 4, Left: 4, Right: 4 FASTER 0.6755457737007793\n",
      "Front: 1, Back: 4, Left: 4, Right: 6 LANE_RIGHT 0.6847917728224749\n",
      "Front: 1, Back: 4, Left: 4, Right: 6 IDLE 0.7164210827635088\n",
      "Front: 1, Back: 4, Left: 4, Right: 6 LANE_LEFT 0.7280347059569342\n",
      "Front: 1, Back: 4, Left: 2, Right: 6 IDLE 0.7369918910353588\n",
      "Front: 5, Back: 4, Left: 3, Right: 4 LANE_LEFT 0.7091500423739447\n",
      "Front: 5, Back: 4, Left: 3, Right: 4 LANE_RIGHT 0.747181870580503\n",
      "Front: 5, Back: 4, Left: 2, Right: 3 FASTER 0.8584631332263004\n",
      "Front: 0, Back: 4, Left: 2, Right: 6 LANE_RIGHT 0.8944550863056966\n",
      "Front: 5, Back: 0, Left: 1, Right: 6 LANE_LEFT 0.9557388572186407\n",
      "Front: 5, Back: 4, Left: 1, Right: 2 SLOWER 0.862957426766599\n",
      "Front: 5, Back: 4, Left: 1, Right: 2 IDLE 0.8130440510847808\n",
      "Front: 5, Back: 1, Left: 1, Right: 6 LANE_RIGHT 0.7678743403333969\n",
      "Front: 5, Back: 2, Left: 6, Right: 6 LANE_RIGHT 0.7607664077387781\n",
      "Front: 5, Back: 2, Left: 6, Right: 6 FASTER 0.8697057648156743\n",
      "Front: 5, Back: 3, Left: 6, Right: 6 FASTER 0.9286056337960361\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 SLOWER 0.8493612702717476\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 FASTER 0.916436084782145\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 FASTER 0.953633292294061\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 SLOWER 0.8630584132040826\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 LANE_LEFT 0.7919630931989802\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 SLOWER 0.663562363569315\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 IDLE 0.5932817944901323\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 LANE_RIGHT 0.5372171473856872\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 IDLE 0.520858986355177\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 LANE_LEFT 0.5049822447528288\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 LANE_RIGHT 0.5034612180149229\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 FASTER 0.6124527364504836\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 FASTER 0.6748595170042783\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 SLOWER 0.5975350796264609\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 IDLE 0.5543152449426612\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 IDLE 0.5302696792263979\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 FASTER 0.6281247607632898\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 LANE_RIGHT 0.6824244747626786\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 FASTER 0.8237683304404196\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 FASTER 0.9022039782344513\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 IDLE 0.945730160659965\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 IDLE 0.9698840952736284\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 LANE_LEFT 0.9638177592718972\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 LANE_RIGHT 0.9888345625182936\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 FASTER 0.9926566551448723\n",
      "Front: 5, Back: 4, Left: 6, Right: 6 LANE_RIGHT 0.9964237748829107\n",
      "Front: 5, Back: 4, Left: 5, Right: 6 IDLE 0.9982542701221864\n",
      "Front: 5, Back: 4, Left: 5, Right: 6 LANE_LEFT 0.9814142592732985\n",
      "Front: 2, Back: 4, Left: 6, Right: 6 LANE_LEFT 0.9453880347132592\n",
      "Front: 2, Back: 4, Left: 6, Right: 6 LANE_RIGHT 0.9996568559400535\n"
     ]
    }
   ],
   "source": [
    "a.test_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OccupancyGrid' object has no attribute 'current_obs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_obs\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OccupancyGrid' object has no attribute 'current_obs'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
